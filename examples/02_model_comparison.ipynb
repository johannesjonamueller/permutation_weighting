{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202b8a2f78a350f2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb6ff34f537fbe",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import expit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import warnings\n",
    "\n",
    "# For better plot styling\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('colorblind')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import the permutation weighting implementation\n",
    "import sys\n",
    "sys.path.append('.')  # Add the current directory to path\n",
    "try:\n",
    "    from permutation_weighting.estimator import PW\n",
    "    print(\"Successfully imported PW from the package\")\n",
    "except ImportError:\n",
    "    print(\"Failed to import PW from the package - will implement a simplified version\")\n",
    "\n",
    "# Kang-Schafer DGP for binary treatment\n",
    "def generate_kang_schafer_binary(n=1000, seed=42, misspecified=False):\n",
    "    \"\"\"\n",
    "    Generate data according to the Kang-Schafer setup with binary treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n: int\n",
    "        Number of observations\n",
    "    seed: int\n",
    "        Random seed\n",
    "    misspecified: bool\n",
    "        Whether to return the misspecified transformations of covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df: pd.DataFrame\n",
    "        Data frame with columns: X1-X4 (covariates), A (treatment), Y (outcome), \n",
    "        Y1 (potential outcome under treatment), Y0 (potential outcome under control),\n",
    "        and X1_mis to X4_mis (misspecified covariates, if requested)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate covariates\n",
    "    X = np.random.normal(0, 1, size=(n, 4))\n",
    "    \n",
    "    # Treatment assignment\n",
    "    ps_linear = X[:, 0] - 0.5 * X[:, 1] + 0.25 * X[:, 2] + 0.1 * X[:, 3]\n",
    "    ps = expit(ps_linear)\n",
    "    A = np.random.binomial(1, ps, size=n)\n",
    "    \n",
    "    # Generate potential outcomes\n",
    "    Y1 = 210 + 1 + 27.4*X[:, 0] + 13.7*X[:, 1] + 13.7*X[:, 2] + 13.7*X[:, 3] + np.random.normal(0, 1, size=n)\n",
    "    Y0 = 210 + 0 + 27.4*X[:, 0] + 13.7*X[:, 1] + 13.7*X[:, 2] + 13.7*X[:, 3] + np.random.normal(0, 1, size=n)\n",
    "    \n",
    "    # Observed outcome\n",
    "    Y = A * Y1 + (1 - A) * Y0\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'X1': X[:, 0],\n",
    "        'X2': X[:, 1],\n",
    "        'X3': X[:, 2],\n",
    "        'X4': X[:, 3],\n",
    "        'A': A,\n",
    "        'Y': Y,\n",
    "        'Y1': Y1,\n",
    "        'Y0': Y0\n",
    "    })\n",
    "    \n",
    "    # Add misspecified covariates if requested\n",
    "    if misspecified:\n",
    "        df['X1_mis'] = np.exp(X[:, 0]/2)\n",
    "        df['X2_mis'] = X[:, 1] / (1 + np.exp(X[:, 0])) + 10\n",
    "        df['X3_mis'] = (X[:, 0] * X[:, 2] / 25 + 0.6)**3\n",
    "        df['X4_mis'] = (X[:, 1] + X[:, 3] + 20)**2\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84909c845cc3ad46",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now I'll implement simplified versions of the baseline methods for comparison\n",
    "# 1. Stabilized Inverse Propensity Score Weighting (IPSW)\n",
    "def compute_ipsw_binary(A, X, misspecified=False):\n",
    "    \"\"\"\n",
    "    Compute Stabilized Inverse Propensity Score Weights for binary treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A: array-like\n",
    "        Binary treatment indicator\n",
    "    X: array-like or DataFrame\n",
    "        Covariates\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates (X*_mis) if available\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights: array\n",
    "        Stabilized IPW weights\n",
    "    \"\"\"\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if misspecified and 'X1_mis' in X.columns:\n",
    "            X_mat = X[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']].values\n",
    "        else:\n",
    "            X_mat = X[['X1', 'X2', 'X3', 'X4']].values\n",
    "    else:\n",
    "        X_mat = X\n",
    "    \n",
    "    # Fit propensity score model\n",
    "    ps_model = LogisticRegression(max_iter=1000)\n",
    "    ps_model.fit(X_mat, A)\n",
    "    \n",
    "    # Compute propensity scores\n",
    "    ps = ps_model.predict_proba(X_mat)[:, 1]\n",
    "    \n",
    "    # Marginal treatment probability\n",
    "    p_A = np.mean(A)\n",
    "    \n",
    "    # Compute stabilized weights\n",
    "    weights = np.where(A == 1, p_A / ps, (1 - p_A) / (1 - ps))\n",
    "    \n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130a7c8e648434db",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Basic implementation of CBPS (simplified version)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def compute_cbps_binary(A, X, misspecified=False):\n",
    "    \"\"\"\n",
    "    Simplified version of Covariate Balancing Propensity Score for binary treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A: array-like\n",
    "        Binary treatment indicator\n",
    "    X: array-like or DataFrame\n",
    "        Covariates\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates (X*_mis) if available\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights: array\n",
    "        CBPS weights\n",
    "    \"\"\"\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if misspecified and 'X1_mis' in X.columns:\n",
    "            X_mat = X[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']].values\n",
    "        else:\n",
    "            X_mat = X[['X1', 'X2', 'X3', 'X4']].values\n",
    "    else:\n",
    "        X_mat = X\n",
    "    \n",
    "    # This is a simplification - true CBPS adds balance constraints\n",
    "    # For demonstration, we'll just use logistic regression with L2 penalty\n",
    "    ps_model = LogisticRegression(C=0.1, max_iter=1000)\n",
    "    ps_model.fit(X_mat, A)\n",
    "    \n",
    "    # Compute propensity scores\n",
    "    ps = ps_model.predict_proba(X_mat)[:, 1]\n",
    "    \n",
    "    # Marginal treatment probability\n",
    "    p_A = np.mean(A)\n",
    "    \n",
    "    # Compute weights\n",
    "    weights = np.where(A == 1, p_A / ps, (1 - p_A) / (1 - ps))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae02852a64a80fc0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_ipsw_gbm(A, X, misspecified=False):\n",
    "    \"\"\"\n",
    "    Compute IPSW weights using gradient boosting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A: array-like\n",
    "        Treatment indicator\n",
    "    X: array-like or DataFrame\n",
    "        Covariates\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights: array\n",
    "        IPSW weights\n",
    "    \"\"\"\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if misspecified and 'X1_mis' in X.columns:\n",
    "            X_mat = X[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']].values\n",
    "        else:\n",
    "            X_mat = X[['X1', 'X2', 'X3', 'X4']].values\n",
    "    else:\n",
    "        X_mat = X\n",
    "    \n",
    "    # Fit propensity score model with GBM\n",
    "    gbm = GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
    "    gbm.fit(X_mat, A)\n",
    "    \n",
    "    # Compute propensity scores\n",
    "    ps = gbm.predict_proba(X_mat)[:, 1]\n",
    "    \n",
    "    # Clip propensity scores to avoid extreme weights\n",
    "    ps = np.clip(ps, 0.01, 0.99)\n",
    "    \n",
    "    # Marginal treatment probability\n",
    "    p_A = np.mean(A)\n",
    "    \n",
    "    # Compute stabilized weights\n",
    "    weights = np.where(A == 1, p_A / ps, (1 - p_A) / (1 - ps))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfad52ec702a1dcf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_sbw_binary(A, X, misspecified=False):\n",
    "    \"\"\"\n",
    "    Simplified version of Stabilized Balancing Weights for binary treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A: array-like\n",
    "        Binary treatment indicator\n",
    "    X: array-like or DataFrame\n",
    "        Covariates\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights: array\n",
    "        SBW weights\n",
    "    \"\"\"\n",
    "    import cvxpy as cp # TODO: add cvxpy to requirements iff used in the package\n",
    "    \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if misspecified and 'X1_mis' in X.columns:\n",
    "            X_mat = X[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']].values\n",
    "        else:\n",
    "            X_mat = X[['X1', 'X2', 'X3', 'X4']].values\n",
    "    else:\n",
    "        X_mat = X\n",
    "    \n",
    "    n = len(A)\n",
    "    n_treated = np.sum(A)\n",
    "    n_control = n - n_treated\n",
    "    \n",
    "    # Separate covariates for treated and control\n",
    "    X_treated = X_mat[A == 1]\n",
    "    X_control = X_mat[A == 0]\n",
    "    \n",
    "    # Calculate means\n",
    "    treated_mean = np.mean(X_treated, axis=0)\n",
    "    \n",
    "    # Initialize weights for control units\n",
    "    w = cp.Variable(n_control, nonneg=True)\n",
    "    \n",
    "    # Balance constraint: weighted mean of control features equals mean of treated features\n",
    "    balance_constraint = []\n",
    "    for j in range(X_mat.shape[1]):\n",
    "        # Allow small imbalance (delta)\n",
    "        delta = 0.1 * np.std(X_mat[:, j])\n",
    "        balance_constraint.append(cp.abs(cp.sum(cp.multiply(w, X_control[:, j])) - treated_mean[j] * cp.sum(w)) <= delta)\n",
    "    \n",
    "    # Sum constraint\n",
    "    balance_constraint.append(cp.sum(w) == 1)\n",
    "    \n",
    "    # Objective: minimize variance\n",
    "    objective = cp.Minimize(cp.sum_squares(w - 1/n_control))\n",
    "    \n",
    "    # Solve optimization problem\n",
    "    prob = cp.Problem(objective, balance_constraint)\n",
    "    try:\n",
    "        prob.solve(solver=cp.OSQP)\n",
    "    except:\n",
    "        try:\n",
    "            prob.solve(solver=cp.ECOS)\n",
    "        except:\n",
    "            prob.solve(solver=cp.SCS)\n",
    "    \n",
    "    # Create final weights vector\n",
    "    weights = np.ones(n)\n",
    "    weights[A == 0] = w.value * n_control\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02458b6a92eceda",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# EVALUATE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d2d6f37081070",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_ate_binary(df, weights, true_ate=1.0):\n",
    "    \"\"\"\n",
    "    Evaluate ATE estimation for binary treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: DataFrame\n",
    "        Data with 'A', 'Y', 'Y1', 'Y0' columns\n",
    "    weights: array-like\n",
    "        Weights for each observation\n",
    "    true_ate: float\n",
    "        True average treatment effect\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with bias and rmse\n",
    "    \"\"\"\n",
    "    # Calculate weighted ATE\n",
    "    treated_idx = df['A'] == 1\n",
    "    control_idx = df['A'] == 0\n",
    "    \n",
    "    treated_mean = np.sum(df.loc[treated_idx, 'Y'] * weights[treated_idx]) / np.sum(weights[treated_idx])\n",
    "    control_mean = np.sum(df.loc[control_idx, 'Y'] * weights[control_idx]) / np.sum(weights[control_idx])\n",
    "    \n",
    "    estimated_ate = treated_mean - control_mean\n",
    "    \n",
    "    error = estimated_ate - true_ate\n",
    "    \n",
    "    return error\n",
    "\n",
    "def evaluate_dose_response_continuous(df, weights, treatment_grid=None):\n",
    "    \"\"\"\n",
    "    Evaluate dose-response estimation for continuous treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: DataFrame\n",
    "        Data with 'A', 'Y', 'true_dr' columns\n",
    "    weights: array-like\n",
    "        Weights for each observation\n",
    "    treatment_grid: array-like, optional\n",
    "        Grid of treatment values for evaluation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with integrated bias and rmse\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # Define treatment grid if not provided\n",
    "    if treatment_grid is None:\n",
    "        min_a, max_a = np.percentile(df['A'], [5, 95])\n",
    "        treatment_grid = np.linspace(min_a, max_a, 50)\n",
    "    \n",
    "    # True dose-response function\n",
    "    true_dr = [210 + 1/(1 + np.exp(a)) for a in treatment_grid]\n",
    "    \n",
    "    # Estimate dose-response function using weighted local linear regression\n",
    "    est_dr = []\n",
    "    for a in treatment_grid:\n",
    "        # Calculate kernel weights\n",
    "        bandwidth = (np.percentile(df['A'], 75) - np.percentile(df['A'], 25)) / 1.34\n",
    "        kernel_weights = np.exp(-0.5 * ((df['A'] - a) / bandwidth)**2) * weights\n",
    "        \n",
    "        # Fit weighted linear regression\n",
    "        model = LinearRegression()\n",
    "        model.fit(\n",
    "            df[['A']], \n",
    "            df['Y'],\n",
    "            sample_weight=kernel_weights\n",
    "        )\n",
    "        \n",
    "        # Predict at treatment value a\n",
    "        est_dr.append(model.predict([[a]])[0])\n",
    "    \n",
    "    # Calculate integrated bias and RMSE\n",
    "    bias = np.mean(np.abs(np.array(est_dr) - np.array(true_dr)))\n",
    "    rmse = np.sqrt(np.mean((np.array(est_dr) - np.array(true_dr))**2))\n",
    "    \n",
    "    return {\n",
    "        'integrated_bias': bias,\n",
    "        'integrated_rmse': rmse,\n",
    "        'treatment_grid': treatment_grid,\n",
    "        'estimated_dr': est_dr,\n",
    "        'true_dr': true_dr\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9780b71f3e9338",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# SIMULATION BINARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233387ec5f5bee29",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_binary_simulation(n_replications=100, sample_sizes=[500, 1000, 1500, 2000], \n",
    "                         misspecified=False, nn_configs=None):\n",
    "    \"\"\"\n",
    "    Run simulation for binary treatment methods\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_replications: int\n",
    "        Number of simulation replications\n",
    "    sample_sizes: list\n",
    "        List of sample sizes to test\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates\n",
    "    nn_configs: dict, optional\n",
    "        Neural network configurations\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results: dict\n",
    "        Dictionary with results for all methods\n",
    "    \"\"\"\n",
    "    # Define baseline methods\n",
    "    methods = {\n",
    "        'Unweighted': lambda a, x: np.ones(len(a)),\n",
    "        'IPSW-GLM': lambda a, x: compute_ipsw_binary(a, x, misspecified),\n",
    "        'IPSW-GBM': lambda a, x: compute_ipsw_gbm(a, x, misspecified),\n",
    "        'CBPS': lambda a, x: compute_cbps_binary(a, x, misspecified),\n",
    "        'SBW': lambda a, x: compute_sbw_binary(a, x, misspecified)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Standard trainers\n",
    "    methods['PW-GLM'] = lambda a, x: PW(\n",
    "        a, x, \n",
    "        classifier='logit', \n",
    "        estimand='ATE',\n",
    "        num_replicates=50,\n",
    "        estimand_params={'bootstrap': True}\n",
    "    )['weights']\n",
    "    \n",
    "    methods['PW-Boosting'] = lambda a, x: PW(\n",
    "        a, x, \n",
    "        classifier='boosting', \n",
    "        estimand='ATE',\n",
    "        classifier_params={'n_estimators': 100, 'max_depth': 3},\n",
    "        num_replicates=50,\n",
    "        estimand_params={'bootstrap': True}\n",
    "    )['weights']\n",
    "    \n",
    "    # SGD-based trainers if nn_configs is provided\n",
    "    if nn_configs is not None:\n",
    "        # SGD logistic regression\n",
    "        methods['PW-SGD-Logit'] = lambda a, x: PW(\n",
    "            a, x, \n",
    "            classifier='logit', \n",
    "            estimand='ATE',\n",
    "            use_sgd=True,\n",
    "            classifier_params={\n",
    "                'alpha': nn_configs.get('alpha', 0.0001),\n",
    "                'max_iter': nn_configs.get('epochs', 100),\n",
    "                'learning_rate': 'optimal'\n",
    "            },\n",
    "            num_replicates=50,\n",
    "            estimand_params={'bootstrap': True}\n",
    "        )['weights']\n",
    "        \n",
    "        # Neural network\n",
    "        methods['PW-Neural'] = lambda a, x: PW(\n",
    "            a, x, \n",
    "            classifier='neural_net', \n",
    "            estimand='ATE',\n",
    "            use_sgd=True,\n",
    "            classifier_params={\n",
    "                'hidden_layer_sizes': (nn_configs.get('hidden_size', 64),),\n",
    "                'max_iter': nn_configs.get('epochs', 100),\n",
    "                'batch_size': nn_configs.get('batch_size', 32),\n",
    "                'learning_rate_init': nn_configs.get('learning_rate', 0.001)\n",
    "            },\n",
    "            num_replicates=50,\n",
    "            estimand_params={'bootstrap': True}\n",
    "        )['weights']\n",
    "        \n",
    "        # PyTorch-based trainers if available\n",
    "        try:\n",
    "            # Test if torch module exists\n",
    "            import torch\n",
    "            \n",
    "            # MLP with PyTorch\n",
    "            methods['PW-Torch-MLP'] = lambda a, x: PW(\n",
    "                a, x, \n",
    "                classifier='mlp', \n",
    "                estimand='ATE',\n",
    "                use_torch=True,\n",
    "                classifier_params={\n",
    "                    'hidden_dims': [nn_configs.get('hidden_size', 64), nn_configs.get('hidden_size', 64)//2],\n",
    "                    'epochs': nn_configs.get('epochs', 100),\n",
    "                    'batch_size': nn_configs.get('batch_size', 32),\n",
    "                    'learning_rate': nn_configs.get('learning_rate', 0.001)\n",
    "                },\n",
    "                num_replicates=50,\n",
    "                estimand_params={'bootstrap': True}\n",
    "            )['weights']\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"PyTorch not available, skipping PyTorch models\")\n",
    "    \n",
    "    results = {\n",
    "        method: {\n",
    "            'bias': {size: [] for size in sample_sizes},\n",
    "            'rmse': {size: [] for size in sample_sizes}\n",
    "        } for method in methods\n",
    "    }\n",
    "    \n",
    "    for rep in range(n_replications):\n",
    "        for size in sample_sizes:\n",
    "            print(f\"Replication {rep+1}/{n_replications}, Sample size {size}\")\n",
    "            \n",
    "            # Generate data\n",
    "            df = generate_kang_schafer_binary(n=size, seed=rep, misspecified=misspecified)\n",
    "            \n",
    "            # Apply each method\n",
    "            for method_name, method_func in methods.items():\n",
    "                try:\n",
    "                    x_features = df[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']] if misspecified else df[['X1', 'X2', 'X3', 'X4']]\n",
    "                    weights = method_func(df['A'].values, x_features)\n",
    "                    \n",
    "                    # Evaluate\n",
    "                    eval_result = evaluate_ate_binary(df, weights)\n",
    "                    \n",
    "                    results[method_name]['bias'][size].append(eval_result['bias'])\n",
    "                    results[method_name]['rmse'][size].append(eval_result['rmse'])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with method {method_name}: {e}\")\n",
    "    \n",
    "    # Compute mean and std\n",
    "    summary = {\n",
    "        method: {\n",
    "            'mean_bias': {size: np.abs(np.mean(results[method]['error'][size])) for size in sample_sizes},\n",
    "            'mean_mse': {size: np.mean(np.power(results[method]['error'][size], 2)) for size in sample_sizes},\n",
    "            'std_bias': {size: np.std(results[method]['bias'][size]) for size in sample_sizes}, # TODO\n",
    "            'std_rmse': {size: np.std(np.power(results[method]['error'][size], 2)) / np.sqrt(n_replications) for size in sample_sizes}\n",
    "        } for method in methods\n",
    "    }\n",
    "    \n",
    "    return results, summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b12a885a7b9d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CONTIOUS  TREAMTMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3838de4c21ba1b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data generation for continuous treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d56aa7072214f58",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Kang-Schafer DGP for continuous treatment\n",
    "def generate_kang_schafer_continuous(n=1000, seed=42, misspecified=False):\n",
    "    \"\"\"\n",
    "    Generate data according to the Kang-Schafer setup with continuous treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n: int\n",
    "        Number of observations\n",
    "    seed: int\n",
    "        Random seed\n",
    "    misspecified: bool\n",
    "        Whether to return the misspecified transformations of covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df: pd.DataFrame\n",
    "        Data frame with columns: X1-X4 (covariates), A (treatment), Y (outcome),\n",
    "        and X1_mis to X4_mis (misspecified covariates, if requested)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate covariates\n",
    "    X = np.random.normal(0, 1, size=(n, 4))\n",
    "    \n",
    "    # Treatment assignment (linear with noise)\n",
    "    A_linear = X[:, 0] - 0.5 * X[:, 1] + 0.25 * X[:, 2] + 0.1 * X[:, 3]\n",
    "    A = A_linear + np.random.normal(0, 1, size=n)\n",
    "    \n",
    "    # Generate outcome with non-linear treatment effect\n",
    "    Y = 210 + 1/(1 + np.exp(A)) + 27.4*X[:, 0] + 13.7*X[:, 1] + 13.7*X[:, 2] + 13.7*X[:, 3] + np.random.normal(0, 1, size=n)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'X1': X[:, 0],\n",
    "        'X2': X[:, 1],\n",
    "        'X3': X[:, 2],\n",
    "        'X4': X[:, 3],\n",
    "        'A': A,\n",
    "        'Y': Y\n",
    "    })\n",
    "    \n",
    "    # True dose-response function (for evaluation)\n",
    "    def true_dose_response(a):\n",
    "        return 210 + 1/(1 + np.exp(a)) + 27.4 * 0 + 13.7 * 0 + 13.7 * 0 + 13.7 * 0  # Expectation of X is 0\n",
    "    \n",
    "    df['true_dr'] = [true_dose_response(a) for a in A]\n",
    "    \n",
    "    # Add misspecified covariates if requested\n",
    "    if misspecified:\n",
    "        df['X1_mis'] = np.exp(X[:, 0]/2)\n",
    "        df['X2_mis'] = X[:, 1] / (1 + np.exp(X[:, 0])) + 10\n",
    "        df['X3_mis'] = (X[:, 0] * X[:, 2] / 25 + 0.6)**3\n",
    "        df['X4_mis'] = (X[:, 1] + X[:, 3] + 20)**2\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Check if our data generation works\n",
    "# binary_df = generate_kang_schafer_binary(n=1000, seed=42, misspecified=True)\n",
    "# print(\"Binary treatment data (first 5 rows):\")\n",
    "# print(binary_df.iloc[:5, :10])  # Show first 5 rows and 10 columns\n",
    "# \n",
    "# continuous_df = generate_kang_schafer_continuous(n=1000, seed=42, misspecified=True)\n",
    "# print(\"\\nContinuous treatment data (first 5 rows):\")\n",
    "# print(continuous_df.iloc[:5, :10])  # Show first 5 rows and 10 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80022a55aa7edd8a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# other methods for continuous treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c646ad3f6c4c3d2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_normal_linear_weights(A, X, misspecified=False):\n",
    "    \"\"\"\n",
    "    Compute weights for continuous treatments using a normal-linear model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A: array-like\n",
    "        Continuous treatment variable\n",
    "    X: array-like or DataFrame\n",
    "        Covariates\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights: array\n",
    "        Stabilized weights\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from scipy.stats import norm\n",
    "    \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if misspecified and 'X1_mis' in X.columns:\n",
    "            X_mat = X[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']].values\n",
    "        else:\n",
    "            X_mat = X[['X1', 'X2', 'X3', 'X4']].values\n",
    "    else:\n",
    "        X_mat = X\n",
    "    \n",
    "    # Fit linear regression for treatment given covariates\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_mat, A)\n",
    "    \n",
    "    # Predict treatment and calculate residuals\n",
    "    A_pred = model.predict(X_mat)\n",
    "    resid = A - A_pred\n",
    "    \n",
    "    # Estimate residual variance\n",
    "    sigma = np.std(resid)\n",
    "    \n",
    "    # Compute likelihood of observed treatment under model\n",
    "    pdf_cond = norm.pdf(A, loc=A_pred, scale=sigma)\n",
    "    \n",
    "    # Compute likelihood under marginal distribution\n",
    "    pdf_marg = norm.pdf(A, loc=np.mean(A), scale=np.std(A))\n",
    "    \n",
    "    # Compute stabilized weights\n",
    "    weights = pdf_marg / pdf_cond\n",
    "    \n",
    "    # Clip extreme weights\n",
    "    weights = np.clip(weights, 0.01, 100)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def compute_np_cbps(A, X, misspecified=False):\n",
    "    \"\"\"\n",
    "    Compute non-parametric Covariate Balancing Propensity Score weights for continuous treatments\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A: array-like\n",
    "        Continuous treatment variable\n",
    "    X: array-like or DataFrame\n",
    "        Covariates\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights: array\n",
    "        CBPS weights\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LassoCV\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if misspecified and 'X1_mis' in X.columns:\n",
    "            X_mat = X[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']].values\n",
    "        else:\n",
    "            X_mat = X[['X1', 'X2', 'X3', 'X4']].values\n",
    "    else:\n",
    "        X_mat = X\n",
    "    \n",
    "    # Create polynomial features to approximate non-parametric model\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=True)\n",
    "    X_poly = poly.fit_transform(X_mat)\n",
    "    \n",
    "    # Fit Lasso model with cross-validation for regularization\n",
    "    model = LassoCV(cv=5, max_iter=2000, random_state=42)\n",
    "    model.fit(X_poly, A)\n",
    "    \n",
    "    # Compute residuals\n",
    "    A_pred = model.predict(X_poly)\n",
    "    resid = A - A_pred\n",
    "    \n",
    "    # Estimate density ratio using balancing constraint approach\n",
    "    # This is a simplified approximation to npCBPS\n",
    "    \n",
    "    # Compute covariance between residuals and covariate functions\n",
    "    cov_mat = np.zeros((X_poly.shape[1], 1))\n",
    "    for j in range(X_poly.shape[1]):\n",
    "        cov_mat[j, 0] = np.cov(X_poly[:, j], resid)[0, 1]\n",
    "    \n",
    "    # Compute weights using exponential tilting\n",
    "    lambda_param = np.linalg.solve(np.cov(X_poly, rowvar=False) + 0.001 * np.eye(X_poly.shape[1]), cov_mat)\n",
    "    scores = X_poly @ lambda_param\n",
    "    \n",
    "    # Apply exponential tilting and normalize\n",
    "    weights = np.exp(scores - np.mean(scores))\n",
    "    weights = weights / np.mean(weights) * len(A)\n",
    "    \n",
    "    # Clip extreme weights\n",
    "    weights = np.clip(weights, 0.01, 100)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303fe2218ad77093",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# continuous treatment simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ac279e4154406",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_continuous_simulation(n_replications=100, sample_sizes=[500, 1000, 1500, 2000], \n",
    "                             misspecified=False, nn_configs=None):\n",
    "    \"\"\"\n",
    "    Run simulation for continuous treatment methods\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_replications: int\n",
    "        Number of simulation replications\n",
    "    sample_sizes: list\n",
    "        List of sample sizes to test\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates\n",
    "    nn_configs: dict, optional\n",
    "        Neural network configurations\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results: dict\n",
    "        Dictionary with results for all methods\n",
    "    \"\"\"\n",
    "    # Define baseline methods\n",
    "    methods = {\n",
    "        'Unweighted': lambda a, x: np.ones(len(a)),\n",
    "    }\n",
    "    \n",
    "    # Add normal linear model for continuous treatments\n",
    "    methods['Normal-Linear'] = lambda a, x: compute_normal_linear_weights(a, x, misspecified)\n",
    "    \n",
    "    # Add non-parametric CBPS for continuous treatments\n",
    "    methods['npCBPS'] = lambda a, x: compute_np_cbps(a, x, misspecified)\n",
    "    \n",
    "    \n",
    "    # Standard trainers\n",
    "    methods['PW-GLM'] = lambda a, x: PW(\n",
    "        a, x, \n",
    "        classifier='logit', \n",
    "        estimand='ATE',\n",
    "        num_replicates=50,\n",
    "        estimand_params={'bootstrap': True}\n",
    "    )['weights']\n",
    "    \n",
    "    methods['PW-Boosting'] = lambda a, x: PW(\n",
    "        a, x, \n",
    "        classifier='boosting', \n",
    "        estimand='ATE',\n",
    "        classifier_params={'n_estimators': 100, 'max_depth': 3},\n",
    "        num_replicates=50,\n",
    "        estimand_params={'bootstrap': True}\n",
    "    )['weights']\n",
    "    \n",
    "    # SGD-based trainers if nn_configs is provided\n",
    "    if nn_configs is not None:\n",
    "        # SGD logistic regression\n",
    "        methods['PW-SGD-Logit'] = lambda a, x: PW(\n",
    "            a, x, \n",
    "            classifier='logit', \n",
    "            estimand='ATE',\n",
    "            use_sgd=True,\n",
    "            classifier_params={\n",
    "                'alpha': nn_configs.get('alpha', 0.0001),\n",
    "                'max_iter': nn_configs.get('epochs', 100),\n",
    "                'learning_rate': 'optimal'\n",
    "            },\n",
    "            num_replicates=50,\n",
    "            estimand_params={'bootstrap': True}\n",
    "        )['weights']\n",
    "        \n",
    "        # Neural network with minibatch training\n",
    "        methods['PW-Neural'] = lambda a, x: PW(\n",
    "            a, x, \n",
    "            classifier='neural_net', \n",
    "            estimand='ATE',\n",
    "            use_sgd=True,\n",
    "            batch_size=nn_configs.get('batch_size', 32),\n",
    "            classifier_params={\n",
    "                'hidden_layer_sizes': (nn_configs.get('hidden_size', 64),),\n",
    "                'max_iter': nn_configs.get('epochs', 100),\n",
    "                'batch_size': nn_configs.get('batch_size', 32),\n",
    "                'learning_rate_init': nn_configs.get('learning_rate', 0.001)\n",
    "            },\n",
    "            num_replicates=50,\n",
    "            estimand_params={'bootstrap': True}\n",
    "        )['weights']\n",
    "        \n",
    "        # PyTorch-based trainers if available\n",
    "        try:\n",
    "            # Test if torch module exists\n",
    "            import torch\n",
    "            \n",
    "            # MLP with PyTorch\n",
    "            methods['PW-Torch-MLP'] = lambda a, x: PW(\n",
    "                a, x, \n",
    "                classifier='mlp', \n",
    "                estimand='ATE',\n",
    "                use_torch=True,\n",
    "                classifier_params={\n",
    "                    'hidden_dims': [nn_configs.get('hidden_size', 64), nn_configs.get('hidden_size', 64)//2],\n",
    "                    'epochs': nn_configs.get('epochs', 100),\n",
    "                    'batch_size': nn_configs.get('batch_size', 32),\n",
    "                    'learning_rate': nn_configs.get('learning_rate', 0.001)\n",
    "                },\n",
    "                num_replicates=50,\n",
    "                estimand_params={'bootstrap': True}\n",
    "            )['weights']\n",
    "            \n",
    "            # ResNet-style model with PyTorch\n",
    "            methods['PW-Torch-ResNet'] = lambda a, x: PW(\n",
    "                a, x, \n",
    "                classifier='resnet', \n",
    "                estimand='ATE',\n",
    "                use_torch=True,\n",
    "                classifier_params={\n",
    "                    'hidden_dim': nn_configs.get('hidden_size', 64),\n",
    "                    'num_blocks': nn_configs.get('num_blocks', 2),\n",
    "                    'epochs': nn_configs.get('epochs', 100),\n",
    "                    'batch_size': nn_configs.get('batch_size', 32),\n",
    "                    'learning_rate': nn_configs.get('learning_rate', 0.001)\n",
    "                },\n",
    "                num_replicates=50,\n",
    "                estimand_params={'bootstrap': True}\n",
    "            )['weights']\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"PyTorch not available, skipping PyTorch models\")\n",
    "    \n",
    "    results = {\n",
    "        method: {\n",
    "            'integrated_bias': {size: [] for size in sample_sizes},\n",
    "            'integrated_rmse': {size: [] for size in sample_sizes}\n",
    "        } for method in methods\n",
    "    }\n",
    "    \n",
    "    for rep in range(n_replications):\n",
    "        for size in sample_sizes:\n",
    "            print(f\"Replication {rep+1}/{n_replications}, Sample size {size}\")\n",
    "            \n",
    "            # Generate data\n",
    "            df = generate_kang_schafer_continuous(n=size, seed=rep, misspecified=misspecified)\n",
    "            \n",
    "            # Apply each method\n",
    "            for method_name, method_func in methods.items():\n",
    "                try:\n",
    "                    x_features = df[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']] if misspecified else df[['X1', 'X2', 'X3', 'X4']]\n",
    "                    weights = method_func(df['A'].values, x_features)\n",
    "                    \n",
    "                    # Evaluate\n",
    "                    eval_result = evaluate_dose_response_continuous(df, weights)\n",
    "                    \n",
    "                    results[method_name]['integrated_bias'][size].append(eval_result['integrated_bias'])\n",
    "                    results[method_name]['integrated_rmse'][size].append(eval_result['integrated_rmse'])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with method {method_name}: {e}\")\n",
    "    \n",
    "    # Compute mean and std\n",
    "    summary = {\n",
    "        method: {\n",
    "            'mean_integrated_bias': {size: np.mean(results[method]['integrated_bias'][size]) for size in sample_sizes},\n",
    "            'mean_integrated_rmse': {size: np.mean(results[method]['integrated_rmse'][size]) for size in sample_sizes},\n",
    "            'std_integrated_bias': {size: np.std(results[method]['integrated_bias'][size]) for size in sample_sizes},\n",
    "            'std_integrated_rmse': {size: np.std(results[method]['integrated_rmse'][size]) for size in sample_sizes}\n",
    "        } for method in methods\n",
    "    }\n",
    "    \n",
    "    return results, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d22ebda51ac07",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26814f1ed3ed5861",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_simulation_results(summary, metric_name, title):\n",
    "    \"\"\"\n",
    "    Plot simulation results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    summary: dict\n",
    "        Summary of simulation results\n",
    "    metric_name: str\n",
    "        Metric to plot (e.g., 'mean_bias', 'mean_rmse')\n",
    "    title: str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    methods = list(summary.keys())\n",
    "    sample_sizes = list(summary[methods[0]][metric_name].keys())\n",
    "    \n",
    "    marker_styles = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'h']\n",
    "    colors = plt.cm.tab10.colors\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        values = [summary[method][metric_name][size] for size in sample_sizes]\n",
    "        std_values = [summary[method].get(f'std_{metric_name.replace(\"mean_\", \"\")}', {}).get(size, 0) for size in sample_sizes]\n",
    "        \n",
    "        plt.errorbar(\n",
    "            sample_sizes, values, yerr=std_values,\n",
    "            marker=marker_styles[i % len(marker_styles)],\n",
    "            color=colors[i % len(colors)],\n",
    "            label=method,\n",
    "            linewidth=2,\n",
    "            markersize=8,\n",
    "            capsize=5\n",
    "        )\n",
    "    \n",
    "    plt.xscale('linear')\n",
    "    plt.xlabel('Sample Size', fontsize=14)\n",
    "    plt.ylabel(metric_name.replace('_', ' ').title(), fontsize=14)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def create_comparison_plots(binary_summary, continuous_summary):\n",
    "    \"\"\"\n",
    "    Create comparison plots for binary and continuous simulations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    binary_summary: dict\n",
    "        Summary for binary treatment simulation\n",
    "    continuous_summary: dict\n",
    "        Summary for continuous treatment simulation\n",
    "    \"\"\"\n",
    "    # Create subplots for binary treatment\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Sample sizes\n",
    "    sample_sizes = list(binary_summary[list(binary_summary.keys())[0]]['mean_bias'].keys())\n",
    "    \n",
    "    # Plot settings\n",
    "    marker_styles = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'h']\n",
    "    colors = plt.cm.tab10.colors\n",
    "    \n",
    "    # Plot binary treatment results\n",
    "    for i, (method, results) in enumerate(binary_summary.items()):\n",
    "        # Bias plot\n",
    "        bias_values = [results['mean_bias'][size] for size in sample_sizes]\n",
    "        axes[0].plot(sample_sizes, bias_values, \n",
    "                    marker=marker_styles[i % len(marker_styles)],\n",
    "                    color=colors[i % len(colors)], \n",
    "                    label=method,\n",
    "                    linewidth=2,\n",
    "                    markersize=8)\n",
    "        \n",
    "        # RMSE plot\n",
    "        rmse_values = [results['mean_rmse'][size] for size in sample_sizes]\n",
    "        axes[1].plot(sample_sizes, rmse_values, \n",
    "                    marker=marker_styles[i % len(marker_styles)],\n",
    "                    color=colors[i % len(colors)], \n",
    "                    label=method,\n",
    "                    linewidth=2,\n",
    "                    markersize=8)\n",
    "    \n",
    "    # Set titles and labels\n",
    "    axes[0].set_title('Integrated Mean Absolute Bias', fontsize=16)\n",
    "    axes[1].set_title('Integrated RMSE', fontsize=16)\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('Sample Size', fontsize=14)\n",
    "        ax.set_ylabel('Metric Value', fontsize=14)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.savefig('tuning_results/binary_treatment_results.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Create subplots for continuous treatment\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot continuous treatment results\n",
    "    for i, (method, results) in enumerate(continuous_summary.items()):\n",
    "        # Bias plot\n",
    "        bias_values = [results['mean_integrated_bias'][size] for size in sample_sizes]\n",
    "        axes[0].plot(sample_sizes, bias_values, \n",
    "                    marker=marker_styles[i % len(marker_styles)],\n",
    "                    color=colors[i % len(colors)], \n",
    "                    label=method,\n",
    "                    linewidth=2,\n",
    "                    markersize=8)\n",
    "        \n",
    "        # RMSE plot\n",
    "        rmse_values = [results['mean_integrated_rmse'][size] for size in sample_sizes]\n",
    "        axes[1].plot(sample_sizes, rmse_values, \n",
    "                    marker=marker_styles[i % len(marker_styles)],\n",
    "                    color=colors[i % len(colors)], \n",
    "                    label=method,\n",
    "                    linewidth=2,\n",
    "                    markersize=8)\n",
    "    \n",
    "    # Set titles and labels\n",
    "    axes[0].set_title('Integrated Mean Absolute Bias', fontsize=16)\n",
    "    axes[1].set_title('Integrated RMSE', fontsize=16)\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('Sample Size', fontsize=14)\n",
    "        ax.set_ylabel('Metric Value', fontsize=14)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.savefig('tuning_results/continuous_treatment_results.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec46f6fd3162b8e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c81ef35bdbde289e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# RUN FULL COMPARISSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d1cd354c74d4b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run simulations and create plots\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Define sample sizes\n",
    "    #sample_sizes = [500, 1000, 1500, 2000]\n",
    "    sample_sizes = [200, 400, 700,1000]\n",
    "    \n",
    "    # Neural network configurations\n",
    "    nn_configs = {\n",
    "        'hidden_size': 64,\n",
    "        'batch_size': 32,\n",
    "        'epochs': 100\n",
    "    }\n",
    "    \n",
    "    # Run binary treatment simulation with correctly specified covariates\n",
    "    print(\"Running binary treatment simulation (correctly specified)...\")\n",
    "    binary_results, binary_summary = run_binary_simulation(\n",
    "        n_replications=1,  # Reduce for testing, use 100 for final results\n",
    "        sample_sizes=sample_sizes,\n",
    "        misspecified=False,\n",
    "        nn_configs=nn_configs\n",
    "    )\n",
    "    \n",
    "    # Run binary treatment simulation with misspecified covariates\n",
    "    print(\"Running binary treatment simulation (misspecified)...\")\n",
    "    binary_mis_results, binary_mis_summary = run_binary_simulation(\n",
    "        n_replications=1,  # Reduce for testing, use 100 for final results\n",
    "        sample_sizes=sample_sizes,\n",
    "        misspecified=True,\n",
    "        nn_configs=nn_configs\n",
    "    )\n",
    "    \n",
    "    # Run continuous treatment simulation with correctly specified covariates\n",
    "    print(\"Running continuous treatment simulation (correctly specified)...\")\n",
    "    continuous_results, continuous_summary = run_continuous_simulation(\n",
    "        n_replications=1,  # Reduce for testing, use 100 for final results\n",
    "        sample_sizes=sample_sizes,\n",
    "        misspecified=False,\n",
    "        nn_configs=nn_configs\n",
    "    )\n",
    "    \n",
    "    # Run continuous treatment simulation with misspecified covariates\n",
    "    print(\"Running continuous treatment simulation (misspecified)...\")\n",
    "    continuous_mis_results, continuous_mis_summary = run_continuous_simulation(\n",
    "        n_replications=1,  # Reduce for testing, use 100 for final results\n",
    "        sample_sizes=sample_sizes,\n",
    "        misspecified=True,\n",
    "        nn_configs=nn_configs\n",
    "    )\n",
    "    \n",
    "    # Create plots\n",
    "    print(\"Creating plots...\")\n",
    "    \n",
    "    # Binary treatment, correctly specified\n",
    "    create_comparison_plots(binary_summary, continuous_summary)\n",
    "    \n",
    "    # Save results\n",
    "    print(\"Saving results...\")\n",
    "    import pickle\n",
    "    with open('tuning_results/simulation_results.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'binary': {\n",
    "                'results': binary_results,\n",
    "                'summary': binary_summary,\n",
    "                'misspecified_results': binary_mis_results,\n",
    "                'misspecified_summary': binary_mis_summary\n",
    "            },\n",
    "            'continuous': {\n",
    "                'results': continuous_results,\n",
    "                'summary': continuous_summary,\n",
    "                'misspecified_results': continuous_mis_results,\n",
    "                'misspecified_summary': continuous_mis_summary\n",
    "            }\n",
    "        }, f)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878db4392bc8f37b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
