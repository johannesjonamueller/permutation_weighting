{
    "Standard Logistic": {
        "ate_error": 2.064383828442027,
        "in_sample_logloss": 0.72616133990491,
        "out_sample_logloss": 0.7303306609605105,
        "C": 10.0,
        "penalty": "l2",
        "solver": "saga",
        "max_iter": 1000.0,
        "n_estimators": NaN,
        "learning_rate": NaN,
        "max_depth": NaN,
        "loss": NaN,
        "alpha": NaN,
        "eta0": NaN,
        "hidden_layer_sizes": NaN,
        "activation": NaN,
        "learning_rate_init": NaN,
        "early_stopping": NaN,
        "tol": NaN,
        "l2_reg": NaN,
        "epochs": NaN,
        "hidden_dims": NaN
    },
    "Gradient Boosting": {
        "ate_error": 4.968290404657244,
        "in_sample_logloss": 0.706171454067772,
        "out_sample_logloss": 0.7395619169300192,
        "C": NaN,
        "penalty": NaN,
        "solver": NaN,
        "max_iter": NaN,
        "n_estimators": 100.0,
        "learning_rate": 0.1,
        "max_depth": 3.0,
        "loss": NaN,
        "alpha": NaN,
        "eta0": NaN,
        "hidden_layer_sizes": NaN,
        "activation": NaN,
        "learning_rate_init": NaN,
        "early_stopping": NaN,
        "tol": NaN,
        "l2_reg": NaN,
        "epochs": NaN,
        "hidden_dims": NaN
    },
    "SGD Logistic": {
        "ate_error": 16.211857892679802,
        "in_sample_logloss": 0.6962416876316538,
        "out_sample_logloss": 0.6955839440363067,
        "C": NaN,
        "penalty": "l2",
        "solver": NaN,
        "max_iter": 500.0,
        "n_estimators": NaN,
        "learning_rate": "constant",
        "max_depth": NaN,
        "loss": "log_loss",
        "alpha": 0.0001,
        "eta0": 0.01,
        "hidden_layer_sizes": NaN,
        "activation": NaN,
        "learning_rate_init": NaN,
        "early_stopping": NaN,
        "tol": NaN,
        "l2_reg": NaN,
        "epochs": NaN,
        "hidden_dims": NaN
    },
    "Neural Network": {
        "ate_error": 15.816837952629266,
        "in_sample_logloss": 0.6679910881964799,
        "out_sample_logloss": 0.6740550197480188,
        "C": NaN,
        "penalty": NaN,
        "solver": NaN,
        "max_iter": 500.0,
        "n_estimators": NaN,
        "learning_rate": NaN,
        "max_depth": NaN,
        "loss": NaN,
        "alpha": 0.001,
        "eta0": NaN,
        "hidden_layer_sizes": [
            32
        ],
        "activation": "tanh",
        "learning_rate_init": 0.001,
        "early_stopping": false,
        "tol": 0.0001,
        "l2_reg": NaN,
        "epochs": NaN,
        "hidden_dims": NaN
    },
    "Batch-Neural Network": {
        "ate_error": 16.41760750358576,
        "in_sample_logloss": 0.6596958487479901,
        "out_sample_logloss": 0.6775169762359109,
        "C": NaN,
        "penalty": NaN,
        "solver": NaN,
        "max_iter": 500.0,
        "n_estimators": NaN,
        "learning_rate": NaN,
        "max_depth": NaN,
        "loss": NaN,
        "alpha": 0.001,
        "eta0": NaN,
        "hidden_layer_sizes": [
            32
        ],
        "activation": "relu",
        "learning_rate_init": 0.001,
        "early_stopping": false,
        "tol": NaN,
        "l2_reg": NaN,
        "epochs": NaN,
        "hidden_dims": NaN
    },
    "PyTorch Logistic": {
        "ate_error": 2.2440163461619513,
        "in_sample_logloss": 0.7268335223197937,
        "out_sample_logloss": 0.7256865501403809,
        "C": NaN,
        "penalty": NaN,
        "solver": NaN,
        "max_iter": NaN,
        "n_estimators": NaN,
        "learning_rate": 0.01,
        "max_depth": NaN,
        "loss": NaN,
        "alpha": NaN,
        "eta0": NaN,
        "hidden_layer_sizes": NaN,
        "activation": NaN,
        "learning_rate_init": NaN,
        "early_stopping": NaN,
        "tol": NaN,
        "l2_reg": 1e-05,
        "epochs": 25.0,
        "hidden_dims": NaN
    },
    "PyTorch MLP": {
        "ate_error": 0.2581045704764146,
        "in_sample_logloss": 0.7195975184440613,
        "out_sample_logloss": 0.7334782481193542,
        "C": NaN,
        "penalty": NaN,
        "solver": NaN,
        "max_iter": NaN,
        "n_estimators": NaN,
        "learning_rate": 0.01,
        "max_depth": NaN,
        "loss": NaN,
        "alpha": NaN,
        "eta0": NaN,
        "hidden_layer_sizes": NaN,
        "activation": NaN,
        "learning_rate_init": NaN,
        "early_stopping": NaN,
        "tol": NaN,
        "l2_reg": 1e-05,
        "epochs": 25.0,
        "hidden_dims": "[64]"
    },
    "Batch-PyTorch Logistic": {
        "ate_error": 5.872751633034653,
        "in_sample_logloss": 0.7132115960121155,
        "out_sample_logloss": 0.7333198189735413,
        "C": NaN,
        "penalty": NaN,
        "solver": NaN,
        "max_iter": NaN,
        "n_estimators": NaN,
        "learning_rate": 0.01,
        "max_depth": NaN,
        "loss": NaN,
        "alpha": NaN,
        "eta0": NaN,
        "hidden_layer_sizes": NaN,
        "activation": NaN,
        "learning_rate_init": NaN,
        "early_stopping": NaN,
        "tol": NaN,
        "l2_reg": 0.0001,
        "epochs": 25.0,
        "hidden_dims": NaN
    },
    "Batch-PyTorch MLP": {
        "ate_error": 0.29517760780170954,
        "in_sample_logloss": 0.7011263966560364,
        "out_sample_logloss": 0.7286979556083679,
        "C": NaN,
        "penalty": NaN,
        "solver": NaN,
        "max_iter": NaN,
        "n_estimators": NaN,
        "learning_rate": 0.01,
        "max_depth": NaN,
        "loss": NaN,
        "alpha": NaN,
        "eta0": NaN,
        "hidden_layer_sizes": NaN,
        "activation": NaN,
        "learning_rate_init": NaN,
        "early_stopping": NaN,
        "tol": NaN,
        "l2_reg": 0.0001,
        "epochs": 25.0,
        "hidden_dims": NaN
    }
}