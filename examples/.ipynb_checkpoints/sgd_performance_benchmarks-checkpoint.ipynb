{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD Performance Benchmarks\n",
    "\n",
    "This notebook benchmarks the performance of SGD-based approaches for permutation weighting against standard batch methods. We'll look at:\n",
    "\n",
    "1. Training time\n",
    "2. Memory usage\n",
    "3. Estimation accuracy\n",
    "\n",
    "as a function of dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from permutation_weighting import PW\n",
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# For tracking memory usage\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / 1024 / 1024  # in MB\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data of Different Sizes\n",
    "\n",
    "We'll use the Kang-Schafer data generation process to create datasets of different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kang_schafer_data(n, misspecified=False):\n",
    "    # Generate covariates\n",
    "    X = np.random.normal(size=(n, 4))\n",
    "    \n",
    "    # Generate propensity scores\n",
    "    propensity = 1 / (1 + np.exp(X[:, 0] - 0.5 * X[:, 1] + 0.25 * X[:, 2] + 0.1 * X[:, 3]))\n",
    "    \n",
    "    # Generate treatment\n",
    "    A = np.random.binomial(1, propensity, size=n)\n",
    "    \n",
    "    # Generate outcome (true effect is 0)\n",
    "    Y = 210 + 27.4 * X[:, 0] + 13.7 * X[:, 1] + 13.7 * X[:, 2] + 13.7 * X[:, 3] + np.random.normal(size=n)\n",
    "    \n",
    "    # Apply transformation if misspecified\n",
    "    if misspecified:\n",
    "        X = np.column_stack([\n",
    "            np.exp(X[:, 0] / 2),\n",
    "            X[:, 1] * (1 + np.exp(X[:, 0])) ** (-1) + 10,\n",
    "            (X[:, 0] * X[:, 2] / 25 + 0.6) ** 3,\n",
    "            (X[:, 1] + X[:, 3] + 20) ** 2\n",
    "        ])\n",
    "    \n",
    "    return A, X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Functions\n",
    "\n",
    "Now, let's define functions to measure training time, memory usage, and estimation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_performance(n, classifier, use_sgd, batch_size=None, num_replicates=10):\n",
    "    # Generate data\n",
    "    A, X, Y = generate_kang_schafer_data(n, misspecified=True)\n",
    "    \n",
    "    # Measure memory before\n",
    "    mem_before = get_memory_usage()\n",
    "    \n",
    "    # Measure training time\n",
    "    start_time = time.time()\n",
    "    model = PW(A, X, classifier=classifier, num_replicates=num_replicates, \n",
    "              use_sgd=use_sgd, batch_size=batch_size)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Measure memory after\n",
    "    mem_after = get_memory_usage()\n",
    "    memory_used = mem_after - mem_before\n",
    "    \n",
    "    # Calculate ATE\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(A.reshape(-1, 1), Y, sample_weight=model['weights'])\n",
    "    ate = lr.coef_[0]\n",
    "    \n",
    "    # Calculate balance metrics\n",
    "    mse = model['train']['MSEEvaluator']\n",
    "    logloss = model['train']['LogLossEvaluator']\n",
    "    \n",
    "    # Free memory\n",
    "    del model\n",
    "    gc.collect()\n",
    "    \n",
    "    return {\n",
    "        'n': n,\n",
    "        'classifier': classifier,\n",
    "        'use_sgd': use_sgd,\n",
    "        'batch_size': batch_size,\n",
    "        'training_time': training_time,\n",
    "        'memory_used': memory_used,\n",
    "        'ate': ate,\n",
    "        'mse': mse,\n",
    "        'logloss': logloss\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmarks\n",
    "\n",
    "Let's run benchmarks for different dataset sizes and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset sizes to test\n",
    "sizes = [1000, 5000, 10000, 50000, 100000]\n",
    "\n",
    "# Methods to benchmark\n",
    "methods = [\n",
    "    {'classifier': 'logit', 'use_sgd': False, 'batch_size': None, 'label': 'Logistic (Batch)'},\n",
    "    {'classifier': 'logit', 'use_sgd': True, 'batch_size': None, 'label': 'Logistic (SGD)'},\n",
    "    {'classifier': 'logit', 'use_sgd': True, 'batch_size': 128, 'label': 'Logistic (Minibatch-128)'},\n",
    "    {'classifier': 'logit', 'use_sgd': True, 'batch_size': 512, 'label': 'Logistic (Minibatch-512)'},\n",
    "    {'classifier': 'neural_net', 'use_sgd': True, 'batch_size': None, 'label': 'Neural Net (SGD)'}\n",
    "]\n",
    "\n",
    "# Run benchmarks\n",
    "results = []\n",
    "\n",
    "for n in sizes:\n",
    "    for method in methods:\n",
    "        # Skip batch methods for very large datasets to avoid memory issues\n",
    "        if n > 10000 and not method['use_sgd']:\n",
    "            continue\n",
    "            \n",
    "        print(f\"Running benchmark: n={n}, method={method['label']}\")\n",
    "        result = benchmark_performance(\n",
    "            n=n, \n",
    "            classifier=method['classifier'], \n",
    "            use_sgd=method['use_sgd'], \n",
    "            batch_size=method['batch_size']\n",
    "        )\n",
    "        result['label'] = method['label']\n",
    "        results.append(result)\n",
    "        \n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n",
    "\n",
    "Let's visualize the benchmark results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot training time\n",
    "plt.subplot(2, 2, 1)\n",
    "for method in set(results_df['label']):\n",
    "    method_df = results_df[results_df['label'] == method]\n",
    "    plt.plot(method_df['n'], method_df['training_time'], marker='o', label=method)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "plt.title('Training Time vs Dataset Size')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Plot memory usage\n",
    "plt.subplot(2, 2, 2)\n",
    "for method in set(results_df['label']):\n",
    "    method_df = results_df[results_df['label'] == method]\n",
    "    plt.plot(method_df['n'], method_df['memory_used'], marker='o', label=method)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('Memory Usage (MB)')\n",
    "plt.title('Memory Usage vs Dataset Size')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Plot ATE\n",
    "plt.subplot(2, 2, 3)\n",
    "for method in set(results_df['label']):\n",
    "    method_df = results_df[results_df['label'] == method]\n",
    "    plt.plot(method_df['n'], method_df['ate'], marker='o', label=method)\n",
    "plt.axhline(y=0, color='r', linestyle='--', label='True ATE')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('ATE Estimate')\n",
    "plt.title('ATE Estimate vs Dataset Size')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# Plot balance metrics\n",
    "plt.subplot(2, 2, 4)\n",
    "for method in set(results_df['label']):\n",
    "    method_df = results_df[results_df['label'] == method]\n",
    "    plt.plot(method_df['n'], method_df['mse'], marker='o', linestyle='-', label=f\"{method} (MSE)\")\n",
    "    plt.plot(method_df['n'], method_df['logloss'], marker='x', linestyle='--', label=f\"{method} (LogLoss)\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Dataset Size')\n",
    "plt.ylabel('Balance Metric')\n",
    "plt.title('Balance Metrics vs Dataset Size')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Time vs Accuracy Tradeoff\n",
    "\n",
    "Let's look at the tradeoff between training time and estimation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Calculate absolute bias\n",
    "results_df['abs_bias'] = np.abs(results_df['ate'] - 0)\n",
    "\n",
    "for n in set(results_df['n']):\n",
    "    subset = results_df[results_df['n'] == n]\n",
    "    plt.scatter(subset['training_time'], subset['abs_bias'], s=100, label=f'n={n}')\n",
    "    \n",
    "    # Add method labels\n",
    "    for i, row in subset.iterrows():\n",
    "        plt.annotate(row['label'], \n",
    "                     (row['training_time'], row['abs_bias']),\n",
    "                     xytext=(5, 5), \n",
    "                     textcoords='offset points')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Training Time (seconds)')\n",
    "plt.ylabel('Absolute Bias')\n",
    "plt.title('Training Time vs Absolute Bias')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Usage vs Accuracy Tradeoff\n",
    "\n",
    "Let's look at the tradeoff between memory usage and estimation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for n in set(results_df['n']):\n",
    "    subset = results_df[results_df['n'] == n]\n",
    "    plt.scatter(subset['memory_used'], subset['abs_bias'], s=100, label=f'n={n}')\n",
    "    \n",
    "    # Add method labels\n",
    "    for i, row in subset.iterrows():\n",
    "        plt.annotate(row['label'], \n",
    "                     (row['memory_used'], row['abs_bias']),\n",
    "                     xytext=(5, 5), \n",
    "                     textcoords='offset points')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Memory Usage (MB)')\n",
    "plt.ylabel('Absolute Bias')\n",
    "plt.title('Memory Usage vs Absolute Bias')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This benchmark demonstrates the scalability advantages of SGD-based approaches for permutation weighting:\n",
    "\n",
    "1. **Training Time**: SGD-based methods scale much better with dataset size than batch methods. Minibatch approaches offer a good compromise between full-batch and SGD.\n",
    "\n",
    "2. **Memory Usage**: SGD-based methods use significantly less memory, making them suitable for large datasets that would otherwise be impossible to process with batch methods.\n",
    "\n",
    "3. **Estimation Accuracy**: Despite their computational advantages, SGD-based methods maintain comparable estimation accuracy to batch methods, especially as dataset size increases.\n",
    "\n",
    "4. **Balance Metrics**: The balance metrics (MSE and LogLoss) show similar trends across methods, indicating that SGD-based approaches are still effective at achieving balance.\n",
    "\n",
    "For large datasets, neural networks with SGD provide a powerful combination of scalability and flexibility, while for moderate-sized datasets, minibatch approaches offer a good balance of efficiency and accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}