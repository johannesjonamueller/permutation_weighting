{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202b8a2f78a350f2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bcb6ff34f537fbe",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-07T11:12:37.947760Z",
     "start_time": "2025-04-07T11:12:37.934377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported PW from the package\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import expit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import warnings\n",
    "\n",
    "# For better plot styling\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('colorblind')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import the permutation weighting implementation\n",
    "import sys\n",
    "sys.path.append('.')  # Add the current directory to path\n",
    "try:\n",
    "    from permutation_weighting.estimator import PW\n",
    "    print(\"Successfully imported PW from the package\")\n",
    "except ImportError:\n",
    "    print(\"Failed to import PW from the package - will implement a simplified version\")\n",
    "\n",
    "# Kang-Schafer DGP for binary treatment\n",
    "def generate_kang_schafer_binary(n=1000, seed=42, misspecified=False):\n",
    "    \"\"\"\n",
    "    Generate data according to the Kang-Schafer setup with binary treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n: int\n",
    "        Number of observations\n",
    "    seed: int\n",
    "        Random seed\n",
    "    misspecified: bool\n",
    "        Whether to return the misspecified transformations of covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df: pd.DataFrame\n",
    "        Data frame with columns: X1-X4 (covariates), A (treatment), Y (outcome), \n",
    "        Y1 (potential outcome under treatment), Y0 (potential outcome under control),\n",
    "        and X1_mis to X4_mis (misspecified covariates, if requested)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate covariates\n",
    "    X = np.random.normal(0, 1, size=(n, 4))\n",
    "    \n",
    "    # Treatment assignment\n",
    "    ps_linear = X[:, 0] - 0.5 * X[:, 1] + 0.25 * X[:, 2] + 0.1 * X[:, 3]\n",
    "    ps = expit(ps_linear)\n",
    "    A = np.random.binomial(1, ps, size=n)\n",
    "    \n",
    "    # Generate potential outcomes\n",
    "    Y1 = 210 + 1 + 27.4*X[:, 0] + 13.7*X[:, 1] + 13.7*X[:, 2] + 13.7*X[:, 3] + np.random.normal(0, 1, size=n)\n",
    "    Y0 = 210 + 0 + 27.4*X[:, 0] + 13.7*X[:, 1] + 13.7*X[:, 2] + 13.7*X[:, 3] + np.random.normal(0, 1, size=n)\n",
    "    \n",
    "    # Observed outcome\n",
    "    Y = A * Y1 + (1 - A) * Y0\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'X1': X[:, 0],\n",
    "        'X2': X[:, 1],\n",
    "        'X3': X[:, 2],\n",
    "        'X4': X[:, 3],\n",
    "        'A': A,\n",
    "        'Y': Y,\n",
    "        'Y1': Y1,\n",
    "        'Y0': Y0\n",
    "    })\n",
    "    \n",
    "    # Add misspecified covariates if requested\n",
    "    if misspecified:\n",
    "        df['X1_mis'] = np.exp(X[:, 0]/2)\n",
    "        df['X2_mis'] = X[:, 1] / (1 + np.exp(X[:, 0])) + 10\n",
    "        df['X3_mis'] = (X[:, 0] * X[:, 2] / 25 + 0.6)**3\n",
    "        df['X4_mis'] = (X[:, 1] + X[:, 3] + 20)**2\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84909c845cc3ad46",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-07T11:12:37.963523Z",
     "start_time": "2025-04-07T11:12:37.959224Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now I'll implement simplified versions of the baseline methods for comparison\n",
    "# 1. Stabilized Inverse Propensity Score Weighting (IPSW)\n",
    "def compute_ipsw_binary(A, X, misspecified=False):\n",
    "    \"\"\"\n",
    "    Compute Stabilized Inverse Propensity Score Weights for binary treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A: array-like\n",
    "        Binary treatment indicator\n",
    "    X: array-like or DataFrame\n",
    "        Covariates\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates (X*_mis) if available\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights: array\n",
    "        Stabilized IPW weights\n",
    "    \"\"\"\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if misspecified and 'X1_mis' in X.columns:\n",
    "            X_mat = X[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']].values\n",
    "        else:\n",
    "            X_mat = X[['X1', 'X2', 'X3', 'X4']].values\n",
    "    else:\n",
    "        X_mat = X\n",
    "    \n",
    "    # Fit propensity score model\n",
    "    ps_model = LogisticRegression(max_iter=1000)\n",
    "    ps_model.fit(X_mat, A)\n",
    "    \n",
    "    # Compute propensity scores\n",
    "    ps = ps_model.predict_proba(X_mat)[:, 1]\n",
    "    \n",
    "    # Marginal treatment probability\n",
    "    p_A = np.mean(A)\n",
    "    \n",
    "    # Compute stabilized weights\n",
    "    weights = np.where(A == 1, p_A / ps, (1 - p_A) / (1 - ps))\n",
    "    \n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "130a7c8e648434db",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-07T11:12:37.993550Z",
     "start_time": "2025-04-07T11:12:37.989684Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Basic implementation of CBPS (simplified version)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def compute_cbps_binary(A, X, misspecified=False):\n",
    "    \"\"\"\n",
    "    Simplified version of Covariate Balancing Propensity Score for binary treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A: array-like\n",
    "        Binary treatment indicator\n",
    "    X: array-like or DataFrame\n",
    "        Covariates\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates (X*_mis) if available\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights: array\n",
    "        CBPS weights\n",
    "    \"\"\"\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if misspecified and 'X1_mis' in X.columns:\n",
    "            X_mat = X[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']].values\n",
    "        else:\n",
    "            X_mat = X[['X1', 'X2', 'X3', 'X4']].values\n",
    "    else:\n",
    "        X_mat = X\n",
    "    \n",
    "    # This is a simplification - true CBPS adds balance constraints\n",
    "    # For demonstration, we'll just use logistic regression with L2 penalty\n",
    "    ps_model = LogisticRegression(C=0.1, max_iter=1000)\n",
    "    ps_model.fit(X_mat, A)\n",
    "    \n",
    "    # Compute propensity scores\n",
    "    ps = ps_model.predict_proba(X_mat)[:, 1]\n",
    "    \n",
    "    # Marginal treatment probability\n",
    "    p_A = np.mean(A)\n",
    "    \n",
    "    # Compute weights\n",
    "    weights = np.where(A == 1, p_A / ps, (1 - p_A) / (1 - ps))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae02852a64a80fc0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-07T11:12:38.011367Z",
     "start_time": "2025-04-07T11:12:38.007074Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_ipsw_gbm(A, X, misspecified=False):\n",
    "    \"\"\"\n",
    "    Compute IPSW weights using gradient boosting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A: array-like\n",
    "        Treatment indicator\n",
    "    X: array-like or DataFrame\n",
    "        Covariates\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights: array\n",
    "        IPSW weights\n",
    "    \"\"\"\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if misspecified and 'X1_mis' in X.columns:\n",
    "            X_mat = X[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']].values\n",
    "        else:\n",
    "            X_mat = X[['X1', 'X2', 'X3', 'X4']].values\n",
    "    else:\n",
    "        X_mat = X\n",
    "    \n",
    "    # Fit propensity score model with GBM\n",
    "    gbm = GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
    "    gbm.fit(X_mat, A)\n",
    "    \n",
    "    # Compute propensity scores\n",
    "    ps = gbm.predict_proba(X_mat)[:, 1]\n",
    "    \n",
    "    # Clip propensity scores to avoid extreme weights\n",
    "    ps = np.clip(ps, 0.01, 0.99)\n",
    "    \n",
    "    # Marginal treatment probability\n",
    "    p_A = np.mean(A)\n",
    "    \n",
    "    # Compute stabilized weights\n",
    "    weights = np.where(A == 1, p_A / ps, (1 - p_A) / (1 - ps))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfad52ec702a1dcf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-07T11:12:38.018006Z",
     "start_time": "2025-04-07T11:12:38.013046Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_sbw_binary(A, X, misspecified=False):\n",
    "    \"\"\"\n",
    "    Simplified version of Stabilized Balancing Weights for binary treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A: array-like\n",
    "        Binary treatment indicator\n",
    "    X: array-like or DataFrame\n",
    "        Covariates\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights: array\n",
    "        SBW weights\n",
    "    \"\"\"\n",
    "    import cvxpy as cp # TODO: add cvxpy to requirements iff used in the package\n",
    "    \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if misspecified and 'X1_mis' in X.columns:\n",
    "            X_mat = X[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']].values\n",
    "        else:\n",
    "            X_mat = X[['X1', 'X2', 'X3', 'X4']].values\n",
    "    else:\n",
    "        X_mat = X\n",
    "    \n",
    "    n = len(A)\n",
    "    n_treated = np.sum(A)\n",
    "    n_control = n - n_treated\n",
    "    \n",
    "    # Separate covariates for treated and control\n",
    "    X_treated = X_mat[A == 1]\n",
    "    X_control = X_mat[A == 0]\n",
    "    \n",
    "    # Calculate means\n",
    "    treated_mean = np.mean(X_treated, axis=0)\n",
    "    \n",
    "    # Initialize weights for control units\n",
    "    w = cp.Variable(n_control, nonneg=True)\n",
    "    \n",
    "    # Balance constraint: weighted mean of control features equals mean of treated features\n",
    "    balance_constraint = []\n",
    "    for j in range(X_mat.shape[1]):\n",
    "        # Allow small imbalance (delta)\n",
    "        delta = 0.1 * np.std(X_mat[:, j])\n",
    "        balance_constraint.append(cp.abs(cp.sum(cp.multiply(w, X_control[:, j])) - treated_mean[j] * cp.sum(w)) <= delta)\n",
    "    \n",
    "    # Sum constraint\n",
    "    balance_constraint.append(cp.sum(w) == 1)\n",
    "    \n",
    "    # Objective: minimize variance\n",
    "    objective = cp.Minimize(cp.sum_squares(w - 1/n_control))\n",
    "    \n",
    "    # Solve optimization problem\n",
    "    prob = cp.Problem(objective, balance_constraint)\n",
    "    try:\n",
    "        prob.solve(solver=cp.OSQP)\n",
    "    except:\n",
    "        try:\n",
    "            prob.solve(solver=cp.ECOS)\n",
    "        except:\n",
    "            prob.solve(solver=cp.SCS)\n",
    "    \n",
    "    # Create final weights vector\n",
    "    weights = np.ones(n)\n",
    "    weights[A == 0] = w.value * n_control\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02458b6a92eceda",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# EVALUATE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e64d2d6f37081070",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-07T11:12:38.024726Z",
     "start_time": "2025-04-07T11:12:38.019669Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_ate_binary(df, weights, true_ate=1.0):\n",
    "    \"\"\"\n",
    "    Evaluate ATE estimation for binary treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: DataFrame\n",
    "        Data with 'A', 'Y', 'Y1', 'Y0' columns\n",
    "    weights: array-like\n",
    "        Weights for each observation\n",
    "    true_ate: float\n",
    "        True average treatment effect\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with bias and rmse\n",
    "    \"\"\"\n",
    "    # Calculate weighted ATE\n",
    "    treated_idx = df['A'] == 1\n",
    "    control_idx = df['A'] == 0\n",
    "    \n",
    "    treated_mean = np.sum(df.loc[treated_idx, 'Y'] * weights[treated_idx]) / np.sum(weights[treated_idx])\n",
    "    control_mean = np.sum(df.loc[control_idx, 'Y'] * weights[control_idx]) / np.sum(weights[control_idx])\n",
    "    \n",
    "    estimated_ate = treated_mean - control_mean\n",
    "    \n",
    "    error = estimated_ate - true_ate\n",
    "    \n",
    "    return error\n",
    "\n",
    "def evaluate_dose_response_continuous(df, weights, treatment_grid=None):\n",
    "    \"\"\"\n",
    "    Evaluate dose-response estimation for continuous treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: DataFrame\n",
    "        Data with 'A', 'Y', 'true_dr' columns\n",
    "    weights: array-like\n",
    "        Weights for each observation\n",
    "    treatment_grid: array-like, optional\n",
    "        Grid of treatment values for evaluation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with integrated bias and rmse\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # Define treatment grid if not provided\n",
    "    if treatment_grid is None:\n",
    "        min_a, max_a = np.percentile(df['A'], [5, 95])\n",
    "        treatment_grid = np.linspace(min_a, max_a, 50)\n",
    "    \n",
    "    # True dose-response function\n",
    "    true_dr = [210 + 1/(1 + np.exp(a)) for a in treatment_grid]\n",
    "    \n",
    "    # Estimate dose-response function using weighted local linear regression\n",
    "    est_dr = []\n",
    "    for a in treatment_grid:\n",
    "        # Calculate kernel weights\n",
    "        bandwidth = (np.percentile(df['A'], 75) - np.percentile(df['A'], 25)) / 1.34\n",
    "        kernel_weights = np.exp(-0.5 * ((df['A'] - a) / bandwidth)**2) * weights\n",
    "        \n",
    "        # Fit weighted linear regression\n",
    "        model = LinearRegression()\n",
    "        model.fit(\n",
    "            df[['A']], \n",
    "            df['Y'],\n",
    "            sample_weight=kernel_weights\n",
    "        )\n",
    "        \n",
    "        # Predict at treatment value a\n",
    "        est_dr.append(model.predict([[a]])[0])\n",
    "    \n",
    "    # Calculate integrated bias and RMSE\n",
    "    bias = np.mean(np.abs(np.array(est_dr) - np.array(true_dr)))\n",
    "    rmse = np.sqrt(np.mean((np.array(est_dr) - np.array(true_dr))**2))\n",
    "    \n",
    "    return {\n",
    "        'integrated_bias': bias,\n",
    "        'integrated_rmse': rmse,\n",
    "        'treatment_grid': treatment_grid,\n",
    "        'estimated_dr': est_dr,\n",
    "        'true_dr': true_dr\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9780b71f3e9338",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# SIMULATION BINARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "233387ec5f5bee29",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-07T11:12:38.039250Z",
     "start_time": "2025-04-07T11:12:38.029114Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_binary_simulation(n_replications=100, sample_sizes=[500, 1000, 1500, 2000], \n",
    "                         misspecified=False, nn_configs=None):\n",
    "    \"\"\"\n",
    "    Run simulation for binary treatment methods\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_replications: int\n",
    "        Number of simulation replications\n",
    "    sample_sizes: list\n",
    "        List of sample sizes to test\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates\n",
    "    nn_configs: dict, optional\n",
    "        Neural network configurations\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results: dict\n",
    "        Dictionary with results for all methods\n",
    "    \"\"\"\n",
    "    # Define baseline methods\n",
    "    methods = {\n",
    "        'Unweighted': lambda a, x: np.ones(len(a)),\n",
    "        'IPSW-GLM': lambda a, x: compute_ipsw_binary(a, x, misspecified),\n",
    "        'IPSW-GBM': lambda a, x: compute_ipsw_gbm(a, x, misspecified),\n",
    "        'CBPS': lambda a, x: compute_cbps_binary(a, x, misspecified),\n",
    "        'SBW': lambda a, x: compute_sbw_binary(a, x, misspecified)\n",
    "    }\n",
    "    \n",
    "    # Standard trainers\n",
    "    methods['PW-GLM'] = lambda a, x: PW(\n",
    "        a, x, \n",
    "        classifier='logit', \n",
    "        estimand='ATE',\n",
    "        num_replicates=50,\n",
    "        estimand_params={'bootstrap': True}\n",
    "    )['weights']\n",
    "    \n",
    "    methods['PW-Boosting'] = lambda a, x: PW(\n",
    "        a, x, \n",
    "        classifier='boosting', \n",
    "        estimand='ATE',\n",
    "        classifier_params={'n_estimators': 100, 'max_depth': 3},\n",
    "        num_replicates=50,\n",
    "        estimand_params={'bootstrap': True}\n",
    "    )['weights']\n",
    "    \n",
    "    # SGD-based trainers if nn_configs is provided\n",
    "    if nn_configs is not None:\n",
    "        # SGD logistic regression\n",
    "        methods['PW-SGD-Logit'] = lambda a, x: PW(\n",
    "            a, x, \n",
    "            classifier='logit', \n",
    "            estimand='ATE',\n",
    "            use_sgd=True,\n",
    "            classifier_params={\n",
    "                'alpha': nn_configs.get('alpha', 0.0001),\n",
    "                'max_iter': nn_configs.get('epochs', 100),\n",
    "                'learning_rate': 'optimal'\n",
    "            },\n",
    "            num_replicates=1, \n",
    "            estimand_params={'bootstrap': True}\n",
    "        )['weights']\n",
    "        \n",
    "        # Neural network\n",
    "        methods['PW-Neural'] = lambda a, x: PW(\n",
    "            a, x, \n",
    "            classifier='neural_net', \n",
    "            estimand='ATE',\n",
    "            use_sgd=True,\n",
    "            classifier_params={\n",
    "                'hidden_layer_sizes': (nn_configs.get('hidden_size', 64),),\n",
    "                'max_iter': nn_configs.get('epochs', 100),\n",
    "                'batch_size': nn_configs.get('batch_size', 32),\n",
    "                'learning_rate_init': nn_configs.get('learning_rate', 0.001)\n",
    "            },\n",
    "            num_replicates=1,\n",
    "            estimand_params={'bootstrap': True}\n",
    "        )['weights']\n",
    "        \n",
    "        # Batch-then-permute SGD approach\n",
    "        methods['PW-Batch-SGD'] = lambda a, x: PW(\n",
    "            a, x, \n",
    "            classifier='logit', \n",
    "            estimand='ATE',\n",
    "            use_sgd=True,\n",
    "            batch_size=nn_configs.get('batch_size', 32),\n",
    "            classifier_params={\n",
    "                'alpha': nn_configs.get('alpha', 0.0001),\n",
    "                'max_iter': nn_configs.get('epochs', 100),\n",
    "                'learning_rate': 'optimal'\n",
    "            },\n",
    "            num_replicates=1,  # Only 1 replicate since permutation happens in batches\n",
    "            estimand_params={'bootstrap': False}\n",
    "        )['weights']\n",
    "        \n",
    "        # PyTorch-based trainers if available\n",
    "        try:\n",
    "            # Test if torch module exists\n",
    "            import torch\n",
    "            \n",
    "            # MLP with PyTorch\n",
    "            methods['PW-Torch-MLP'] = lambda a, x: PW(\n",
    "                a, x, \n",
    "                classifier='mlp', \n",
    "                estimand='ATE',\n",
    "                use_torch=True,\n",
    "                classifier_params={\n",
    "                    'hidden_dims': [nn_configs.get('hidden_size', 64), nn_configs.get('hidden_size', 64)//2],\n",
    "                    'epochs': nn_configs.get('epochs', 100),\n",
    "                    'batch_size': nn_configs.get('batch_size', 32),\n",
    "                    'learning_rate': nn_configs.get('learning_rate', 0.001)\n",
    "                },\n",
    "                num_replicates=1,\n",
    "                estimand_params={'bootstrap': True}\n",
    "            )['weights']\n",
    "            \n",
    "            # Batch-then-permute PyTorch approach\n",
    "            methods['PW-Batch-Torch'] = lambda a, x: PW(\n",
    "                a, x, \n",
    "                classifier='mlp', \n",
    "                estimand='ATE',\n",
    "                use_torch=True,\n",
    "                batch_size=nn_configs.get('batch_size', 32),\n",
    "                classifier_params={\n",
    "                    'hidden_dims': [nn_configs.get('hidden_size', 64), nn_configs.get('hidden_size', 64)//2],\n",
    "                    'epochs': nn_configs.get('epochs', 100),\n",
    "                    'learning_rate': nn_configs.get('learning_rate', 0.001)\n",
    "                },\n",
    "                num_replicates=1,  # Only 1 replicate since permutation happens in batches\n",
    "                estimand_params={'bootstrap': False}\n",
    "            )['weights']\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"PyTorch not available, skipping PyTorch models\")\n",
    "    \n",
    "    # Initialize results dictionary - store raw errors instead of bias/rmse\n",
    "    results = {\n",
    "        method: {\n",
    "            'error': {size: [] for size in sample_sizes}\n",
    "        } for method in methods\n",
    "    }\n",
    "    \n",
    "    for rep in range(n_replications):\n",
    "        for size in sample_sizes:\n",
    "            print(f\"Replication {rep+1}/{n_replications}, Sample size {size}\")\n",
    "            \n",
    "            # Generate data\n",
    "            df = generate_kang_schafer_binary(n=size, seed=rep, misspecified=misspecified)\n",
    "            \n",
    "            # Apply each method\n",
    "            for method_name, method_func in methods.items():\n",
    "                try:\n",
    "                    x_features = df[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']] if misspecified else df[['X1', 'X2', 'X3', 'X4']]\n",
    "                    weights = method_func(df['A'].values, x_features)\n",
    "                    \n",
    "                    # Evaluate - now returns raw error\n",
    "                    error = evaluate_ate_binary(df, weights)\n",
    "                    \n",
    "                    # Store raw error\n",
    "                    results[method_name]['error'][size].append(error)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with method {method_name}: {e}\")\n",
    "    \n",
    "    # Compute mean and std with corrected formulas\n",
    "    summary = {\n",
    "        method: {\n",
    "            'mean_bias': {size: np.abs(np.mean(results[method]['error'][size])) for size in sample_sizes},\n",
    "            'mean_mse': {size: np.mean(np.power(results[method]['error'][size], 2)) for size in sample_sizes},\n",
    "            'std_bias': {size: np.std(results[method]['error'][size]) / np.sqrt(n_replications) for size in sample_sizes},\n",
    "            'std_rmse': {size: np.std(np.power(results[method]['error'][size], 2)) / np.sqrt(n_replications) for size in sample_sizes}\n",
    "        } for method in methods\n",
    "    }\n",
    "    \n",
    "    return results, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b12a885a7b9d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CONTIOUS  TREAMTMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3838de4c21ba1b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data generation for continuous treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d56aa7072214f58",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-07T11:12:38.049790Z",
     "start_time": "2025-04-07T11:12:38.044626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Kang-Schafer DGP for continuous treatment\n",
    "def generate_kang_schafer_continuous(n=1000, seed=42, misspecified=False):\n",
    "    \"\"\"\n",
    "    Generate data according to the Kang-Schafer setup with continuous treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n: int\n",
    "        Number of observations\n",
    "    seed: int\n",
    "        Random seed\n",
    "    misspecified: bool\n",
    "        Whether to return the misspecified transformations of covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df: pd.DataFrame\n",
    "        Data frame with columns: X1-X4 (covariates), A (treatment), Y (outcome),\n",
    "        and X1_mis to X4_mis (misspecified covariates, if requested)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate covariates\n",
    "    X = np.random.normal(0, 1, size=(n, 4))\n",
    "    \n",
    "    # Treatment assignment (linear with noise)\n",
    "    A_linear = X[:, 0] - 0.5 * X[:, 1] + 0.25 * X[:, 2] + 0.1 * X[:, 3]\n",
    "    A = A_linear + np.random.normal(0, 1, size=n)\n",
    "    \n",
    "    # Generate outcome with non-linear treatment effect\n",
    "    Y = 210 + 1/(1 + np.exp(A)) + 27.4*X[:, 0] + 13.7*X[:, 1] + 13.7*X[:, 2] + 13.7*X[:, 3] + np.random.normal(0, 1, size=n)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'X1': X[:, 0],\n",
    "        'X2': X[:, 1],\n",
    "        'X3': X[:, 2],\n",
    "        'X4': X[:, 3],\n",
    "        'A': A,\n",
    "        'Y': Y\n",
    "    })\n",
    "    \n",
    "    # True dose-response function (for evaluation)\n",
    "    def true_dose_response(a):\n",
    "        return 210 + 1/(1 + np.exp(a)) + 27.4 * 0 + 13.7 * 0 + 13.7 * 0 + 13.7 * 0  # Expectation of X is 0\n",
    "    \n",
    "    df['true_dr'] = [true_dose_response(a) for a in A]\n",
    "    \n",
    "    # Add misspecified covariates if requested\n",
    "    if misspecified:\n",
    "        df['X1_mis'] = np.exp(X[:, 0]/2)\n",
    "        df['X2_mis'] = X[:, 1] / (1 + np.exp(X[:, 0])) + 10\n",
    "        df['X3_mis'] = (X[:, 0] * X[:, 2] / 25 + 0.6)**3\n",
    "        df['X4_mis'] = (X[:, 1] + X[:, 3] + 20)**2\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Check if our data generation works\n",
    "# binary_df = generate_kang_schafer_binary(n=1000, seed=42, misspecified=True)\n",
    "# print(\"Binary treatment data (first 5 rows):\")\n",
    "# print(binary_df.iloc[:5, :10])  # Show first 5 rows and 10 columns\n",
    "# \n",
    "# continuous_df = generate_kang_schafer_continuous(n=1000, seed=42, misspecified=True)\n",
    "# print(\"\\nContinuous treatment data (first 5 rows):\")\n",
    "# print(continuous_df.iloc[:5, :10])  # Show first 5 rows and 10 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80022a55aa7edd8a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# other methods for continuous treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c646ad3f6c4c3d2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-07T11:12:38.058663Z",
     "start_time": "2025-04-07T11:12:38.053556Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_normal_linear_weights(A, X, misspecified=False):\n",
    "    \"\"\"\n",
    "    Compute weights for continuous treatments using a normal-linear model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A: array-like\n",
    "        Continuous treatment variable\n",
    "    X: array-like or DataFrame\n",
    "        Covariates\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights: array\n",
    "        Stabilized weights\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from scipy.stats import norm\n",
    "    \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if misspecified and 'X1_mis' in X.columns:\n",
    "            X_mat = X[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']].values\n",
    "        else:\n",
    "            X_mat = X[['X1', 'X2', 'X3', 'X4']].values\n",
    "    else:\n",
    "        X_mat = X\n",
    "    \n",
    "    # Fit linear regression for treatment given covariates\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_mat, A)\n",
    "    \n",
    "    # Predict treatment and calculate residuals\n",
    "    A_pred = model.predict(X_mat)\n",
    "    resid = A - A_pred\n",
    "    \n",
    "    # Estimate residual variance\n",
    "    sigma = np.std(resid)\n",
    "    \n",
    "    # Compute likelihood of observed treatment under model\n",
    "    pdf_cond = norm.pdf(A, loc=A_pred, scale=sigma)\n",
    "    \n",
    "    # Compute likelihood under marginal distribution\n",
    "    pdf_marg = norm.pdf(A, loc=np.mean(A), scale=np.std(A))\n",
    "    \n",
    "    # Compute stabilized weights\n",
    "    weights = pdf_marg / pdf_cond\n",
    "    \n",
    "    # Clip extreme weights\n",
    "    weights = np.clip(weights, 0.01, 100)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def compute_np_cbps(A, X, misspecified=False):\n",
    "    \"\"\"\n",
    "    Compute non-parametric Covariate Balancing Propensity Score weights for continuous treatments\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A: array-like\n",
    "        Continuous treatment variable\n",
    "    X: array-like or DataFrame\n",
    "        Covariates\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights: array\n",
    "        CBPS weights\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LassoCV\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if misspecified and 'X1_mis' in X.columns:\n",
    "            X_mat = X[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']].values\n",
    "        else:\n",
    "            X_mat = X[['X1', 'X2', 'X3', 'X4']].values\n",
    "    else:\n",
    "        X_mat = X\n",
    "    \n",
    "    # Create polynomial features to approximate non-parametric model\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=True)\n",
    "    X_poly = poly.fit_transform(X_mat)\n",
    "    \n",
    "    # Fit Lasso model with cross-validation for regularization\n",
    "    model = LassoCV(cv=5, max_iter=2000, random_state=42)\n",
    "    model.fit(X_poly, A)\n",
    "    \n",
    "    # Compute residuals\n",
    "    A_pred = model.predict(X_poly)\n",
    "    resid = A - A_pred\n",
    "    \n",
    "    # Estimate density ratio using balancing constraint approach\n",
    "    # This is a simplified approximation to npCBPS\n",
    "    \n",
    "    # Compute covariance between residuals and covariate functions\n",
    "    cov_mat = np.zeros((X_poly.shape[1], 1))\n",
    "    for j in range(X_poly.shape[1]):\n",
    "        cov_mat[j, 0] = np.cov(X_poly[:, j], resid)[0, 1]\n",
    "    \n",
    "    # Compute weights using exponential tilting\n",
    "    lambda_param = np.linalg.solve(np.cov(X_poly, rowvar=False) + 0.001 * np.eye(X_poly.shape[1]), cov_mat)\n",
    "    scores = X_poly @ lambda_param\n",
    "    \n",
    "    # Apply exponential tilting and normalize\n",
    "    weights = np.exp(scores - np.mean(scores))\n",
    "    weights = weights / np.mean(weights) * len(A)\n",
    "    \n",
    "    # Clip extreme weights\n",
    "    weights = np.clip(weights, 0.01, 100)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303fe2218ad77093",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# continuous treatment simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c7ac279e4154406",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-07T11:12:38.069075Z",
     "start_time": "2025-04-07T11:12:38.059940Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_continuous_simulation(n_replications=100, sample_sizes=[500, 1000, 1500, 2000], \n",
    "                             misspecified=False, nn_configs=None):\n",
    "    \"\"\"\n",
    "    Run simulation for continuous treatment methods\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_replications: int\n",
    "        Number of simulation replications\n",
    "    sample_sizes: list\n",
    "        List of sample sizes to test\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates\n",
    "    nn_configs: dict, optional\n",
    "        Neural network configurations\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results: dict\n",
    "        Dictionary with results for all methods\n",
    "    \"\"\"\n",
    "    # Define baseline methods\n",
    "    methods = {\n",
    "        'Unweighted': lambda a, x: np.ones(len(a)),\n",
    "    }\n",
    "    \n",
    "    # Add normal linear model for continuous treatments\n",
    "    methods['Normal-Linear'] = lambda a, x: compute_normal_linear_weights(a, x, misspecified)\n",
    "    \n",
    "    # Add non-parametric CBPS for continuous treatments\n",
    "    methods['npCBPS'] = lambda a, x: compute_np_cbps(a, x, misspecified)\n",
    "    \n",
    "    # Standard trainers\n",
    "    methods['PW-GLM'] = lambda a, x: PW(\n",
    "        a, x, \n",
    "        classifier='logit', \n",
    "        estimand='ATE',\n",
    "        num_replicates=50,\n",
    "        estimand_params={'bootstrap': True}\n",
    "    )['weights']\n",
    "    \n",
    "    methods['PW-Boosting'] = lambda a, x: PW(\n",
    "        a, x, \n",
    "        classifier='boosting', \n",
    "        estimand='ATE',\n",
    "        classifier_params={'n_estimators': 100, 'max_depth': 3},\n",
    "        num_replicates=50,\n",
    "        estimand_params={'bootstrap': True}\n",
    "    )['weights']\n",
    "    \n",
    "    # SGD-based trainers if nn_configs is provided\n",
    "    if nn_configs is not None:\n",
    "        # SGD logistic regression\n",
    "        methods['PW-SGD-Logit'] = lambda a, x: PW(\n",
    "            a, x, \n",
    "            classifier='logit', \n",
    "            estimand='ATE',\n",
    "            use_sgd=True,\n",
    "            classifier_params={\n",
    "                'alpha': nn_configs.get('alpha', 0.0001),\n",
    "                'max_iter': nn_configs.get('epochs', 100),\n",
    "                'learning_rate': 'optimal'\n",
    "            },\n",
    "            num_replicates=1,\n",
    "            estimand_params={'bootstrap': True}\n",
    "        )['weights']\n",
    "        \n",
    "        # Neural network with minibatch training\n",
    "        methods['PW-Neural'] = lambda a, x: PW(\n",
    "            a, x, \n",
    "            classifier='neural_net', \n",
    "            estimand='ATE',\n",
    "            use_sgd=True,\n",
    "            classifier_params={\n",
    "                'hidden_layer_sizes': (nn_configs.get('hidden_size', 64),),\n",
    "                'max_iter': nn_configs.get('epochs', 100),\n",
    "                'batch_size': nn_configs.get('batch_size', 32),\n",
    "                'learning_rate_init': nn_configs.get('learning_rate', 0.001)\n",
    "            },\n",
    "            num_replicates=1,\n",
    "            estimand_params={'bootstrap': True}\n",
    "        )['weights']\n",
    "        \n",
    "        # Batch-then-permute SGD approach\n",
    "        methods['PW-Batch-SGD'] = lambda a, x: PW(\n",
    "            a, x, \n",
    "            classifier='logit', \n",
    "            estimand='ATE',\n",
    "            use_sgd=True,\n",
    "            batch_size=nn_configs.get('batch_size', 32),\n",
    "            classifier_params={\n",
    "                'alpha': nn_configs.get('alpha', 0.0001),\n",
    "                'max_iter': nn_configs.get('epochs', 100),\n",
    "                'learning_rate': 'optimal'\n",
    "            },\n",
    "            num_replicates=1,  # Only 1 replicate since permutation happens in batches\n",
    "            estimand_params={'bootstrap': False}\n",
    "        )['weights']\n",
    "        \n",
    "        # PyTorch-based trainers if available\n",
    "        try:\n",
    "            # Test if torch module exists\n",
    "            import torch\n",
    "            \n",
    "            # MLP with PyTorch\n",
    "            methods['PW-Torch-MLP'] = lambda a, x: PW(\n",
    "                a, x, \n",
    "                classifier='mlp', \n",
    "                estimand='ATE',\n",
    "                use_torch=True,\n",
    "                classifier_params={\n",
    "                    'hidden_dims': [nn_configs.get('hidden_size', 64), nn_configs.get('hidden_size', 64)//2],\n",
    "                    'epochs': nn_configs.get('epochs', 100),\n",
    "                    'batch_size': nn_configs.get('batch_size', 32),\n",
    "                    'learning_rate': nn_configs.get('learning_rate', 0.001)\n",
    "                },\n",
    "                num_replicates=1,\n",
    "                estimand_params={'bootstrap': True}\n",
    "            )['weights']\n",
    "            \n",
    "            # Batch-then-permute PyTorch approach\n",
    "            methods['PW-Batch-Torch'] = lambda a, x: PW(\n",
    "                a, x, \n",
    "                classifier='mlp', \n",
    "                estimand='ATE',\n",
    "                use_torch=True,\n",
    "                batch_size=nn_configs.get('batch_size', 32),\n",
    "                classifier_params={\n",
    "                    'hidden_dims': [nn_configs.get('hidden_size', 64), nn_configs.get('hidden_size', 64)//2],\n",
    "                    'epochs': nn_configs.get('epochs', 100),\n",
    "                    'learning_rate': nn_configs.get('learning_rate', 0.001)\n",
    "                },\n",
    "                num_replicates=1,  # Only 1 replicate since permutation happens in batches\n",
    "                estimand_params={'bootstrap': False}\n",
    "            )['weights']\n",
    "            \n",
    "            # ResNet-style model with PyTorch\n",
    "            methods['PW-Torch-ResNet'] = lambda a, x: PW(\n",
    "                a, x, \n",
    "                classifier='resnet', \n",
    "                estimand='ATE',\n",
    "                use_torch=True,\n",
    "                classifier_params={\n",
    "                    'hidden_dim': nn_configs.get('hidden_size', 64),\n",
    "                    'num_blocks': nn_configs.get('num_blocks', 2),\n",
    "                    'epochs': nn_configs.get('epochs', 100),\n",
    "                    'batch_size': nn_configs.get('batch_size', 32),\n",
    "                    'learning_rate': nn_configs.get('learning_rate', 0.001)\n",
    "                },\n",
    "                num_replicates=1,\n",
    "                estimand_params={'bootstrap': True}\n",
    "            )['weights']\n",
    "            \n",
    "            # Batch-then-permute ResNet PyTorch approach\n",
    "            methods['PW-Batch-Torch-ResNet'] = lambda a, x: PW(\n",
    "                a, x, \n",
    "                classifier='resnet', \n",
    "                estimand='ATE',\n",
    "                use_torch=True,\n",
    "                batch_size=nn_configs.get('batch_size', 32),\n",
    "                classifier_params={\n",
    "                    'hidden_dim': nn_configs.get('hidden_size', 64),\n",
    "                    'num_blocks': nn_configs.get('num_blocks', 2),\n",
    "                    'epochs': nn_configs.get('epochs', 100),\n",
    "                    'learning_rate': nn_configs.get('learning_rate', 0.001)\n",
    "                },\n",
    "                num_replicates=1,  # Only 1 replicate since permutation happens in batches\n",
    "                estimand_params={'bootstrap': False}\n",
    "            )['weights']\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"PyTorch not available, skipping PyTorch models\")\n",
    "    \n",
    "    # Initialize results dictionary with appropriate structure for continuous outcomes\n",
    "    results = {\n",
    "        method: {\n",
    "            'integrated_bias': {size: [] for size in sample_sizes},\n",
    "            'integrated_rmse': {size: [] for size in sample_sizes}\n",
    "        } for method in methods\n",
    "    }\n",
    "    \n",
    "    for rep in range(n_replications):\n",
    "        for size in sample_sizes:\n",
    "            print(f\"Replication {rep+1}/{n_replications}, Sample size {size}\")\n",
    "            \n",
    "            # Generate data\n",
    "            df = generate_kang_schafer_continuous(n=size, seed=rep, misspecified=misspecified)\n",
    "            \n",
    "            # Apply each method\n",
    "            for method_name, method_func in methods.items():\n",
    "                try:\n",
    "                    x_features = df[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']] if misspecified else df[['X1', 'X2', 'X3', 'X4']]\n",
    "                    weights = method_func(df['A'].values, x_features)\n",
    "                    \n",
    "                    # Evaluate\n",
    "                    eval_result = evaluate_dose_response_continuous(df, weights)\n",
    "                    \n",
    "                    results[method_name]['integrated_bias'][size].append(eval_result['integrated_bias'])\n",
    "                    results[method_name]['integrated_rmse'][size].append(eval_result['integrated_rmse'])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with method {method_name}: {e}\")\n",
    "    \n",
    "    # Compute mean and std\n",
    "    summary = {\n",
    "        method: {\n",
    "            'mean_integrated_bias': {size: np.mean(results[method]['integrated_bias'][size]) for size in sample_sizes},\n",
    "            'mean_integrated_rmse': {size: np.mean(results[method]['integrated_rmse'][size]) for size in sample_sizes},\n",
    "            'std_integrated_bias': {size: np.std(results[method]['integrated_bias'][size]) / np.sqrt(n_replications) for size in sample_sizes},\n",
    "            'std_integrated_rmse': {size: np.std(results[method]['integrated_rmse'][size]) / np.sqrt(n_replications) for size in sample_sizes}\n",
    "        } for method in methods\n",
    "    }\n",
    "    \n",
    "    return results, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d22ebda51ac07",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26814f1ed3ed5861",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-07T11:12:38.077274Z",
     "start_time": "2025-04-07T11:12:38.070372Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_simulation_results(summary, metric_name, title):\n",
    "    \"\"\"\n",
    "    Plot simulation results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    summary: dict\n",
    "        Summary of simulation results\n",
    "    metric_name: str\n",
    "        Metric to plot (e.g., 'mean_bias', 'mean_rmse')\n",
    "    title: str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    methods = list(summary.keys())\n",
    "    sample_sizes = list(summary[methods[0]][metric_name].keys())\n",
    "    \n",
    "    marker_styles = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'h']\n",
    "    colors = plt.cm.tab10.colors\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        values = [summary[method][metric_name][size] for size in sample_sizes]\n",
    "        std_values = [summary[method].get(f'std_{metric_name.replace(\"mean_\", \"\")}', {}).get(size, 0) for size in sample_sizes]\n",
    "        \n",
    "        plt.errorbar(\n",
    "            sample_sizes, values, yerr=std_values,\n",
    "            marker=marker_styles[i % len(marker_styles)],\n",
    "            color=colors[i % len(colors)],\n",
    "            label=method,\n",
    "            linewidth=2,\n",
    "            markersize=8,\n",
    "            capsize=5\n",
    "        )\n",
    "    \n",
    "    plt.xscale('linear')\n",
    "    plt.xlabel('Sample Size', fontsize=14)\n",
    "    plt.ylabel(metric_name.replace('_', ' ').title(), fontsize=14)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def create_comparison_plots(binary_summary, continuous_summary):\n",
    "    \"\"\"\n",
    "    Create comparison plots for binary and continuous simulations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    binary_summary: dict\n",
    "        Summary for binary treatment simulation\n",
    "    continuous_summary: dict\n",
    "        Summary for continuous treatment simulation\n",
    "    \"\"\"\n",
    "    # Create subplots for binary treatment\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Sample sizes\n",
    "    sample_sizes = list(binary_summary[list(binary_summary.keys())[0]]['mean_bias'].keys())\n",
    "    \n",
    "    # Plot settings\n",
    "    marker_styles = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'h', 's','c']\n",
    "    colors = plt.cm.tab12.colors\n",
    "    \n",
    "    # Plot binary treatment results\n",
    "    for i, (method, results) in enumerate(binary_summary.items()):\n",
    "        # Bias plot\n",
    "        bias_values = [results['mean_bias'][size] for size in sample_sizes]\n",
    "        axes[0].plot(sample_sizes, bias_values, \n",
    "                    marker=marker_styles[i % len(marker_styles)],\n",
    "                    color=colors[i % len(colors)], \n",
    "                    label=method,\n",
    "                    linewidth=2,\n",
    "                    markersize=8)\n",
    "        \n",
    "        # MSE plot - changed from 'mean_rmse' to 'mean_mse'\n",
    "        mse_values = [results['mean_mse'][size] for size in sample_sizes]\n",
    "        # Optionally, we can take square root to display RMSE instead of MSE\n",
    "        rmse_values = [np.sqrt(mse) for mse in mse_values]\n",
    "        axes[1].plot(sample_sizes, rmse_values, \n",
    "                    marker=marker_styles[i % len(marker_styles)],\n",
    "                    color=colors[i % len(colors)], \n",
    "                    label=method,\n",
    "                    linewidth=2,\n",
    "                    markersize=8)\n",
    "    \n",
    "    # Set titles and labels\n",
    "    axes[0].set_title('Integrated Mean Absolute Bias', fontsize=16)\n",
    "    axes[1].set_title('Integrated RMSE', fontsize=16)  # Still displaying as RMSE for interpretation\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('Sample Size', fontsize=14)\n",
    "        ax.set_ylabel('Metric Value', fontsize=14)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.savefig('tuning_results/binary_treatment_results.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Create subplots for continuous treatment\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot continuous treatment results\n",
    "    for i, (method, results) in enumerate(continuous_summary.items()):\n",
    "        # Bias plot\n",
    "        bias_values = [results['mean_integrated_bias'][size] for size in sample_sizes]\n",
    "        axes[0].plot(sample_sizes, bias_values, \n",
    "                    marker=marker_styles[i % len(marker_styles)],\n",
    "                    color=colors[i % len(colors)], \n",
    "                    label=method,\n",
    "                    linewidth=2,\n",
    "                    markersize=8)\n",
    "        \n",
    "        # RMSE plot\n",
    "        rmse_values = [results['mean_integrated_rmse'][size] for size in sample_sizes]\n",
    "        axes[1].plot(sample_sizes, rmse_values, \n",
    "                    marker=marker_styles[i % len(marker_styles)],\n",
    "                    color=colors[i % len(colors)], \n",
    "                    label=method,\n",
    "                    linewidth=2,\n",
    "                    markersize=8)\n",
    "    \n",
    "    # Set titles and labels\n",
    "    axes[0].set_title('Integrated Mean Absolute Bias', fontsize=16)\n",
    "    axes[1].set_title('Integrated RMSE', fontsize=16)\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('Sample Size', fontsize=14)\n",
    "        ax.set_ylabel('Metric Value', fontsize=14)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.savefig('tuning_results/continuous_treatment_results.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eec46f6fd3162b8e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-07T11:12:38.079496Z",
     "start_time": "2025-04-07T11:12:38.078139Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c81ef35bdbde289e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# RUN FULL COMPARISSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3d1cd354c74d4b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-07T11:12:38.084315Z",
     "start_time": "2025-04-07T11:12:38.080669Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run simulations and create plots\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Define sample sizes\n",
    "    sample_sizes = [500, 1000, 2000]\n",
    "    #sample_sizes = [ 200,700, 1000]\n",
    "    \n",
    "    # Neural network configurations\n",
    "    nn_configs = {\n",
    "        'hidden_size': 64,\n",
    "        'batch_size': 64,\n",
    "        'epochs': 100,\n",
    "        'learning_rate': 0.01\n",
    "    }\n",
    "    \n",
    "    # Run binary treatment simulation with correctly specified covariates\n",
    "    print(\"Running binary treatment simulation (correctly specified)...\")\n",
    "    binary_results, binary_summary = run_binary_simulation(\n",
    "        n_replications=1,  # Reduce for testing, use 100 for final results\n",
    "        sample_sizes=sample_sizes,\n",
    "        misspecified=False,\n",
    "        nn_configs=nn_configs\n",
    "    )\n",
    "    \n",
    "    # # Run binary treatment simulation with misspecified covariates\n",
    "    # print(\"Running binary treatment simulation (misspecified)...\")\n",
    "    # binary_mis_results, binary_mis_summary = run_binary_simulation(\n",
    "    #     n_replications=1,  # Reduce for testing, use 100 for final results\n",
    "    #     sample_sizes=sample_sizes,\n",
    "    #     misspecified=True,\n",
    "    #     nn_configs=nn_configs\n",
    "    # )\n",
    "    \n",
    "    # Run continuous treatment simulation with correctly specified covariates\n",
    "    print(\"Running continuous treatment simulation (correctly specified)...\")\n",
    "    continuous_results, continuous_summary = run_continuous_simulation(\n",
    "        n_replications=1,  # Reduce for testing, use 100 for final results\n",
    "        sample_sizes=sample_sizes,\n",
    "        misspecified=False,\n",
    "        nn_configs=nn_configs\n",
    "    )\n",
    "    \n",
    "    # # Run continuous treatment simulation with misspecified covariates\n",
    "    # print(\"Running continuous treatment simulation (misspecified)...\")\n",
    "    # continuous_mis_results, continuous_mis_summary = run_continuous_simulation(\n",
    "    #     n_replications=50,  # Reduce for testing, use 100 for final results\n",
    "    #     sample_sizes=sample_sizes,\n",
    "    #     misspecified=True,\n",
    "    #     nn_configs=nn_configs\n",
    "    # )\n",
    "    \n",
    "    # Create plots\n",
    "    print(\"Creating plots...\")\n",
    "    \n",
    "    # Binary treatment, correctly specified\n",
    "    create_comparison_plots(binary_summary, continuous_summary)\n",
    "    \n",
    "    # Save results\n",
    "    print(\"Saving results...\")\n",
    "    import pickle\n",
    "    with open('tuning_results/simulation_results.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'binary': {\n",
    "                'results': binary_results,\n",
    "                # 'misspecified_results': binary_mis_results,\n",
    "                # 'misspecified_summary': binary_mis_summary\n",
    "            },\n",
    "            'continuous': {\n",
    "                'results': continuous_results,\n",
    "                'summary': continuous_summary,\n",
    "                # 'misspecified_results': continuous_mis_results,\n",
    "                # 'misspecified_summary': continuous_mis_summary\n",
    "            }\n",
    "        }, f)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "878db4392bc8f37b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-07T11:18:38.043798Z",
     "start_time": "2025-04-07T11:12:38.086904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running binary treatment simulation (correctly specified)...\n",
      "Replication 1/1, Sample size 500\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Replication 1/1, Sample size 1000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Replication 1/1, Sample size 2000\n",
      "Running continuous treatment simulation (correctly specified)...\n",
      "Replication 1/1, Sample size 500\n",
      "Error with method npCBPS: Data must be 1-dimensional, got ndarray of shape (500, 500) instead\n",
      "Error with method PW-Batch-SGD: Negative dimensions are not allowed\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Replication 1/1, Sample size 1000\n",
      "Error with method npCBPS: Data must be 1-dimensional, got ndarray of shape (1000, 1000) instead\n",
      "Error with method PW-Batch-SGD: Negative dimensions are not allowed\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Replication 1/1, Sample size 2000\n",
      "Error with method npCBPS: Data must be 1-dimensional, got ndarray of shape (2000, 2000) instead\n",
      "Error with method PW-Batch-SGD: Negative dimensions are not allowed\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Creating plots...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.cm' has no attribute 'tab12'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m----> 2\u001B[0m     main()\n",
      "Cell \u001B[0;32mIn[26], line 60\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCreating plots...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     59\u001B[0m \u001B[38;5;66;03m# Binary treatment, correctly specified\u001B[39;00m\n\u001B[0;32m---> 60\u001B[0m create_comparison_plots(binary_summary, continuous_summary)\n\u001B[1;32m     62\u001B[0m \u001B[38;5;66;03m# Save results\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSaving results...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[25], line 65\u001B[0m, in \u001B[0;36mcreate_comparison_plots\u001B[0;34m(binary_summary, continuous_summary)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# Plot settings\u001B[39;00m\n\u001B[1;32m     64\u001B[0m marker_styles \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mo\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m^\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mD\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mv\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m<\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m>\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mp\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mh\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mc\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m---> 65\u001B[0m colors \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39mcm\u001B[38;5;241m.\u001B[39mtab12\u001B[38;5;241m.\u001B[39mcolors\n\u001B[1;32m     67\u001B[0m \u001B[38;5;66;03m# Plot binary treatment results\u001B[39;00m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (method, results) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(binary_summary\u001B[38;5;241m.\u001B[39mitems()):\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# Bias plot\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'matplotlib.cm' has no attribute 'tab12'"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 2000x800 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Test neural network replicates behavior\n",
    "print(\"Testing neural network replicate reduction...\")\n",
    "\n",
    "import time\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "X = np.random.normal(size=(n, 2))\n",
    "propensity = 1 / (1 + np.exp(-X[:, 0] + 0.5 * X[:, 1]))\n",
    "A = np.random.binomial(1, propensity, size=n)\n",
    "\n",
    "# Compare standard multi-replicate approach with neural network\n",
    "start_time = time.time()\n",
    "result_standard = PW(\n",
    "    A=A, \n",
    "    X=X, \n",
    "    classifier='logit', \n",
    "    num_replicates=10\n",
    ")\n",
    "standard_time = time.time() - start_time\n",
    "print(f\"Standard approach with 10 replicates: {standard_time:.2f} seconds\")\n",
    "\n",
    "# Neural network should automatically use fewer replicates\n",
    "start_time = time.time()\n",
    "result_nn = PW(\n",
    "    A=A, \n",
    "    X=X, \n",
    "    classifier='neural_net',\n",
    "    use_sgd=True,\n",
    "    num_replicates=10\n",
    ")\n",
    "nn_time = time.time() - start_time\n",
    "print(f\"Neural network with requested 10 replicates: {nn_time:.2f} seconds\")\n",
    "\n",
    "# Check convergence info\n",
    "if 'convergence_info' in result_nn:\n",
    "    print(\"\\nConvergence information:\")\n",
    "    print(f\"Converged: {result_nn['convergence_info'].get('converged', 'Not available')}\")\n",
    "    print(f\"Iterations: {result_nn['convergence_info'].get('iterations', 'Not available')}\")\n",
    "    if 'best_loss' in result_nn['convergence_info']:\n",
    "        print(f\"Best loss: {result_nn['convergence_info']['best_loss']:.4f}\")\n",
    "    if 'losses' in result_nn['convergence_info']:\n",
    "        print(f\"Final loss: {result_nn['convergence_info']['losses'][0]:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e6925f3ee627ba4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ad117227d9ef66f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
