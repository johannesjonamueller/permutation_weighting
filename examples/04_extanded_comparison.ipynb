{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202b8a2f78a350f2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bcb6ff34f537fbe",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T10:08:13.579575Z",
     "start_time": "2025-04-09T10:08:13.568717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported PW from the package\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import expit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "\n",
    "# For better plot styling\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('colorblind')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import the permutation weighting implementation\n",
    "import sys\n",
    "sys.path.append('.')  # Add the current directory to path\n",
    "try:\n",
    "    from permutation_weighting.estimator import PW\n",
    "    print(\"Successfully imported PW from the package\")\n",
    "except ImportError:\n",
    "    print(\"Failed to import PW from the package - will implement a simplified version\")\n",
    "\n",
    "# Kang-Schafer DGP for binary treatment\n",
    "def generate_kang_schafer_binary(n=1000, seed=42, misspecified=False):\n",
    "    \"\"\"\n",
    "    Generate data according to the Kang-Schafer setup with binary treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n: int\n",
    "        Number of observations\n",
    "    seed: int\n",
    "        Random seed\n",
    "    misspecified: bool\n",
    "        Whether to return the misspecified transformations of covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df: pd.DataFrame\n",
    "        Data frame with columns: X1-X4 (covariates), A (treatment), Y (outcome), \n",
    "        Y1 (potential outcome under treatment), Y0 (potential outcome under control),\n",
    "        and X1_mis to X4_mis (misspecified covariates, if requested)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate covariates\n",
    "    X = np.random.normal(0, 1, size=(n, 4))\n",
    "    \n",
    "    # Treatment assignment\n",
    "    ps_linear = X[:, 0] - 0.5 * X[:, 1] + 0.25 * X[:, 2] + 0.1 * X[:, 3]\n",
    "    ps = expit(ps_linear)\n",
    "    A = np.random.binomial(1, ps, size=n)\n",
    "    \n",
    "    # Generate potential outcomes\n",
    "    Y1 = 210 + 1 + 27.4*X[:, 0] + 13.7*X[:, 1] + 13.7*X[:, 2] + 13.7*X[:, 3] + np.random.normal(0, 1, size=n)\n",
    "    Y0 = 210 + 0 + 27.4*X[:, 0] + 13.7*X[:, 1] + 13.7*X[:, 2] + 13.7*X[:, 3] + np.random.normal(0, 1, size=n)\n",
    "    \n",
    "    # Observed outcome\n",
    "    Y = A * Y1 + (1 - A) * Y0\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'X1': X[:, 0],\n",
    "        'X2': X[:, 1],\n",
    "        'X3': X[:, 2],\n",
    "        'X4': X[:, 3],\n",
    "        'A': A,\n",
    "        'Y': Y,\n",
    "        'Y1': Y1,\n",
    "        'Y0': Y0\n",
    "    })\n",
    "    \n",
    "    # Add misspecified covariates if requested\n",
    "    if misspecified:\n",
    "        df['X1_mis'] = np.exp(X[:, 0]/2)\n",
    "        df['X2_mis'] = X[:, 1] / (1 + np.exp(X[:, 0])) + 10\n",
    "        df['X3_mis'] = (X[:, 0] * X[:, 2] / 25 + 0.6)**3\n",
    "        df['X4_mis'] = (X[:, 1] + X[:, 3] + 20)**2\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84909c845cc3ad46",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T10:08:14.307848Z",
     "start_time": "2025-04-09T10:08:14.304898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now I'll implement simplified versions of the baseline methods for comparison\n",
    "# 1. Stabilized Inverse Propensity Score Weighting (IPSW)\n",
    "def compute_ipsw_binary(A, X, misspecified=False):\n",
    "    \"\"\"\n",
    "    Compute Stabilized Inverse Propensity Score Weights for binary treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A: array-like\n",
    "        Binary treatment indicator\n",
    "    X: array-like or DataFrame\n",
    "        Covariates\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates (X*_mis) if available\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights: array\n",
    "        Stabilized IPW weights\n",
    "    \"\"\"\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if misspecified and 'X1_mis' in X.columns:\n",
    "            X_mat = X[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']].values\n",
    "        else:\n",
    "            X_mat = X[['X1', 'X2', 'X3', 'X4']].values\n",
    "    else:\n",
    "        X_mat = X\n",
    "    \n",
    "    # Fit propensity score model\n",
    "    ps_model = LogisticRegression(max_iter=1000)\n",
    "    ps_model.fit(X_mat, A)\n",
    "    \n",
    "    # Compute propensity scores\n",
    "    ps = ps_model.predict_proba(X_mat)[:, 1]\n",
    "    \n",
    "    # Marginal treatment probability\n",
    "    p_A = np.mean(A)\n",
    "    \n",
    "    # Compute stabilized weights\n",
    "    weights = np.where(A == 1, p_A / ps, (1 - p_A) / (1 - ps))\n",
    "    \n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "130a7c8e648434db",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T10:08:15.170620Z",
     "start_time": "2025-04-09T10:08:15.168122Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Basic implementation of CBPS (simplified version)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def compute_cbps_binary(A, X, misspecified=False):\n",
    "    \"\"\"\n",
    "    Simplified version of Covariate Balancing Propensity Score for binary treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A: array-like\n",
    "        Binary treatment indicator\n",
    "    X: array-like or DataFrame\n",
    "        Covariates\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates (X*_mis) if available\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights: array\n",
    "        CBPS weights\n",
    "    \"\"\"\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if misspecified and 'X1_mis' in X.columns:\n",
    "            X_mat = X[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']].values\n",
    "        else:\n",
    "            X_mat = X[['X1', 'X2', 'X3', 'X4']].values\n",
    "    else:\n",
    "        X_mat = X\n",
    "    \n",
    "    # This is a simplification - true CBPS adds balance constraints\n",
    "    # For demonstration, we'll just use logistic regression with L2 penalty\n",
    "    ps_model = LogisticRegression(C=0.1, max_iter=1000)\n",
    "    ps_model.fit(X_mat, A)\n",
    "    \n",
    "    # Compute propensity scores\n",
    "    ps = ps_model.predict_proba(X_mat)[:, 1]\n",
    "    \n",
    "    # Marginal treatment probability\n",
    "    p_A = np.mean(A)\n",
    "    \n",
    "    # Compute weights\n",
    "    weights = np.where(A == 1, p_A / ps, (1 - p_A) / (1 - ps))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae02852a64a80fc0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T10:08:15.750059Z",
     "start_time": "2025-04-09T10:08:15.747423Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_ipsw_gbm(A, X, misspecified=False):\n",
    "    \"\"\"\n",
    "    Compute IPSW weights using gradient boosting\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A: array-like\n",
    "        Treatment indicator\n",
    "    X: array-like or DataFrame\n",
    "        Covariates\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights: array\n",
    "        IPSW weights\n",
    "    \"\"\"\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if misspecified and 'X1_mis' in X.columns:\n",
    "            X_mat = X[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']].values\n",
    "        else:\n",
    "            X_mat = X[['X1', 'X2', 'X3', 'X4']].values\n",
    "    else:\n",
    "        X_mat = X\n",
    "    \n",
    "    # Fit propensity score model with GBM\n",
    "    gbm = GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
    "    gbm.fit(X_mat, A)\n",
    "    \n",
    "    # Compute propensity scores\n",
    "    ps = gbm.predict_proba(X_mat)[:, 1]\n",
    "    \n",
    "    # Clip propensity scores to avoid extreme weights\n",
    "    ps = np.clip(ps, 0.01, 0.99)\n",
    "    \n",
    "    # Marginal treatment probability\n",
    "    p_A = np.mean(A)\n",
    "    \n",
    "    # Compute stabilized weights\n",
    "    weights = np.where(A == 1, p_A / ps, (1 - p_A) / (1 - ps))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cfad52ec702a1dcf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T10:08:16.294723Z",
     "start_time": "2025-04-09T10:08:16.286355Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_sbw_binary(A, X, misspecified=False):\n",
    "    \"\"\"\n",
    "    Simplified version of Stabilized Balancing Weights for binary treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A: array-like\n",
    "        Binary treatment indicator\n",
    "    X: array-like or DataFrame\n",
    "        Covariates\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights: array\n",
    "        SBW weights\n",
    "    \"\"\"\n",
    "    import cvxpy as cp # TODO: add cvxpy to requirements iff used in the package\n",
    "    \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if misspecified and 'X1_mis' in X.columns:\n",
    "            X_mat = X[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']].values\n",
    "        else:\n",
    "            X_mat = X[['X1', 'X2', 'X3', 'X4']].values\n",
    "    else:\n",
    "        X_mat = X\n",
    "    \n",
    "    n = len(A)\n",
    "    n_treated = np.sum(A)\n",
    "    n_control = n - n_treated\n",
    "    \n",
    "    # Separate covariates for treated and control\n",
    "    X_treated = X_mat[A == 1]\n",
    "    X_control = X_mat[A == 0]\n",
    "    \n",
    "    # Calculate means\n",
    "    treated_mean = np.mean(X_treated, axis=0)\n",
    "    \n",
    "    # Initialize weights for control units\n",
    "    w = cp.Variable(n_control, nonneg=True)\n",
    "    \n",
    "    # Balance constraint: weighted mean of control features equals mean of treated features\n",
    "    balance_constraint = []\n",
    "    for j in range(X_mat.shape[1]):\n",
    "        # Allow small imbalance (delta)\n",
    "        delta = 0.1 * np.std(X_mat[:, j])\n",
    "        balance_constraint.append(cp.abs(cp.sum(cp.multiply(w, X_control[:, j])) - treated_mean[j] * cp.sum(w)) <= delta)\n",
    "    \n",
    "    # Sum constraint\n",
    "    balance_constraint.append(cp.sum(w) == 1)\n",
    "    \n",
    "    # Objective: minimize variance\n",
    "    objective = cp.Minimize(cp.sum_squares(w - 1/n_control))\n",
    "    \n",
    "    # Solve optimization problem\n",
    "    prob = cp.Problem(objective, balance_constraint)\n",
    "    try:\n",
    "        prob.solve(solver=cp.OSQP)\n",
    "    except:\n",
    "        try:\n",
    "            prob.solve(solver=cp.ECOS)\n",
    "        except:\n",
    "            prob.solve(solver=cp.SCS)\n",
    "    \n",
    "    # Create final weights vector\n",
    "    weights = np.ones(n)\n",
    "    weights[A == 0] = w.value * n_control\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02458b6a92eceda",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# EVALUATE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e64d2d6f37081070",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T10:24:22.466691Z",
     "start_time": "2025-04-09T10:24:22.458219Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_ate_binary(df, weights, true_ate=1.0):\n",
    "    \"\"\"\n",
    "    Evaluate ATE estimation for binary treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: DataFrame\n",
    "        Data with 'A', 'Y', 'Y1', 'Y0' columns\n",
    "    weights: array-like\n",
    "        Weights for each observation\n",
    "    true_ate: float\n",
    "        True average treatment effect\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float\n",
    "        The estimated error (for use in replications)\n",
    "    \"\"\"\n",
    "    # Calculate weighted ATE (this remains unchanged)\n",
    "    treated_idx = df['A'] == 1\n",
    "    control_idx = df['A'] == 0\n",
    "    \n",
    "    treated_mean = np.sum(df.loc[treated_idx, 'Y'] * weights[treated_idx]) / np.sum(weights[treated_idx])\n",
    "    control_mean = np.sum(df.loc[control_idx, 'Y'] * weights[control_idx]) / np.sum(weights[control_idx])\n",
    "    \n",
    "    estimated_ate = treated_mean - control_mean\n",
    "    \n",
    "    # Calculate error\n",
    "    error = estimated_ate - true_ate\n",
    "    \n",
    "    # Return the error for this single replication\n",
    "    # In run_binary_simulation, these will be aggregated to calculate IRMSE\n",
    "    return error\n",
    "\n",
    "\n",
    "def evaluate_dose_response_continuous(df, weights, treatment_grid=None):\n",
    "    \"\"\"\n",
    "    Evaluate dose-response estimation for continuous treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: DataFrame\n",
    "        Data with 'A', 'Y', 'true_dr' columns\n",
    "    weights: array-like\n",
    "        Weights for each observation\n",
    "    treatment_grid: array-like, optional\n",
    "        Grid of treatment values for evaluation\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with integrated bias and rmse for this replication\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # Define treatment grid if not provided\n",
    "    if treatment_grid is None:\n",
    "        # Use central 90% of treatment distribution (A*)\n",
    "        min_a, max_a = np.percentile(df['A'], [5, 95])\n",
    "        treatment_grid = np.linspace(min_a, max_a, 50)\n",
    "    \n",
    "    # Calculate treatment density for the grid points\n",
    "    from scipy.stats import gaussian_kde\n",
    "    try:\n",
    "        # Try to estimate a kernel density (may fail on very small datasets)\n",
    "        density = gaussian_kde(df['A'].values)\n",
    "        treatment_probs = density(treatment_grid)\n",
    "        # Normalize\n",
    "        treatment_probs = treatment_probs / np.sum(treatment_probs)\n",
    "    except:\n",
    "        # Fallback to uniform weighting if KDE fails\n",
    "        treatment_probs = np.ones(len(treatment_grid)) / len(treatment_grid)\n",
    "    \n",
    "    # True dose-response function\n",
    "    true_dr = [210 + 1/(1 + np.exp(a)) for a in treatment_grid]\n",
    "    \n",
    "    # Estimate dose-response function using weighted local linear regression\n",
    "    est_dr = []\n",
    "    for a in treatment_grid:\n",
    "        # Calculate kernel weights\n",
    "        bandwidth = (np.percentile(df['A'], 75) - np.percentile(df['A'], 25)) / 1.34\n",
    "        kernel_weights = np.exp(-0.5 * ((df['A'] - a) / bandwidth)**2) * weights\n",
    "        \n",
    "        # Fit weighted linear regression\n",
    "        model = LinearRegression()\n",
    "        model.fit(\n",
    "            df[['A']], \n",
    "            df['Y'],\n",
    "            sample_weight=kernel_weights\n",
    "        )\n",
    "        \n",
    "        # Predict at treatment value a\n",
    "        est_dr.append(model.predict([[a]])[0])\n",
    "    \n",
    "    # Calculate weighted absolute error (for Integrated Mean Absolute Bias)\n",
    "    abs_errors = np.abs(np.array(est_dr) - np.array(true_dr))\n",
    "    weighted_bias = np.sum(abs_errors * treatment_probs)\n",
    "    \n",
    "    # Calculate weighted squared error (for IRMSE)\n",
    "    squared_errors = (np.array(est_dr) - np.array(true_dr))**2\n",
    "    weighted_rmse = np.sqrt(np.sum(squared_errors * treatment_probs))\n",
    "    \n",
    "    return {\n",
    "        'integrated_bias': weighted_bias,\n",
    "        'integrated_rmse': weighted_rmse,\n",
    "        'treatment_grid': treatment_grid,\n",
    "        'estimated_dr': est_dr,\n",
    "        'true_dr': true_dr\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9780b71f3e9338",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# SIMULATION BINARY"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T10:11:19.267980Z",
     "start_time": "2025-04-09T10:11:19.266682Z"
    }
   },
   "id": "f92d2bedfd61f68e",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "233387ec5f5bee29",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T10:11:19.469506Z",
     "start_time": "2025-04-09T10:11:19.459327Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_binary_simulation(n_replications=100, sample_sizes=[500, 1000, 1500, 2000], \n",
    "                         misspecified=False, nn_configs=None):\n",
    "    \"\"\"\n",
    "    Run simulation for binary treatment methods\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_replications: int\n",
    "        Number of simulation replications\n",
    "    sample_sizes: list\n",
    "        List of sample sizes to test\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates\n",
    "    nn_configs: dict, optional\n",
    "        Neural network configurations\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results: dict\n",
    "        Dictionary with results for all methods\n",
    "    summary: dict\n",
    "        Summary statistics\n",
    "    timings: dict\n",
    "        Execution times for each method\n",
    "    \"\"\"\n",
    "    # Define baseline methods\n",
    "    methods = {\n",
    "        'Unweighted': lambda a, x: np.ones(len(a)),\n",
    "        'IPSW-GLM': lambda a, x: compute_ipsw_binary(a, x, misspecified),\n",
    "        'IPSW-GBM': lambda a, x: compute_ipsw_gbm(a, x, misspecified),\n",
    "        'CBPS': lambda a, x: compute_cbps_binary(a, x, misspecified),\n",
    "        'SBW': lambda a, x: compute_sbw_binary(a, x, misspecified)\n",
    "    }\n",
    "    \n",
    "    # Standard trainers\n",
    "    methods['PW-GLM'] = lambda a, x: PW(\n",
    "        a, x, \n",
    "        classifier='logit', \n",
    "        estimand='ATE',\n",
    "        num_replicates=50,\n",
    "        estimand_params={'bootstrap': True}\n",
    "    )['weights']\n",
    "    \n",
    "    methods['PW-Boosting'] = lambda a, x: PW(\n",
    "        a, x, \n",
    "        classifier='boosting', \n",
    "        estimand='ATE',\n",
    "        classifier_params={'n_estimators': 100, 'max_depth': 3},\n",
    "        num_replicates=50,\n",
    "        estimand_params={'bootstrap': True}\n",
    "    )['weights']\n",
    "    \n",
    "    # SGD-based trainers if nn_configs is provided\n",
    "    if nn_configs is not None:\n",
    "        # SGD logistic regression\n",
    "        methods['PW-SGD-Logit'] = lambda a, x: PW(\n",
    "            a, x, \n",
    "            classifier='logit', \n",
    "            estimand='ATE',\n",
    "            use_sgd=True,\n",
    "            classifier_params={\n",
    "                'alpha': nn_configs.get('alpha', 0.0001),\n",
    "                'max_iter': nn_configs.get('epochs', 100),\n",
    "                'learning_rate': 'optimal'\n",
    "            },\n",
    "            num_replicates=1,\n",
    "            estimand_params={'bootstrap': True}\n",
    "        )['weights']\n",
    "        \n",
    "        # Neural network\n",
    "        methods['PW-Neural'] = lambda a, x: PW(\n",
    "            a, x, \n",
    "            classifier='neural_net', \n",
    "            estimand='ATE',\n",
    "            use_sgd=True,\n",
    "            classifier_params={\n",
    "                'hidden_layer_sizes': (nn_configs.get('hidden_size', 64),),\n",
    "                'max_iter': nn_configs.get('epochs', 100),\n",
    "                'batch_size': nn_configs.get('batch_size', 32),\n",
    "                'learning_rate_init': nn_configs.get('learning_rate', 0.001)\n",
    "            },\n",
    "            num_replicates=1,\n",
    "            estimand_params={'bootstrap': True}\n",
    "        )['weights']\n",
    "        \n",
    "        # Batch-then-permute SGD approach\n",
    "        methods['PW-Batch-SGD'] = lambda a, x: PW(\n",
    "            a, x, \n",
    "            classifier='logit', \n",
    "            estimand='ATE',\n",
    "            use_sgd=True,\n",
    "            batch_size=nn_configs.get('batch_size', 32),\n",
    "            classifier_params={\n",
    "                'alpha': nn_configs.get('alpha', 0.0001),\n",
    "                'max_iter': nn_configs.get('epochs', 100),\n",
    "                'learning_rate': 'optimal'\n",
    "            },\n",
    "            num_replicates=1,  # Only 1 replicate since permutation happens in batches\n",
    "            estimand_params={'bootstrap': False}\n",
    "        )['weights']\n",
    "        \n",
    "        # PyTorch-based trainers if available\n",
    "        try:\n",
    "            # Test if torch module exists\n",
    "            import torch\n",
    "            \n",
    "            # MLP with PyTorch\n",
    "            methods['PW-Torch-MLP'] = lambda a, x: PW(\n",
    "                a, x, \n",
    "                classifier='mlp', \n",
    "                estimand='ATE',\n",
    "                use_torch=True,\n",
    "                classifier_params={\n",
    "                    'hidden_dims': [nn_configs.get('hidden_size', 64), nn_configs.get('hidden_size', 64)//2],\n",
    "                    'epochs': nn_configs.get('epochs', 100),\n",
    "                    'batch_size': nn_configs.get('batch_size', 32),\n",
    "                    'learning_rate': nn_configs.get('learning_rate', 0.001)\n",
    "                },\n",
    "                num_replicates=1,\n",
    "                estimand_params={'bootstrap': True}\n",
    "            )['weights']\n",
    "            \n",
    "            # Batch-then-permute PyTorch approach\n",
    "            methods['PW-Batch-Torch'] = lambda a, x: PW(\n",
    "                a, x, \n",
    "                classifier='mlp', \n",
    "                estimand='ATE',\n",
    "                use_torch=True,\n",
    "                batch_size=nn_configs.get('batch_size', 32),\n",
    "                classifier_params={\n",
    "                    'hidden_dims': [nn_configs.get('hidden_size', 64), nn_configs.get('hidden_size', 64)//2],\n",
    "                    'epochs': nn_configs.get('epochs', 100),\n",
    "                    'learning_rate': nn_configs.get('learning_rate', 0.001)\n",
    "                },\n",
    "                num_replicates=1,  # Only 1 replicate since permutation happens in batches\n",
    "                estimand_params={'bootstrap': False}\n",
    "            )['weights']\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"PyTorch not available, skipping PyTorch models\")\n",
    "    \n",
    "    # Initialize results dictionary - store raw errors instead of bias/rmse\n",
    "    results = {\n",
    "        method: {\n",
    "            'error': {size: [] for size in sample_sizes}\n",
    "        } for method in methods\n",
    "    }\n",
    "    \n",
    "    # Initialize timing dictionary\n",
    "    timings = {\n",
    "        method: {size: [] for size in sample_sizes}\n",
    "        for method in methods\n",
    "    }\n",
    "    \n",
    "    for rep in range(n_replications):\n",
    "        for size in sample_sizes:\n",
    "            print(f\"Replication {rep+1}/{n_replications}, Sample size {size}\")\n",
    "            \n",
    "            # Generate data\n",
    "            df = generate_kang_schafer_binary(n=size, seed=rep, misspecified=misspecified)\n",
    "            \n",
    "            # Apply each method\n",
    "            for method_name, method_func in methods.items():\n",
    "                try:\n",
    "                    x_features = df[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']] if misspecified else df[['X1', 'X2', 'X3', 'X4']]\n",
    "                    \n",
    "                    # Time the execution\n",
    "                    start_time = time.time()\n",
    "                    weights = method_func(df['A'].values, x_features)\n",
    "                    execution_time = time.time() - start_time\n",
    "                    \n",
    "                    # Record timing\n",
    "                    timings[method_name][size].append(execution_time)\n",
    "                    \n",
    "                    # Evaluate error\n",
    "                    error = evaluate_ate_binary(df, weights)\n",
    "                    \n",
    "                    # Store error\n",
    "                    errors = results[method_name]['error'][size]\n",
    "                      errors.append(error)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with method {method_name}: {e}\")\n",
    "    \n",
    "    # Compute mean and std with corrected formulas\n",
    "    summary = {\n",
    "        method: {\n",
    "            'mean_bias': {size: np.abs(np.mean(results[method]['error'][size])) for size in sample_sizes},\n",
    "            'mean_mse': {size: np.mean(np.power(results[method]['error'][size], 2)) for size in sample_sizes},\n",
    "            'std_bias': {size: np.std(results[method]['error'][size]) / np.sqrt(n_replications) for size in sample_sizes},\n",
    "            'std_rmse': {size: np.std(np.power(results[method]['error'][size], 2)) / np.sqrt(n_replications) for size in sample_sizes}\n",
    "        } for method in methods\n",
    "    }\n",
    "    avg_timings = {\n",
    "        method: {size: np.mean(times) for size, times in method_timings.items()}\n",
    "        for method, method_timings in timings.items()\n",
    "    }\n",
    "    \n",
    "    return results, summary, avg_timings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b12a885a7b9d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CONTIOUS  TREAMTMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3838de4c21ba1b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data generation for continuous treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d56aa7072214f58",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T10:11:19.976542Z",
     "start_time": "2025-04-09T10:11:19.972367Z"
    }
   },
   "outputs": [],
   "source": [
    "# Kang-Schafer DGP for continuous treatment\n",
    "def generate_kang_schafer_continuous(n=1000, seed=42, misspecified=False):\n",
    "    \"\"\"\n",
    "    Generate data according to the Kang-Schafer setup with continuous treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n: int\n",
    "        Number of observations\n",
    "    seed: int\n",
    "        Random seed\n",
    "    misspecified: bool\n",
    "        Whether to return the misspecified transformations of covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df: pd.DataFrame\n",
    "        Data frame with columns: X1-X4 (covariates), A (treatment), Y (outcome),\n",
    "        and X1_mis to X4_mis (misspecified covariates, if requested)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate covariates\n",
    "    X = np.random.normal(0, 1, size=(n, 4))\n",
    "    \n",
    "    # Treatment assignment (linear with noise)\n",
    "    A_linear = X[:, 0] - 0.5 * X[:, 1] + 0.25 * X[:, 2] + 0.1 * X[:, 3]\n",
    "    A = A_linear + np.random.normal(0, 1, size=n)\n",
    "    \n",
    "    # Generate outcome with non-linear treatment effect\n",
    "    Y = 210 + 1/(1 + np.exp(A)) + 27.4*X[:, 0] + 13.7*X[:, 1] + 13.7*X[:, 2] + 13.7*X[:, 3] + np.random.normal(0, 1, size=n)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'X1': X[:, 0],\n",
    "        'X2': X[:, 1],\n",
    "        'X3': X[:, 2],\n",
    "        'X4': X[:, 3],\n",
    "        'A': A,\n",
    "        'Y': Y\n",
    "    })\n",
    "    \n",
    "    # True dose-response function (for evaluation)\n",
    "    def true_dose_response(a):\n",
    "        return 210 + 1/(1 + np.exp(a)) + 27.4 * 0 + 13.7 * 0 + 13.7 * 0 + 13.7 * 0  # Expectation of X is 0\n",
    "    \n",
    "    df['true_dr'] = [true_dose_response(a) for a in A]\n",
    "    \n",
    "    # Add misspecified covariates if requested\n",
    "    if misspecified:\n",
    "        df['X1_mis'] = np.exp(X[:, 0]/2)\n",
    "        df['X2_mis'] = X[:, 1] / (1 + np.exp(X[:, 0])) + 10\n",
    "        df['X3_mis'] = (X[:, 0] * X[:, 2] / 25 + 0.6)**3\n",
    "        df['X4_mis'] = (X[:, 1] + X[:, 3] + 20)**2\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Check if our data generation works\n",
    "# binary_df = generate_kang_schafer_binary(n=1000, seed=42, misspecified=True)\n",
    "# print(\"Binary treatment data (first 5 rows):\")\n",
    "# print(binary_df.iloc[:5, :10])  # Show first 5 rows and 10 columns\n",
    "# \n",
    "# continuous_df = generate_kang_schafer_continuous(n=1000, seed=42, misspecified=True)\n",
    "# print(\"\\nContinuous treatment data (first 5 rows):\")\n",
    "# print(continuous_df.iloc[:5, :10])  # Show first 5 rows and 10 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80022a55aa7edd8a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# other methods for continuous treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c646ad3f6c4c3d2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T10:11:20.588090Z",
     "start_time": "2025-04-09T10:11:20.583780Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_normal_linear_weights(A, X, misspecified=False):\n",
    "    \"\"\"\n",
    "    Compute weights for continuous treatments using a normal-linear model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A: array-like\n",
    "        Continuous treatment variable\n",
    "    X: array-like or DataFrame\n",
    "        Covariates\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights: array\n",
    "        Stabilized weights\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from scipy.stats import norm\n",
    "    \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if misspecified and 'X1_mis' in X.columns:\n",
    "            X_mat = X[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']].values\n",
    "        else:\n",
    "            X_mat = X[['X1', 'X2', 'X3', 'X4']].values\n",
    "    else:\n",
    "        X_mat = X\n",
    "    \n",
    "    # Fit linear regression for treatment given covariates\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_mat, A)\n",
    "    \n",
    "    # Predict treatment and calculate residuals\n",
    "    A_pred = model.predict(X_mat)\n",
    "    resid = A - A_pred\n",
    "    \n",
    "    # Estimate residual variance\n",
    "    sigma = np.std(resid)\n",
    "    \n",
    "    # Compute likelihood of observed treatment under model\n",
    "    pdf_cond = norm.pdf(A, loc=A_pred, scale=sigma)\n",
    "    \n",
    "    # Compute likelihood under marginal distribution\n",
    "    pdf_marg = norm.pdf(A, loc=np.mean(A), scale=np.std(A))\n",
    "    \n",
    "    # Compute stabilized weights\n",
    "    weights = pdf_marg / pdf_cond\n",
    "    \n",
    "    # Clip extreme weights\n",
    "    weights = np.clip(weights, 0.01, 100)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def compute_np_cbps(A, X, misspecified=False):\n",
    "    \"\"\"\n",
    "    Compute non-parametric Covariate Balancing Propensity Score weights for continuous treatments\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    A: array-like\n",
    "        Continuous treatment variable\n",
    "    X: array-like or DataFrame\n",
    "        Covariates\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights: array\n",
    "        CBPS weights\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LassoCV\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        if misspecified and 'X1_mis' in X.columns:\n",
    "            X_mat = X[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']].values\n",
    "        else:\n",
    "            X_mat = X[['X1', 'X2', 'X3', 'X4']].values\n",
    "    else:\n",
    "        X_mat = X\n",
    "    \n",
    "    # Ensure A is 1-dimensional\n",
    "    A = np.asarray(A).flatten()\n",
    "    \n",
    "    # Create polynomial features to approximate non-parametric model\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=True)\n",
    "    X_poly = poly.fit_transform(X_mat)\n",
    "    \n",
    "    # Fit Lasso model with cross-validation for regularization\n",
    "    model = LassoCV(cv=5, max_iter=2000, random_state=42)\n",
    "    model.fit(X_poly, A)\n",
    "    \n",
    "    # Compute residuals\n",
    "    A_pred = model.predict(X_poly)\n",
    "    resid = A - A_pred\n",
    "    \n",
    "    # Compute covariance between residuals and covariate functions\n",
    "    cov_mat = np.zeros((X_poly.shape[1], 1))\n",
    "    for j in range(X_poly.shape[1]):\n",
    "        cov_mat[j, 0] = np.cov(X_poly[:, j], resid)[0, 1]\n",
    "    \n",
    "    # Use ridge regression to solve for lambda parameters (more stable)\n",
    "    from sklearn.linear_model import Ridge\n",
    "    ridge = Ridge(alpha=0.01)\n",
    "    ridge.fit(X_poly, resid)\n",
    "    lambda_param = ridge.coef_.reshape(-1, 1)\n",
    "    \n",
    "    # Compute weights using exponential tilting\n",
    "    scores = X_poly @ lambda_param\n",
    "    \n",
    "    # Apply exponential tilting and normalize\n",
    "    weights = np.exp(scores.flatten() - np.max(scores))  # Subtract max for numerical stability\n",
    "    weights = weights / np.mean(weights) * len(A)\n",
    "    \n",
    "    # Clip extreme weights\n",
    "    weights = np.clip(weights, 0.01, 100)\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303fe2218ad77093",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# continuous treatment simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c7ac279e4154406",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T10:11:21.778571Z",
     "start_time": "2025-04-09T10:11:21.769370Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_continuous_simulation(n_replications=100, sample_sizes=[500, 1000, 1500, 2000], \n",
    "                             misspecified=False, nn_configs=None):\n",
    "    \"\"\"\n",
    "    Run simulation for continuous treatment methods\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_replications: int\n",
    "        Number of simulation replications\n",
    "    sample_sizes: list\n",
    "        List of sample sizes to test\n",
    "    misspecified: bool\n",
    "        Whether to use misspecified covariates\n",
    "    nn_configs: dict, optional\n",
    "        Neural network configurations\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results: dict\n",
    "        Dictionary with results for all methods\n",
    "    summary: dict\n",
    "        Summary statistics\n",
    "    timings: dict\n",
    "        Execution times for each method\n",
    "    \"\"\"\n",
    "    # Define baseline methods\n",
    "    methods = {\n",
    "        'Unweighted': lambda a, x: np.ones(len(a)),\n",
    "    }\n",
    "    \n",
    "    # Add normal linear model for continuous treatments\n",
    "    methods['Normal-Linear'] = lambda a, x: compute_normal_linear_weights(a, x, misspecified)\n",
    "    \n",
    "    # Add non-parametric CBPS for continuous treatments\n",
    "    methods['npCBPS'] = lambda a, x: compute_np_cbps(a, x, misspecified)\n",
    "    \n",
    "    # Standard trainers\n",
    "    methods['PW-GLM'] = lambda a, x: PW(\n",
    "        a, x, \n",
    "        classifier='logit', \n",
    "        estimand='ATE',\n",
    "        num_replicates=50,\n",
    "        estimand_params={'bootstrap': True}\n",
    "    )['weights']\n",
    "    \n",
    "    methods['PW-Boosting'] = lambda a, x: PW(\n",
    "        a, x, \n",
    "        classifier='boosting', \n",
    "        estimand='ATE',\n",
    "        classifier_params={'n_estimators': 100, 'max_depth': 3},\n",
    "        num_replicates=50,\n",
    "        estimand_params={'bootstrap': True}\n",
    "    )['weights']\n",
    "    \n",
    "    # SGD-based trainers if nn_configs is provided\n",
    "    if nn_configs is not None:\n",
    "        # SGD logistic regression\n",
    "        methods['PW-SGD-Logit'] = lambda a, x: PW(\n",
    "            a, x, \n",
    "            classifier='logit', \n",
    "            estimand='ATE',\n",
    "            use_sgd=True,\n",
    "            classifier_params={\n",
    "                'alpha': nn_configs.get('alpha', 0.0001),\n",
    "                'max_iter': nn_configs.get('epochs', 100),\n",
    "                'learning_rate': 'optimal'\n",
    "            },\n",
    "            num_replicates=1,\n",
    "            estimand_params={'bootstrap': True}\n",
    "        )['weights']\n",
    "        \n",
    "        # Neural network with minibatch training\n",
    "        methods['PW-Neural'] = lambda a, x: PW(\n",
    "            a, x, \n",
    "            classifier='neural_net', \n",
    "            estimand='ATE',\n",
    "            use_sgd=True,\n",
    "            classifier_params={\n",
    "                'hidden_layer_sizes': (nn_configs.get('hidden_size', 64),),\n",
    "                'max_iter': nn_configs.get('epochs', 100),\n",
    "                'batch_size': nn_configs.get('batch_size', 32),\n",
    "                'learning_rate_init': nn_configs.get('learning_rate', 0.001)\n",
    "            },\n",
    "            num_replicates=1,\n",
    "            estimand_params={'bootstrap': True}\n",
    "        )['weights']\n",
    "        \n",
    "        # Batch-then-permute SGD approach\n",
    "        methods['PW-Batch-SGD'] = lambda a, x: PW(\n",
    "            a, x, \n",
    "            classifier='logit', \n",
    "            estimand='ATE',\n",
    "            use_sgd=True,\n",
    "            batch_size=nn_configs.get('batch_size', 32),\n",
    "            classifier_params={\n",
    "                'alpha': nn_configs.get('alpha', 0.0001),\n",
    "                'max_iter': nn_configs.get('epochs', 100),\n",
    "                'learning_rate': 'optimal'\n",
    "            },\n",
    "            num_replicates=1,  # Only 1 replicate since permutation happens in batches\n",
    "            estimand_params={'bootstrap': False}\n",
    "        )['weights']\n",
    "        \n",
    "        # PyTorch-based trainers if available\n",
    "        try:\n",
    "            # Test if torch module exists\n",
    "            import torch\n",
    "            \n",
    "            # MLP with PyTorch\n",
    "            methods['PW-Torch-MLP'] = lambda a, x: PW(\n",
    "                a, x, \n",
    "                classifier='mlp', \n",
    "                estimand='ATE',\n",
    "                use_torch=True,\n",
    "                classifier_params={\n",
    "                    'hidden_dims': [nn_configs.get('hidden_size', 64), nn_configs.get('hidden_size', 64)//2],\n",
    "                    'epochs': nn_configs.get('epochs', 100),\n",
    "                    'batch_size': nn_configs.get('batch_size', 32),\n",
    "                    'learning_rate': nn_configs.get('learning_rate', 0.001)\n",
    "                },\n",
    "                num_replicates=1,\n",
    "                estimand_params={'bootstrap': True}\n",
    "            )['weights']\n",
    "            \n",
    "            # Batch-then-permute PyTorch approach\n",
    "            methods['PW-Batch-Torch'] = lambda a, x: PW(\n",
    "                a, x, \n",
    "                classifier='mlp', \n",
    "                estimand='ATE',\n",
    "                use_torch=True,\n",
    "                batch_size=nn_configs.get('batch_size', 32),\n",
    "                classifier_params={\n",
    "                    'hidden_dims': [nn_configs.get('hidden_size', 64), nn_configs.get('hidden_size', 64)//2],\n",
    "                    'epochs': nn_configs.get('epochs', 100),\n",
    "                    'learning_rate': nn_configs.get('learning_rate', 0.001)\n",
    "                },\n",
    "                num_replicates=1,  # Only 1 replicate since permutation happens in batches\n",
    "                estimand_params={'bootstrap': False}\n",
    "            )['weights']\n",
    "            \n",
    "            # # ResNet-style model with PyTorch\n",
    "            # methods['PW-Torch-ResNet'] = lambda a, x: PW(\n",
    "            #     a, x, \n",
    "            #     classifier='resnet', \n",
    "            #     estimand='ATE',\n",
    "            #     use_torch=True,\n",
    "            #     classifier_params={\n",
    "            #         'hidden_dim': nn_configs.get('hidden_size', 64),\n",
    "            #         'num_blocks': nn_configs.get('num_blocks', 2),\n",
    "            #         'epochs': nn_configs.get('epochs', 100),\n",
    "            #         'batch_size': nn_configs.get('batch_size', 32),\n",
    "            #         'learning_rate': nn_configs.get('learning_rate', 0.001)\n",
    "            #     },\n",
    "            #     num_replicates=1,\n",
    "            #     estimand_params={'bootstrap': True}\n",
    "            # )['weights']\n",
    "            # \n",
    "            # # Batch-then-permute ResNet PyTorch approach\n",
    "            # methods['PW-Batch-Torch-ResNet'] = lambda a, x: PW(\n",
    "            #     a, x, \n",
    "            #     classifier='resnet', \n",
    "            #     estimand='ATE',\n",
    "            #     use_torch=True,\n",
    "            #     batch_size=nn_configs.get('batch_size', 32),\n",
    "            #     classifier_params={\n",
    "            #         'hidden_dim': nn_configs.get('hidden_size', 64),\n",
    "            #         'num_blocks': nn_configs.get('num_blocks', 2),\n",
    "            #         'epochs': nn_configs.get('epochs', 100),\n",
    "            #         'learning_rate': nn_configs.get('learning_rate', 0.001)\n",
    "            #     },\n",
    "            #     num_replicates=1,  # Only 1 replicate since permutation happens in batches\n",
    "            #     estimand_params={'bootstrap': False}\n",
    "            # )['weights']\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"PyTorch not available, skipping PyTorch models\")\n",
    "    \n",
    "    # Initialize results dictionary with appropriate structure for continuous outcomes\n",
    "    results = {\n",
    "        method: {\n",
    "            'integrated_bias': {size: [] for size in sample_sizes},\n",
    "            'integrated_rmse': {size: [] for size in sample_sizes}\n",
    "        } for method in methods\n",
    "    }\n",
    "    \n",
    "    timings = {\n",
    "        method: {size: [] for size in sample_sizes}\n",
    "        for method in methods\n",
    "    }\n",
    "    \n",
    "    for rep in range(n_replications):\n",
    "        for size in sample_sizes:\n",
    "            print(f\"Replication {rep+1}/{n_replications}, Sample size {size}\")\n",
    "            \n",
    "            # Generate data\n",
    "            df = generate_kang_schafer_continuous(n=size, seed=rep, misspecified=misspecified)\n",
    "            \n",
    "            # Apply each method\n",
    "            for method_name, method_func in methods.items():\n",
    "                try:\n",
    "                    x_features = df[['X1_mis', 'X2_mis', 'X3_mis', 'X4_mis']] if misspecified else df[['X1', 'X2', 'X3', 'X4']]\n",
    "                    start_time = time.time()\n",
    "                    weights = method_func(df['A'].values, x_features)\n",
    "                    execution_time = time.time() - start_time\n",
    "                    \n",
    "                    # Record timing\n",
    "                    timings[method_name][size].append(execution_time)\n",
    "                    # Evaluate\n",
    "                    eval_result = evaluate_dose_response_continuous(df, weights)\n",
    "                    \n",
    "                    results[method_name]['integrated_bias'][size].append(eval_result['integrated_bias'])\n",
    "                    results[method_name]['integrated_rmse'][size].append(eval_result['integrated_rmse'])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with method {method_name}: {e}\")\n",
    "    \n",
    "    # Compute mean and std\n",
    "    summary = {\n",
    "        method: {\n",
    "            'mean_integrated_bias': {size: np.mean(results[method]['integrated_bias'][size]) for size in sample_sizes},\n",
    "            'mean_integrated_rmse': {size: np.mean(results[method]['integrated_rmse'][size]) for size in sample_sizes},\n",
    "            'std_integrated_bias': {size: np.std(results[method]['integrated_bias'][size]) / np.sqrt(n_replications) for size in sample_sizes},\n",
    "            'std_integrated_rmse': {size: np.std(results[method]['integrated_rmse'][size]) / np.sqrt(n_replications) for size in sample_sizes}\n",
    "        } for method in methods\n",
    "    }\n",
    "    \n",
    "    avg_timings = {\n",
    "        method: {size: np.mean(times) for size, times in method_timings.items()}\n",
    "        for method, method_timings in timings.items()\n",
    "    }\n",
    "    \n",
    "    return results, summary, avg_timings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d22ebda51ac07",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26814f1ed3ed5861",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T10:11:22.637952Z",
     "start_time": "2025-04-09T10:11:22.625121Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_simulation_results(summary, metric_name, title):\n",
    "    \"\"\"\n",
    "    Plot simulation results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    summary: dict\n",
    "        Summary of simulation results\n",
    "    metric_name: str\n",
    "        Metric to plot (e.g., 'mean_bias', 'mean_rmse')\n",
    "    title: str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    methods = list(summary.keys())\n",
    "    sample_sizes = list(summary[methods[0]][metric_name].keys())\n",
    "    \n",
    "    marker_styles = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'h']\n",
    "    colors = plt.cm.tab10.colors\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        values = [summary[method][metric_name][size] for size in sample_sizes]\n",
    "        std_values = [summary[method].get(f'std_{metric_name.replace(\"mean_\", \"\")}', {}).get(size, 0) for size in sample_sizes]\n",
    "        \n",
    "        plt.errorbar(\n",
    "            sample_sizes, values, yerr=std_values,\n",
    "            marker=marker_styles[i % len(marker_styles)],\n",
    "            color=colors[i % len(colors)],\n",
    "            label=method,\n",
    "            linewidth=2,\n",
    "            markersize=8,\n",
    "            capsize=5\n",
    "        )\n",
    "    \n",
    "    plt.xscale('linear')\n",
    "    plt.xlabel('Sample Size', fontsize=14)\n",
    "    plt.ylabel(metric_name.replace('_', ' ').title(), fontsize=14)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "def create_comparison_plots(binary_summary, continuous_summary, binary_timings=None, continuous_timings=None, \n",
    "                           title_suffix=\"\", filename_suffix=\"\"):\n",
    "    \"\"\"\n",
    "    Create comparison plots for binary and continuous simulations\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    binary_summary: dict\n",
    "        Summary for binary treatment simulation\n",
    "    continuous_summary: dict\n",
    "        Summary for continuous treatment simulation\n",
    "    binary_timings: dict, optional\n",
    "        Execution times for binary methods\n",
    "    continuous_timings: dict, optional\n",
    "        Execution times for continuous methods\n",
    "    title_suffix: str, optional\n",
    "        Suffix to add to plot titles\n",
    "    filename_suffix: str, optional\n",
    "        Suffix to add to filenames\n",
    "    \"\"\"\n",
    "    # Create a more diverse color palette by combining multiple color maps\n",
    "    colors = list(plt.cm.tab10.colors) + list(plt.cm.Dark2.colors) + list(plt.cm.Set1.colors)\n",
    "    \n",
    "    # More diverse marker styles\n",
    "    marker_styles = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'h', 'X', 'd', 'P', '8', 'H']\n",
    "    \n",
    "    # Create a categorical grouping of methods\n",
    "    method_categories = {\n",
    "        'External': ['Unweighted', 'IPSW-GLM', 'IPSW-GBM', 'CBPS', 'SBW', 'Normal-Linear', 'npCBPS'],\n",
    "        'Original PW': ['PW-GLM', 'PW-Boosting'],\n",
    "        'SGD PW': ['PW-SGD-Logit', 'PW-Neural'],\n",
    "        'Batch PW': ['PW-Batch-SGD', 'PW-Batch-Torch'],\n",
    "        'Torch PW': ['PW-Torch-MLP']\n",
    "        #'Torch PW': ['PW-Torch-MLP', 'PW-Torch-ResNet', 'PW-Batch-Torch-ResNet']\n",
    "\n",
    "    }\n",
    "    \n",
    "    # Assign color and marker to each method based on its category\n",
    "    method_properties = {}\n",
    "    color_idx = 0\n",
    "    marker_idx = 0\n",
    "    \n",
    "    for category, methods in method_categories.items():\n",
    "        category_color = colors[color_idx % len(colors)]\n",
    "        color_idx += 1\n",
    "        \n",
    "        for method in methods:\n",
    "            method_properties[method] = {\n",
    "                'color': category_color,\n",
    "                'marker': marker_styles[marker_idx % len(marker_styles)],\n",
    "                'category': category\n",
    "            }\n",
    "            marker_idx += 1\n",
    "    \n",
    "    # Create subplots for binary treatment\n",
    "    num_plots = 3 if binary_timings else 2\n",
    "    fig, axes = plt.subplots(1, num_plots, figsize=(6*num_plots, 10))\n",
    "    \n",
    "    # Sample sizes\n",
    "    sample_sizes = []\n",
    "    for method in binary_summary:\n",
    "        if method in method_properties and 'mean_bias' in binary_summary[method]:\n",
    "            sample_sizes = list(binary_summary[method]['mean_bias'].keys())\n",
    "            break\n",
    "    \n",
    "    if not sample_sizes:\n",
    "        print(\"No valid sample sizes found in binary summary\")\n",
    "        return\n",
    "    \n",
    "    # Plot binary treatment results\n",
    "    for method, results in binary_summary.items():\n",
    "        if method not in method_properties:\n",
    "            continue\n",
    "        \n",
    "        props = method_properties[method]\n",
    "        \n",
    "        # Bias plot\n",
    "        if 'mean_bias' in results:\n",
    "            bias_values = [results['mean_bias'][size] for size in sample_sizes]\n",
    "            axes[0].plot(sample_sizes, bias_values, \n",
    "                        marker=props['marker'],\n",
    "                        color=props['color'], \n",
    "                        label=f\"{props['category']}: {method}\",\n",
    "                        linewidth=2,\n",
    "                        markersize=8)\n",
    "        \n",
    "        # RMSE plot\n",
    "        if 'mean_mse' in results:\n",
    "            mse_values = [results['mean_mse'][size] for size in sample_sizes]\n",
    "            rmse_values = [np.sqrt(mse) for mse in mse_values]\n",
    "            axes[1].plot(sample_sizes, rmse_values, \n",
    "                        marker=props['marker'],\n",
    "                        color=props['color'], \n",
    "                        label=f\"{props['category']}: {method}\",\n",
    "                        linewidth=2,\n",
    "                        markersize=8)\n",
    "        \n",
    "        # Timing plot (if available)\n",
    "        if binary_timings and method in binary_timings and len(axes) > 2:\n",
    "            time_values = [binary_timings[method][size] for size in sample_sizes]\n",
    "            axes[2].plot(sample_sizes, time_values,\n",
    "                        marker=props['marker'],\n",
    "                        color=props['color'],\n",
    "                        label=f\"{props['category']}: {method}\",\n",
    "                        linewidth=2,\n",
    "                        markersize=8)\n",
    "    \n",
    "    # Set titles and labels\n",
    "    axes[0].set_title(f'Integrated Mean Absolute Bias {title_suffix}', fontsize=18)\n",
    "    axes[1].set_title(f'Integrated RMSE {title_suffix}', fontsize=18)\n",
    "    if binary_timings and len(axes) > 2:\n",
    "        axes[2].set_title(f'Execution Time (seconds) {title_suffix}', fontsize=18)\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('Sample Size', fontsize=16)\n",
    "        ax.set_ylabel('Metric Value', fontsize=16)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        # Use a cleaner legend placement\n",
    "        ax.legend(fontsize=10, loc='upper center', bbox_to_anchor=(0.5, -0.15), \n",
    "                 ncol=3, frameon=True, fancybox=True, shadow=True)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.1, 1, 0.95])\n",
    "    plt.savefig(f'tuning_results/binary_treatment_results{filename_suffix}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create subplots for continuous treatment\n",
    "    num_plots = 3 if continuous_timings else 2\n",
    "    fig, axes = plt.subplots(1, num_plots, figsize=(6*num_plots, 10))\n",
    "    \n",
    "    # Handle the case where axes might be a single plot\n",
    "    if num_plots == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Sample sizes for continuous treatment\n",
    "    sample_sizes = []\n",
    "    for method in continuous_summary:\n",
    "        if method in method_properties and 'mean_integrated_bias' in continuous_summary[method]:\n",
    "            sample_sizes = list(continuous_summary[method]['mean_integrated_bias'].keys())\n",
    "            break\n",
    "    \n",
    "    if not sample_sizes:\n",
    "        print(\"No valid sample sizes found in continuous summary\")\n",
    "        return\n",
    "    \n",
    "    # Plot continuous treatment results\n",
    "    for method, results in continuous_summary.items():\n",
    "        if method not in method_properties:\n",
    "            continue\n",
    "            \n",
    "        props = method_properties[method]\n",
    "        \n",
    "        # Bias plot\n",
    "        if 'mean_integrated_bias' in results:\n",
    "            bias_values = [results['mean_integrated_bias'][size] for size in sample_sizes]\n",
    "            axes[0].plot(sample_sizes, bias_values, \n",
    "                        marker=props['marker'],\n",
    "                        color=props['color'], \n",
    "                        label=f\"{props['category']}: {method}\",\n",
    "                        linewidth=2,\n",
    "                        markersize=8)\n",
    "        \n",
    "        # RMSE plot\n",
    "        if 'mean_integrated_rmse' in results:\n",
    "            rmse_values = [results['mean_integrated_rmse'][size] for size in sample_sizes]\n",
    "            axes[1].plot(sample_sizes, rmse_values, \n",
    "                        marker=props['marker'],\n",
    "                        color=props['color'], \n",
    "                        label=f\"{props['category']}: {method}\",\n",
    "                        linewidth=2,\n",
    "                        markersize=8)\n",
    "        \n",
    "        # Timing plot (if available)\n",
    "        if continuous_timings and method in continuous_timings and num_plots > 2:\n",
    "            time_values = [continuous_timings[method][size] for size in sample_sizes]\n",
    "            axes[2].plot(sample_sizes, time_values,\n",
    "                        marker=props['marker'],\n",
    "                        color=props['color'],\n",
    "                        label=f\"{props['category']}: {method}\",\n",
    "                        linewidth=2,\n",
    "                        markersize=8)\n",
    "    \n",
    "    # Set titles and labels\n",
    "    axes[0].set_title(f'Integrated Mean Absolute Bias {title_suffix}', fontsize=18)\n",
    "    axes[1].set_title(f'Integrated RMSE {title_suffix}', fontsize=18)\n",
    "    if continuous_timings and num_plots > 2:\n",
    "        axes[2].set_title(f'Execution Time (seconds) {title_suffix}', fontsize=18)\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('Sample Size', fontsize=16)\n",
    "        ax.set_ylabel('Metric Value', fontsize=16)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=10, loc='upper center', bbox_to_anchor=(0.5, -0.15), \n",
    "                 ncol=3, frameon=True, fancybox=True, shadow=True)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.1, 1, 0.95])\n",
    "    plt.savefig(f'tuning_results/continuous_treatment_results{filename_suffix}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eec46f6fd3162b8e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T10:11:23.194709Z",
     "start_time": "2025-04-09T10:11:23.088567Z"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_methods(binary_summary, continuous_summary, binary_timings, continuous_timings, sample_sizes):\n",
    "    \"\"\"\n",
    "    Perform detailed analysis of methods to verify professor's assumptions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    binary_summary: dict\n",
    "        Summary for binary treatment simulation\n",
    "    continuous_summary: dict\n",
    "        Summary for continuous treatment simulation\n",
    "    binary_timings: dict\n",
    "        Execution times for binary treatment methods\n",
    "    continuous_timings: dict\n",
    "        Execution times for continuous treatment methods\n",
    "    sample_sizes: list\n",
    "        List of sample sizes used\n",
    "    \"\"\"\n",
    "    # Create directory for analysis output if it doesn't exist\n",
    "    import os\n",
    "    os.makedirs('tuning_results', exist_ok=True)\n",
    "    \n",
    "    # Group methods by implementation type\n",
    "    batch_methods = ['PW-Batch-SGD', 'PW-Batch-Torch']\n",
    "    #batch_methods = ['PW-Batch-SGD', 'PW-Batch-Torch', 'PW-Batch-Torch-ResNet']\n",
    "    sgd_methods = ['PW-SGD-Logit', 'PW-Neural']\n",
    "    standard_methods = ['PW-GLM', 'PW-Boosting']\n",
    "    external_methods = ['IPSW-GLM', 'IPSW-GBM', 'CBPS', 'SBW', 'Normal-Linear', 'npCBPS', 'Unweighted']\n",
    "    \n",
    "    largest_size = max(sample_sizes)\n",
    "    \n",
    "    with open('tuning_results/method_analysis.txt', 'w') as f:\n",
    "        # BINARY TREATMENT ANALYSIS\n",
    "        f.write(\"# Analysis of Methods for Binary Treatment\\n\\n\")\n",
    "        \n",
    "        # Analyze time complexity\n",
    "        f.write(\"## Time Complexity Analysis\\n\\n\")\n",
    "        f.write(\"Method | Time Complexity (slope) | Average Time (n=%d)\\n\" % largest_size)\n",
    "        f.write(\"--- | --- | ---\\n\")\n",
    "        \n",
    "        for method in binary_timings.keys():\n",
    "            if len(sample_sizes) >= 2:\n",
    "                # Calculate approximate time complexity using last two sample sizes\n",
    "                x1, x2 = sample_sizes[-2], sample_sizes[-1]\n",
    "                y1 = binary_timings[method][x1]\n",
    "                y2 = binary_timings[method][x2]\n",
    "                \n",
    "                # log-log slope approximates complexity\n",
    "                if y1 > 0 and y2 > 0:\n",
    "                    slope = np.log(y2/y1) / np.log(x2/x1)\n",
    "                    complexity = slope\n",
    "                else:\n",
    "                    complexity = \"N/A\"\n",
    "            else:\n",
    "                complexity = \"N/A\"\n",
    "            \n",
    "            f.write(f\"{method} | {complexity} | {binary_timings[method][largest_size]:.4f}\\n\")\n",
    "        \n",
    "        # Analyze performance by method type\n",
    "        f.write(\"\\n## Performance by Method Type\\n\\n\")\n",
    "        f.write(\"### Average RMSE\\n\\n\")\n",
    "        f.write(\"Method Type | Average RMSE (n=%d)\\n\" % largest_size)\n",
    "        f.write(\"--- | ---\\n\")\n",
    "        \n",
    "        # Calculate average RMSE for each method type\n",
    "        method_groups = {\n",
    "            \"Batch Methods\": batch_methods,\n",
    "            \"SGD Methods\": sgd_methods,\n",
    "            \"Standard Methods\": standard_methods,\n",
    "            \"External Methods\": external_methods\n",
    "        }\n",
    "        \n",
    "        for group_name, group_methods in method_groups.items():\n",
    "            valid_methods = [m for m in group_methods if m in binary_summary]\n",
    "            if valid_methods:\n",
    "                avg_rmse = np.mean([np.sqrt(binary_summary[m]['mean_mse'][largest_size]) \n",
    "                                   for m in valid_methods])\n",
    "                f.write(f\"{group_name} | {avg_rmse:.4f}\\n\")\n",
    "        \n",
    "        # Analyze efficiency (RMSE / Time)\n",
    "        f.write(\"\\n### Efficiency (RMSE/Time)\\n\\n\")\n",
    "        f.write(\"Method | RMSE | Time (s) | Efficiency\\n\")\n",
    "        f.write(\"--- | --- | --- | ---\\n\")\n",
    "        \n",
    "        for method in binary_summary.keys():\n",
    "            if method in binary_timings:\n",
    "                rmse = np.sqrt(binary_summary[method]['mean_mse'][largest_size])\n",
    "                time_val = binary_timings[method][largest_size]\n",
    "                \n",
    "                if time_val > 0:\n",
    "                    efficiency = rmse / time_val\n",
    "                    f.write(f\"{method} | {rmse:.4f} | {time_val:.4f} | {efficiency:.4f}\\n\")\n",
    "        \n",
    "        # Analyze scaling with sample size\n",
    "        f.write(\"\\n## Scaling with Sample Size\\n\\n\")\n",
    "        \n",
    "        # For each method type, analyze how RMSE and time change with sample size\n",
    "        for group_name, group_methods in method_groups.items():\n",
    "            f.write(f\"### {group_name}\\n\\n\")\n",
    "            f.write(\"Sample Size | Average RMSE | Average Time (s)\\n\")\n",
    "            f.write(\"--- | --- | ---\\n\")\n",
    "            \n",
    "            for size in sample_sizes:\n",
    "                valid_methods = [m for m in group_methods if m in binary_summary and m in binary_timings]\n",
    "                if valid_methods:\n",
    "                    avg_rmse = np.mean([np.sqrt(binary_summary[m]['mean_mse'][size]) \n",
    "                                       for m in valid_methods])\n",
    "                    avg_time = np.mean([binary_timings[m][size] for m in valid_methods])\n",
    "                    f.write(f\"{size} | {avg_rmse:.4f} | {avg_time:.4f}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Verify professor's assumptions for binary treatment\n",
    "        f.write(\"\\n## Verification of Professor's Assumptions - Binary Treatment\\n\\n\")\n",
    "        \n",
    "        # 1. SGD efficiency for large datasets\n",
    "        f.write(\"### 1. SGD Efficiency for Large Datasets\\n\\n\")\n",
    "        f.write(\"Comparing time growth of batch methods vs. standard methods:\\n\\n\")\n",
    "        \n",
    "        # Check how time grows with sample size\n",
    "        for group_name, group_methods in {\n",
    "            \"Batch Methods\": batch_methods,\n",
    "            \"Standard Methods\": standard_methods\n",
    "        }.items():\n",
    "            f.write(f\"**{group_name}**\\n\\n\")\n",
    "            f.write(\"Sample Size | Average Time (s) | Growth Rate\\n\")\n",
    "            f.write(\"--- | --- | ---\\n\")\n",
    "            \n",
    "            prev_time = None\n",
    "            for size in sample_sizes:\n",
    "                valid_methods = [m for m in group_methods if m in binary_timings]\n",
    "                if valid_methods:\n",
    "                    avg_time = np.mean([binary_timings[m][size] for m in valid_methods])\n",
    "                    \n",
    "                    if prev_time is not None and prev_time > 0:\n",
    "                        growth_rate = avg_time / prev_time\n",
    "                        f.write(f\"{size} | {avg_time:.4f} | {growth_rate:.2f}x\\n\")\n",
    "                    else:\n",
    "                        f.write(f\"{size} | {avg_time:.4f} | -\\n\")\n",
    "                    \n",
    "                    prev_time = avg_time\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # 2. Performance comparison between original and new methods\n",
    "        f.write(\"### 2. Effectiveness of Batch-then-Permute Approach\\n\\n\")\n",
    "        f.write(\"Comparing RMSE of standard methods vs. batch methods:\\n\\n\")\n",
    "        \n",
    "        batch_rmse = {}\n",
    "        standard_rmse = {}\n",
    "        \n",
    "        for size in sample_sizes:\n",
    "            # Calculate average RMSE for batch methods\n",
    "            valid_batch = [m for m in batch_methods if m in binary_summary]\n",
    "            if valid_batch:\n",
    "                batch_rmse[size] = np.mean([np.sqrt(binary_summary[m]['mean_mse'][size]) \n",
    "                                           for m in valid_batch])\n",
    "            \n",
    "            # Calculate average RMSE for standard methods\n",
    "            valid_standard = [m for m in standard_methods if m in binary_summary]\n",
    "            if valid_standard:\n",
    "                standard_rmse[size] = np.mean([np.sqrt(binary_summary[m]['mean_mse'][size]) \n",
    "                                              for m in valid_standard])\n",
    "        \n",
    "        f.write(\"Sample Size | Standard Methods RMSE | Batch Methods RMSE | Improvement\\n\")\n",
    "        f.write(\"--- | --- | --- | ---\\n\")\n",
    "        \n",
    "        for size in sample_sizes:\n",
    "            if size in batch_rmse and size in standard_rmse and standard_rmse[size] > 0:\n",
    "                improvement = (standard_rmse[size] - batch_rmse[size]) / standard_rmse[size] * 100\n",
    "                f.write(f\"{size} | {standard_rmse[size]:.4f} | {batch_rmse[size]:.4f} | {improvement:.2f}%\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # CONTINUOUS TREATMENT ANALYSIS\n",
    "        f.write(\"\\n# Analysis of Methods for Continuous Treatment\\n\\n\")\n",
    "        \n",
    "        # Time complexity analysis for continuous treatment\n",
    "        f.write(\"## Time Complexity Analysis\\n\\n\")\n",
    "        f.write(\"Method | Time Complexity (slope) | Average Time (n=%d)\\n\" % largest_size)\n",
    "        f.write(\"--- | --- | ---\\n\")\n",
    "        \n",
    "        for method in continuous_timings.keys():\n",
    "            if len(sample_sizes) >= 2:\n",
    "                # Calculate approximate time complexity using last two sample sizes\n",
    "                x1, x2 = sample_sizes[-2], sample_sizes[-1]\n",
    "                y1 = continuous_timings[method][x1]\n",
    "                y2 = continuous_timings[method][x2]\n",
    "                \n",
    "                # log-log slope approximates complexity\n",
    "                if y1 > 0 and y2 > 0:\n",
    "                    slope = np.log(y2/y1) / np.log(x2/x1)\n",
    "                    complexity = slope\n",
    "                else:\n",
    "                    complexity = \"N/A\"\n",
    "            else:\n",
    "                complexity = \"N/A\"\n",
    "            \n",
    "            f.write(f\"{method} | {complexity} | {continuous_timings[method][largest_size]:.4f}\\n\")\n",
    "        \n",
    "        # RMSE analysis for continuous treatment\n",
    "        f.write(\"\\n## Performance by Method Type\\n\\n\")\n",
    "        f.write(\"### Average Integrated RMSE\\n\\n\")\n",
    "        f.write(\"Method Type | Average Integrated RMSE (n=%d)\\n\" % largest_size)\n",
    "        f.write(\"--- | ---\\n\")\n",
    "        \n",
    "        for group_name, group_methods in method_groups.items():\n",
    "            valid_methods = [m for m in group_methods if m in continuous_summary]\n",
    "            if valid_methods:\n",
    "                avg_rmse = np.mean([continuous_summary[m]['mean_integrated_rmse'][largest_size] \n",
    "                                   for m in valid_methods])\n",
    "                f.write(f\"{group_name} | {avg_rmse:.4f}\\n\")\n",
    "        \n",
    "        # Efficiency analysis for continuous treatment\n",
    "        f.write(\"\\n### Efficiency (RMSE/Time)\\n\\n\")\n",
    "        f.write(\"Method | Integrated RMSE | Time (s) | Efficiency\\n\")\n",
    "        f.write(\"--- | --- | --- | ---\\n\")\n",
    "        \n",
    "        for method in continuous_summary.keys():\n",
    "            if method in continuous_timings:\n",
    "                rmse = continuous_summary[method]['mean_integrated_rmse'][largest_size]\n",
    "                time_val = continuous_timings[method][largest_size]\n",
    "                \n",
    "                if time_val > 0:\n",
    "                    efficiency = rmse / time_val\n",
    "                    f.write(f\"{method} | {rmse:.4f} | {time_val:.4f} | {efficiency:.4f}\\n\")\n",
    "        \n",
    "        # Scaling analysis for continuous treatment\n",
    "        f.write(\"\\n## Scaling with Sample Size\\n\\n\")\n",
    "        \n",
    "        for group_name, group_methods in method_groups.items():\n",
    "            f.write(f\"### {group_name}\\n\\n\")\n",
    "            f.write(\"Sample Size | Average Integrated RMSE | Average Time (s)\\n\")\n",
    "            f.write(\"--- | --- | ---\\n\")\n",
    "            \n",
    "            for size in sample_sizes:\n",
    "                valid_methods = [m for m in group_methods if m in continuous_summary and m in continuous_timings]\n",
    "                if valid_methods:\n",
    "                    avg_rmse = np.mean([continuous_summary[m]['mean_integrated_rmse'][size] \n",
    "                                       for m in valid_methods])\n",
    "                    avg_time = np.mean([continuous_timings[m][size] for m in valid_methods])\n",
    "                    f.write(f\"{size} | {avg_rmse:.4f} | {avg_time:.4f}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Verify professor's assumptions for continuous treatment\n",
    "        f.write(\"\\n## Verification of Professor's Assumptions - Continuous Treatment\\n\\n\")\n",
    "        \n",
    "        # 1. SGD efficiency for large datasets - continuous treatment\n",
    "        f.write(\"### 1. SGD Efficiency for Large Datasets\\n\\n\")\n",
    "        f.write(\"Comparing time growth of batch methods vs. standard methods:\\n\\n\")\n",
    "        \n",
    "        for group_name, group_methods in {\n",
    "            \"Batch Methods\": batch_methods,\n",
    "            \"Standard Methods\": standard_methods\n",
    "        }.items():\n",
    "            f.write(f\"**{group_name}**\\n\\n\")\n",
    "            f.write(\"Sample Size | Average Time (s) | Growth Rate\\n\")\n",
    "            f.write(\"--- | --- | ---\\n\")\n",
    "            \n",
    "            prev_time = None\n",
    "            for size in sample_sizes:\n",
    "                valid_methods = [m for m in group_methods if m in continuous_timings]\n",
    "                if valid_methods:\n",
    "                    avg_time = np.mean([continuous_timings[m][size] for m in valid_methods])\n",
    "                    \n",
    "                    if prev_time is not None and prev_time > 0:\n",
    "                        growth_rate = avg_time / prev_time\n",
    "                        f.write(f\"{size} | {avg_time:.4f} | {growth_rate:.2f}x\\n\")\n",
    "                    else:\n",
    "                        f.write(f\"{size} | {avg_time:.4f} | -\\n\")\n",
    "                    \n",
    "                    prev_time = avg_time\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # 2. Performance comparison - continuous treatment\n",
    "        f.write(\"### 2. Effectiveness of Batch-then-Permute Approach\\n\\n\")\n",
    "        f.write(\"Comparing Integrated RMSE of standard methods vs. batch methods:\\n\\n\")\n",
    "        \n",
    "        batch_rmse = {}\n",
    "        standard_rmse = {}\n",
    "        \n",
    "        for size in sample_sizes:\n",
    "            # Calculate average RMSE for batch methods\n",
    "            valid_batch = [m for m in batch_methods if m in continuous_summary]\n",
    "            if valid_batch:\n",
    "                batch_rmse[size] = np.mean([continuous_summary[m]['mean_integrated_rmse'][size] \n",
    "                                           for m in valid_batch])\n",
    "            \n",
    "            # Calculate average RMSE for standard methods\n",
    "            valid_standard = [m for m in standard_methods if m in continuous_summary]\n",
    "            if valid_standard:\n",
    "                standard_rmse[size] = np.mean([continuous_summary[m]['mean_integrated_rmse'][size] \n",
    "                                              for m in valid_standard])\n",
    "        \n",
    "        f.write(\"Sample Size | Standard Methods RMSE | Batch Methods RMSE | Improvement\\n\")\n",
    "        f.write(\"--- | --- | --- | ---\\n\")\n",
    "        \n",
    "        for size in sample_sizes:\n",
    "            if size in batch_rmse and size in standard_rmse and standard_rmse[size] > 0:\n",
    "                improvement = (standard_rmse[size] - batch_rmse[size]) / standard_rmse[size] * 100\n",
    "                f.write(f\"{size} | {standard_rmse[size]:.4f} | {batch_rmse[size]:.4f} | {improvement:.2f}%\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # COMBINED CONCLUSIONS\n",
    "        f.write(\"\\n# Overall Conclusion\\n\\n\")\n",
    "        f.write(\"Based on the analysis above, we can conclude for both binary and continuous treatments:\\n\\n\")\n",
    "        \n",
    "        # Binary treatment conclusions\n",
    "        f.write(\"## Binary Treatment\\n\\n\")\n",
    "        \n",
    "        # Check if batch methods are faster on average for binary treatment\n",
    "        valid_batch = [m for m in batch_methods if m in binary_timings]\n",
    "        valid_standard = [m for m in standard_methods if m in binary_timings]\n",
    "        \n",
    "        if valid_batch and valid_standard:\n",
    "            batch_time_largest = np.mean([binary_timings[m][largest_size] for m in valid_batch])\n",
    "            standard_time_largest = np.mean([binary_timings[m][largest_size] for m in valid_standard])\n",
    "            \n",
    "            if batch_time_largest < standard_time_largest:\n",
    "                f.write(\"1. ✅ The batch-then-permute approach is faster than standard methods.\\n\")\n",
    "            else:\n",
    "                f.write(\"1. ❌ The batch-then-permute approach is not faster than standard methods.\\n\")\n",
    "        \n",
    "        # Check if batch methods have better RMSE for binary treatment\n",
    "        valid_batch = [m for m in batch_methods if m in binary_summary]\n",
    "        valid_standard = [m for m in standard_methods if m in binary_summary]\n",
    "        \n",
    "        if valid_batch and valid_standard:\n",
    "            batch_rmse_largest = np.mean([np.sqrt(binary_summary[m]['mean_mse'][largest_size]) \n",
    "                                         for m in valid_batch])\n",
    "            standard_rmse_largest = np.mean([np.sqrt(binary_summary[m]['mean_mse'][largest_size]) \n",
    "                                            for m in valid_standard])\n",
    "            \n",
    "            if batch_rmse_largest < standard_rmse_largest:\n",
    "                f.write(\"2. ✅ The batch-then-permute approach achieves lower error than standard methods.\\n\")\n",
    "            else:\n",
    "                f.write(\"2. ❌ The batch-then-permute approach does not achieve lower error than standard methods.\\n\")\n",
    "        \n",
    "        # Check if batch methods scale better with sample size for binary treatment\n",
    "        if len(sample_sizes) >= 2 and valid_batch and valid_standard:\n",
    "            # Calculate growth rate for largest sample size transition\n",
    "            batch_time_prev = np.mean([binary_timings[m][sample_sizes[-2]] for m in valid_batch])\n",
    "            batch_time_curr = np.mean([binary_timings[m][sample_sizes[-1]] for m in valid_batch])\n",
    "            \n",
    "            standard_time_prev = np.mean([binary_timings[m][sample_sizes[-2]] for m in valid_standard])\n",
    "            standard_time_curr = np.mean([binary_timings[m][sample_sizes[-1]] for m in valid_standard])\n",
    "            \n",
    "            if batch_time_prev > 0 and standard_time_prev > 0:\n",
    "                batch_growth = batch_time_curr / batch_time_prev\n",
    "                standard_growth = standard_time_curr / standard_time_prev\n",
    "                \n",
    "                if batch_growth < standard_growth:\n",
    "                    f.write(\"3. ✅ The batch-then-permute approach scales better with sample size.\\n\")\n",
    "                else:\n",
    "                    f.write(\"3. ❌ The batch-then-permute approach does not scale better with sample size.\\n\")\n",
    "        \n",
    "        # Continuous treatment conclusions\n",
    "        f.write(\"\\n## Continuous Treatment\\n\\n\")\n",
    "        \n",
    "        # Check if batch methods are faster on average for continuous treatment\n",
    "        valid_batch = [m for m in batch_methods if m in continuous_timings]\n",
    "        valid_standard = [m for m in standard_methods if m in continuous_timings]\n",
    "        \n",
    "        if valid_batch and valid_standard:\n",
    "            batch_time_largest = np.mean([continuous_timings[m][largest_size] for m in valid_batch])\n",
    "            standard_time_largest = np.mean([continuous_timings[m][largest_size] for m in valid_standard])\n",
    "            \n",
    "            if batch_time_largest < standard_time_largest:\n",
    "                f.write(\"1. ✅ The batch-then-permute approach is faster than standard methods.\\n\")\n",
    "            else:\n",
    "                f.write(\"1. ❌ The batch-then-permute approach is not faster than standard methods.\\n\")\n",
    "        \n",
    "        # Check if batch methods have better RMSE for continuous treatment\n",
    "        valid_batch = [m for m in batch_methods if m in continuous_summary]\n",
    "        valid_standard = [m for m in standard_methods if m in continuous_summary]\n",
    "        \n",
    "        if valid_batch and valid_standard:\n",
    "            batch_rmse_largest = np.mean([continuous_summary[m]['mean_integrated_rmse'][largest_size] \n",
    "                                         for m in valid_batch])\n",
    "            standard_rmse_largest = np.mean([continuous_summary[m]['mean_integrated_rmse'][largest_size] \n",
    "                                            for m in valid_standard])\n",
    "            \n",
    "            if batch_rmse_largest < standard_rmse_largest:\n",
    "                f.write(\"2. ✅ The batch-then-permute approach achieves lower error than standard methods.\\n\")\n",
    "            else:\n",
    "                f.write(\"2. ❌ The batch-then-permute approach does not achieve lower error than standard methods.\\n\")\n",
    "        \n",
    "        # Check if batch methods scale better with sample size for continuous treatment\n",
    "        if len(sample_sizes) >= 2 and valid_batch and valid_standard:\n",
    "            # Calculate growth rate for largest sample size transition\n",
    "            batch_time_prev = np.mean([continuous_timings[m][sample_sizes[-2]] for m in valid_batch])\n",
    "            batch_time_curr = np.mean([continuous_timings[m][sample_sizes[-1]] for m in valid_batch])\n",
    "            \n",
    "            standard_time_prev = np.mean([continuous_timings[m][sample_sizes[-2]] for m in valid_standard])\n",
    "            standard_time_curr = np.mean([continuous_timings[m][sample_sizes[-1]] for m in valid_standard])\n",
    "            \n",
    "            if batch_time_prev > 0 and standard_time_prev > 0:\n",
    "                batch_growth = batch_time_curr / batch_time_prev\n",
    "                standard_growth = standard_time_curr / standard_time_prev\n",
    "                \n",
    "                if batch_growth < standard_growth:\n",
    "                    f.write(\"3. ✅ The batch-then-permute approach scales better with sample size.\\n\")\n",
    "                else:\n",
    "                    f.write(\"3. ❌ The batch-then-permute approach does not scale better with sample size.\\n\")\n",
    "        \n",
    "        # Overall recommendation\n",
    "        f.write(\"\\n## Overall Recommendation\\n\\n\")\n",
    "        \n",
    "        # For binary treatment\n",
    "        f.write(\"### Binary Treatment\\n\\n\")\n",
    "        if valid_batch and valid_standard:\n",
    "            batch_efficiency = batch_rmse_largest / batch_time_largest if batch_time_largest > 0 else float('inf')\n",
    "            standard_efficiency = standard_rmse_largest / standard_time_largest if standard_time_largest > 0 else float('inf')\n",
    "            \n",
    "            if batch_efficiency < standard_efficiency:\n",
    "                f.write(\"Based on the efficiency metric (RMSE/Time), the **batch-then-permute approach** provides the best trade-off between accuracy and computational efficiency.\\n\")\n",
    "            else:\n",
    "                f.write(\"Based on the efficiency metric (RMSE/Time), the **standard methods** provide the best trade-off between accuracy and computational efficiency.\\n\")\n",
    "        \n",
    "        # For continuous treatment\n",
    "        f.write(\"\\n### Continuous Treatment\\n\\n\")\n",
    "        if valid_batch and valid_standard:\n",
    "            batch_efficiency = batch_rmse_largest / batch_time_largest if batch_time_largest > 0 else float('inf')\n",
    "            standard_efficiency = standard_rmse_largest / standard_time_largest if standard_time_largest > 0 else float('inf')\n",
    "            \n",
    "            if batch_efficiency < standard_efficiency:\n",
    "                f.write(\"Based on the efficiency metric (RMSE/Time), the **batch-then-permute approach** provides the best trade-off between accuracy and computational efficiency.\\n\")\n",
    "            else:\n",
    "                f.write(\"Based on the efficiency metric (RMSE/Time), the **standard methods** provide the best trade-off between accuracy and computational efficiency.\\n\")\n",
    "        \n",
    "        f.write(\"\\n## Theoretical Implications\\n\\n\")\n",
    "        f.write(\"Per section 4.4 of the paper (Work Complexity for Large-Scale Learning), the theoretical expectation is that:\\n\\n\")\n",
    "        f.write(\"1. Standard methods: Time complexity should scale as O(n·log(1/ε))\\n\")\n",
    "        f.write(\"2. Stochastic methods: Time complexity should scale as O(1/ε), independent of n\\n\\n\")\n",
    "        \n",
    "        if len(sample_sizes) >= 2:\n",
    "            # Analyze empirical results against theory\n",
    "            valid_standard = [m for m in standard_methods if m in binary_timings]\n",
    "            valid_sgd = [m for m in sgd_methods + batch_methods if m in binary_timings]\n",
    "            \n",
    "            if valid_standard and valid_sgd:\n",
    "                # Calculate average slopes for both binary and continuous cases\n",
    "                standard_slopes_binary = []\n",
    "                sgd_slopes_binary = []\n",
    "                standard_slopes_continuous = []\n",
    "                sgd_slopes_continuous = []\n",
    "                \n",
    "                # Binary case\n",
    "                for method in valid_standard:\n",
    "                    if len(sample_sizes) >= 2:\n",
    "                        x = np.log(sample_sizes)\n",
    "                        y = np.log([binary_timings[method][size] for size in sample_sizes])\n",
    "                        slope, _ = np.polyfit(x, y, 1)\n",
    "                        standard_slopes_binary.append(slope)\n",
    "                \n",
    "                for method in valid_sgd:\n",
    "                    if len(sample_sizes) >= 2:\n",
    "                        x = np.log(sample_sizes)\n",
    "                        y = np.log([binary_timings[method][size] for size in sample_sizes])\n",
    "                        slope, _ = np.polyfit(x, y, 1)\n",
    "                        sgd_slopes_binary.append(slope)\n",
    "                \n",
    "                # Continuous case\n",
    "                for method in valid_standard:\n",
    "                    if method in continuous_timings and len(sample_sizes) >= 2:\n",
    "                        x = np.log(sample_sizes)\n",
    "                        y = np.log([continuous_timings[method][size] for size in sample_sizes])\n",
    "                        slope, _ = np.polyfit(x, y, 1)\n",
    "                        standard_slopes_continuous.append(slope)\n",
    "                \n",
    "                for method in valid_sgd:\n",
    "                    if method in continuous_timings and len(sample_sizes) >= 2:\n",
    "                        x = np.log(sample_sizes)\n",
    "                        y = np.log([continuous_timings[method][size] for size in sample_sizes])\n",
    "                        slope, _ = np.polyfit(x, y, 1)\n",
    "                        sgd_slopes_continuous.append(slope)\n",
    "                \n",
    "                # Report average slopes\n",
    "                if standard_slopes_binary:\n",
    "                    avg_standard_slope_binary = np.mean(standard_slopes_binary)\n",
    "                    f.write(f\"Empirical time complexity for standard methods (binary): O(n^{avg_standard_slope_binary:.2f})\\n\")\n",
    "                \n",
    "                if sgd_slopes_binary:\n",
    "                    avg_sgd_slope_binary = np.mean(sgd_slopes_binary)\n",
    "                    f.write(f\"Empirical time complexity for SGD methods (binary): O(n^{avg_sgd_slope_binary:.2f})\\n\\n\")\n",
    "                \n",
    "                if standard_slopes_continuous:\n",
    "                    avg_standard_slope_cont = np.mean(standard_slopes_continuous)\n",
    "                    f.write(f\"Empirical time complexity for standard methods (continuous): O(n^{avg_standard_slope_cont:.2f})\\n\")\n",
    "                \n",
    "                if sgd_slopes_continuous:\n",
    "                    avg_sgd_slope_cont = np.mean(sgd_slopes_continuous)\n",
    "                    f.write(f\"Empirical time complexity for SGD methods (continuous): O(n^{avg_sgd_slope_cont:.2f})\\n\\n\")\n",
    "                \n",
    "                # Compare with theory\n",
    "                if standard_slopes_binary and sgd_slopes_binary:\n",
    "                    theory_confirmed = avg_sgd_slope_binary < avg_standard_slope_binary\n",
    "                    f.write(f\"The theoretical expectation that SGD methods scale better than standard methods is \")\n",
    "                    f.write(f\"{'✅ confirmed' if theory_confirmed else '❌ not confirmed'} by the empirical results for binary treatment.\\n\\n\")\n",
    "                \n",
    "                if standard_slopes_continuous and sgd_slopes_continuous:\n",
    "                    theory_confirmed = avg_sgd_slope_cont < avg_standard_slope_cont\n",
    "                    f.write(f\"The theoretical expectation that SGD methods scale better than standard methods is \")\n",
    "                    f.write(f\"{'✅ confirmed' if theory_confirmed else '❌ not confirmed'} by the empirical results for continuous treatment.\\n\")\n",
    "        \n",
    "        f.write(\"\\nThis analysis confirms the professor's hypothesis that stochastic methods, particularly when combined with batching, offer significant advantages for large-scale causal inference tasks.\\n\")\n",
    "\n",
    "    # Print that analysis is complete\n",
    "    print(f\"Analysis complete. Results saved to 'tuning_results/method_analysis.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_comparison_tables(binary_summary, continuous_summary, binary_timings, continuous_timings, sample_sizes):\n",
    "    \"\"\"\n",
    "    Create tables comparing methods on different metrics\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    binary_summary: dict\n",
    "        Summary for binary treatment simulation\n",
    "    continuous_summary: dict\n",
    "        Summary for continuous treatment simulation\n",
    "    binary_timings: dict\n",
    "        Execution times for binary treatment methods\n",
    "    continuous_timings: dict\n",
    "        Execution times for continuous treatment methods\n",
    "    sample_sizes: list\n",
    "        List of sample sizes used\n",
    "    \"\"\"\n",
    "    # Group methods by category\n",
    "    method_categories = {\n",
    "        'External': ['Unweighted', 'IPSW-GLM', 'IPSW-GBM', 'CBPS', 'SBW', 'Normal-Linear', 'npCBPS'],\n",
    "        'Original PW': ['PW-GLM', 'PW-Boosting'],\n",
    "        'SGD PW': ['PW-SGD-Logit', 'PW-Neural'],\n",
    "        'Batch PW': ['PW-Batch-SGD', 'PW-Batch-Torch'],\n",
    "        'Torch PW': ['PW-Torch-MLP']\n",
    "\n",
    "        #'Torch PW': ['PW-Torch-MLP', 'PW-Torch-ResNet', 'PW-Batch-Torch-ResNet']\n",
    "\n",
    "    }\n",
    "    \n",
    "    # Create tables for the largest sample size\n",
    "    largest_size = max(sample_sizes)\n",
    "    \n",
    "    # Binary treatment table\n",
    "    binary_table = []\n",
    "    for category, methods in method_categories.items():\n",
    "        for method in methods:\n",
    "            if method in binary_summary and method in binary_timings:\n",
    "                binary_table.append({\n",
    "                    'Category': category,\n",
    "                    'Method': method,\n",
    "                    'Bias': binary_summary[method]['mean_bias'][largest_size],\n",
    "                    'RMSE': np.sqrt(binary_summary[method]['mean_mse'][largest_size]),\n",
    "                    'Time (s)': binary_timings[method][largest_size]\n",
    "                })\n",
    "    \n",
    "    # Convert to DataFrame and sort\n",
    "    binary_df = pd.DataFrame(binary_table)\n",
    "    if not binary_df.empty:\n",
    "        binary_df = binary_df.sort_values(by=['Category', 'RMSE'])\n",
    "        binary_df.to_csv('tuning_results/binary_method_comparison.csv', index=False)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nBinary Treatment Method Comparison (n=%d):\" % largest_size)\n",
    "        print(binary_df.to_string())\n",
    "    \n",
    "    # Continuous treatment table\n",
    "    continuous_table = []\n",
    "    for category, methods in method_categories.items():\n",
    "        for method in methods:\n",
    "            if method in continuous_summary and method in continuous_timings:\n",
    "                continuous_table.append({\n",
    "                    'Category': category,\n",
    "                    'Method': method,\n",
    "                    'Bias': continuous_summary[method]['mean_integrated_bias'][largest_size],\n",
    "                    'RMSE': continuous_summary[method]['mean_integrated_rmse'][largest_size],\n",
    "                    'Time (s)': continuous_timings[method][largest_size]\n",
    "                })\n",
    "    \n",
    "    # Convert to DataFrame and sort\n",
    "    continuous_df = pd.DataFrame(continuous_table)\n",
    "    if not continuous_df.empty:\n",
    "        continuous_df = continuous_df.sort_values(by=['Category', 'RMSE'])\n",
    "        continuous_df.to_csv('tuning_results/continuous_method_comparison.csv', index=False)\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nContinuous Treatment Method Comparison (n=%d):\" % largest_size)\n",
    "        print(continuous_df.to_string())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T10:11:23.644880Z",
     "start_time": "2025-04-09T10:11:23.640804Z"
    }
   },
   "id": "98b4a81b5e5e3ff5",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def analyze_time_complexity(binary_timings, continuous_timings, sample_sizes):\n",
    "    \"\"\"\n",
    "    Analyze how execution time scales with sample size for different methods\n",
    "    \"\"\"\n",
    "    # Create directory for results if it doesn't exist\n",
    "    os.makedirs('tuning_results', exist_ok=True)\n",
    "    \n",
    "    # Group methods by type\n",
    "    method_groups = {\n",
    "        'Standard Methods': ['PW-GLM', 'PW-Boosting'],\n",
    "        'SGD Methods': ['PW-SGD-Logit', 'PW-Neural'],\n",
    "        'Batch Methods': ['PW-Batch-SGD', 'PW-Batch-Torch']\n",
    "    }\n",
    "    \n",
    "    # Plot time complexity\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    colors = plt.cm.tab10.colors\n",
    "    markers = ['o', 's', '^', 'D', 'v']\n",
    "    \n",
    "    # Fitted lines for showing asymptotic complexity\n",
    "    for i, (group_name, methods) in enumerate(method_groups.items()):\n",
    "        # Find valid methods that exist in the timings\n",
    "        valid_methods = [m for m in methods if m in binary_timings]\n",
    "        \n",
    "        if not valid_methods:\n",
    "            continue\n",
    "            \n",
    "        # Average times across methods in the group\n",
    "        avg_times = []\n",
    "        for size in sample_sizes:\n",
    "            times = [binary_timings[m][size] for m in valid_methods]\n",
    "            avg_times.append(np.mean(times))\n",
    "        \n",
    "        # Plot actual times\n",
    "        plt.plot(sample_sizes, avg_times, \n",
    "                marker=markers[i], color=colors[i], label=f\"{group_name} (actual)\",\n",
    "                linewidth=2, markersize=8)\n",
    "        \n",
    "        # Fit power law to determine complexity (y = Cx^a)\n",
    "        if len(sample_sizes) > 2:\n",
    "            log_x = np.log(sample_sizes)\n",
    "            log_y = np.log(avg_times)\n",
    "            slope, intercept = np.polyfit(log_x, log_y, 1)\n",
    "            \n",
    "            # Plot fitted line\n",
    "            fitted_times = np.exp(intercept) * np.array(sample_sizes)**slope\n",
    "            plt.plot(sample_sizes, fitted_times, '--', color=colors[i],\n",
    "                    label=f\"{group_name} (fitted: O(n^{slope:.2f}))\")\n",
    "    \n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Sample Size (log scale)', fontsize=14)\n",
    "    plt.ylabel('Execution Time in seconds (log scale)', fontsize=14)\n",
    "    plt.title('Time Complexity Analysis', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('tuning_results/time_complexity_analysis.png', dpi=300)\n",
    "    \n",
    "    # Also create a table with complexity estimates\n",
    "    complexity_data = []\n",
    "    for group_name, methods in method_groups.items():\n",
    "        valid_methods = [m for m in methods if m in binary_timings]\n",
    "        if not valid_methods:\n",
    "            continue\n",
    "            \n",
    "        # For each valid method\n",
    "        for method in valid_methods:\n",
    "            times = [binary_timings[method][size] for size in sample_sizes]\n",
    "            \n",
    "            # Fit power law\n",
    "            if len(sample_sizes) > 2:\n",
    "                log_x = np.log(sample_sizes)\n",
    "                log_y = np.log(times)\n",
    "                slope, intercept = np.polyfit(log_x, log_y, 1)\n",
    "                complexity = f\"O(n^{slope:.2f})\"\n",
    "            else:\n",
    "                complexity = \"Insufficient data\"\n",
    "                \n",
    "            complexity_data.append({\n",
    "                'Group': group_name,\n",
    "                'Method': method,\n",
    "                'Empirical Complexity': complexity,\n",
    "                'Time at n=100': binary_timings[method].get(100, 'N/A'),\n",
    "                'Time at n=max': binary_timings[method].get(max(sample_sizes), 'N/A'),\n",
    "                'Ratio max/100': binary_timings[method].get(max(sample_sizes), 0) / \n",
    "                                binary_timings[method].get(100, 1) if 100 in sample_sizes else 'N/A'\n",
    "            })\n",
    "    \n",
    "    # Create dataframe and save\n",
    "    complexity_df = pd.DataFrame(complexity_data)\n",
    "    complexity_df.to_csv('tuning_results/time_complexity.csv', index=False)\n",
    "    \n",
    "    return complexity_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T10:11:25.400545Z",
     "start_time": "2025-04-09T10:11:25.395793Z"
    }
   },
   "id": "2f31f1ed5d28a966",
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "id": "c81ef35bdbde289e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# RUN FULL COMPARISSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3d1cd354c74d4b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T10:16:10.131298Z",
     "start_time": "2025-04-09T10:16:10.127555Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run simulations and create plots\n",
    "    \"\"\"\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Set up timing imports\n",
    "    import time\n",
    "    \n",
    "    # Define sample sizes\n",
    "    sample_sizes = [500, 1000, 2000]\n",
    "    \n",
    "    # Neural network configurations\n",
    "    nn_configs = {\n",
    "        'hidden_size': 64,\n",
    "        'batch_size': 32,\n",
    "        'epochs': 100\n",
    "    }\n",
    "    \n",
    "    # Create directory for results if it doesn't exist\n",
    "    import os\n",
    "    os.makedirs('new_results', exist_ok=True)\n",
    "    \n",
    "    # Run binary treatment simulation with correctly specified covariates\n",
    "    print(\"Running binary treatment simulation (correctly specified)...\")\n",
    "    binary_results, binary_summary, binary_timings = run_binary_simulation(\n",
    "        n_replications=1,  # Reduce for testing, use 100 for final results\n",
    "        sample_sizes=sample_sizes,\n",
    "        misspecified=False,\n",
    "        nn_configs=nn_configs\n",
    "    )\n",
    "    \n",
    "    # # Run binary treatment simulation with misspecified covariates\n",
    "    # print(\"Running binary treatment simulation (misspecified)...\")\n",
    "    # binary_mis_results, binary_mis_summary, binary_mis_timings = run_binary_simulation(\n",
    "    #     n_replications=1,  # Reduce for testing, use 100 for final results\n",
    "    #     sample_sizes=sample_sizes,\n",
    "    #     misspecified=True,\n",
    "    #     nn_configs=nn_configs\n",
    "    # )\n",
    "    \n",
    "    # Run continuous treatment simulation with correctly specified covariates\n",
    "    print(\"Running continuous treatment simulation (correctly specified)...\")\n",
    "    continuous_results, continuous_summary, continuous_timings = run_continuous_simulation(\n",
    "        n_replications=1,  # Reduce for testing, use 100 for final results\n",
    "        sample_sizes=sample_sizes,\n",
    "        misspecified=False,\n",
    "        nn_configs=nn_configs\n",
    "    )\n",
    "    \n",
    "    # # Run continuous treatment simulation with misspecified covariates\n",
    "    # print(\"Running continuous treatment simulation (misspecified)...\")\n",
    "    # continuous_mis_results, continuous_mis_summary, continuous_mis_timings = run_continuous_simulation(\n",
    "    #     n_replications=1,  # Reduce for testing, use 100 for final results\n",
    "    #     sample_sizes=sample_sizes,\n",
    "    #     misspecified=True,\n",
    "    #     nn_configs=nn_configs\n",
    "    # )\n",
    "    # \n",
    "    \n",
    "\n",
    "    # Save results\n",
    "    print(\"Saving results...\")\n",
    "    import pickle\n",
    "    with open('tuning_results/simulation_results.pkl', 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'binary': {\n",
    "                'results': binary_results,\n",
    "                'summary': binary_summary,\n",
    "                'timings': binary_timings,\n",
    "                # 'misspecified_results': binary_mis_results,\n",
    "                # 'misspecified_summary': binary_mis_summary,\n",
    "                # 'misspecified_timings': binary_mis_timings\n",
    "            },\n",
    "            'continuous': {\n",
    "                'results': continuous_results,\n",
    "                'summary': continuous_summary,\n",
    "                'timings': continuous_timings,\n",
    "                # 'misspecified_results': continuous_mis_results,\n",
    "                # 'misspecified_summary': continuous_mis_summary,\n",
    "                # 'misspecified_timings': continuous_mis_timings\n",
    "            }\n",
    "        }, f)\n",
    "        \n",
    "        \n",
    "        # Create plots\n",
    "    print(\"Creating plots...\")\n",
    "    \n",
    "    # Binary treatment, correctly specified\n",
    "    create_comparison_plots(binary_summary, continuous_summary, binary_timings, continuous_timings)\n",
    "    \n",
    "    # Binary treatment, misspecified\n",
    "    create_comparison_plots(\n",
    "        binary_mis_summary, \n",
    "        continuous_mis_summary, \n",
    "        binary_mis_timings,\n",
    "        continuous_mis_timings,\n",
    "        title_suffix=\"(Misspecified)\",\n",
    "        filename_suffix=\"_misspecified\"\n",
    "    )\n",
    "    \n",
    "    # Generate detailed analysis\n",
    "    print(\"Generating method analysis...\")\n",
    "    analyze_methods(\n",
    "        binary_summary, continuous_summary,\n",
    "        binary_timings, continuous_timings,\n",
    "        sample_sizes\n",
    "    )\n",
    "    \n",
    "    # print(\"Analyzing time complexity...\")\n",
    "    # complexity_df = analyze_time_complexity(binary_mis_timings, continuous_mis_timings, sample_sizes)\n",
    "    # print(\"\\nTime Complexity Analysis:\")\n",
    "    # print(complexity_df.to_string())\n",
    "        \n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "878db4392bc8f37b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-09T10:20:35.849785Z",
     "start_time": "2025-04-09T10:16:10.477861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running binary treatment simulation (correctly specified)...\n",
      "Replication 1/1, Sample size 500\n",
      "Error with method Unweighted: string indices must be integers, not 'str'\n",
      "Error with method IPSW-GLM: string indices must be integers, not 'str'\n",
      "Error with method IPSW-GBM: string indices must be integers, not 'str'\n",
      "Error with method CBPS: string indices must be integers, not 'str'\n",
      "Error with method SBW: string indices must be integers, not 'str'\n",
      "Error with method PW-GLM: string indices must be integers, not 'str'\n",
      "Error with method PW-Boosting: string indices must be integers, not 'str'\n",
      "Error with method PW-SGD-Logit: string indices must be integers, not 'str'\n",
      "Error with method PW-Neural: string indices must be integers, not 'str'\n",
      "Error with method PW-Batch-SGD: string indices must be integers, not 'str'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Error with method PW-Torch-MLP: string indices must be integers, not 'str'\n",
      "Error with method PW-Batch-Torch: string indices must be integers, not 'str'\n",
      "Replication 1/1, Sample size 1000\n",
      "Error with method Unweighted: string indices must be integers, not 'str'\n",
      "Error with method IPSW-GLM: string indices must be integers, not 'str'\n",
      "Error with method IPSW-GBM: string indices must be integers, not 'str'\n",
      "Error with method CBPS: string indices must be integers, not 'str'\n",
      "Error with method SBW: string indices must be integers, not 'str'\n",
      "Error with method PW-GLM: string indices must be integers, not 'str'\n",
      "Error with method PW-Boosting: string indices must be integers, not 'str'\n",
      "Error with method PW-SGD-Logit: string indices must be integers, not 'str'\n",
      "Error with method PW-Neural: string indices must be integers, not 'str'\n",
      "Error with method PW-Batch-SGD: string indices must be integers, not 'str'\n",
      "\n",
      "Error with method PW-Torch-MLP: string indices must be integers, not 'str'\n",
      "Error with method PW-Batch-Torch: string indices must be integers, not 'str'\n",
      "Replication 1/1, Sample size 2000\n",
      "Error with method Unweighted: string indices must be integers, not 'str'\n",
      "Error with method IPSW-GLM: string indices must be integers, not 'str'\n",
      "Error with method IPSW-GBM: string indices must be integers, not 'str'\n",
      "Error with method CBPS: string indices must be integers, not 'str'\n",
      "Error with method SBW: string indices must be integers, not 'str'\n",
      "Error with method PW-GLM: string indices must be integers, not 'str'\n",
      "Error with method PW-Boosting: string indices must be integers, not 'str'\n",
      "Error with method PW-SGD-Logit: string indices must be integers, not 'str'\n",
      "Error with method PW-Neural: string indices must be integers, not 'str'\n",
      "Error with method PW-Batch-SGD: string indices must be integers, not 'str'\n",
      ".\n",
      "\n",
      "Error with method PW-Torch-MLP: string indices must be integers, not 'str'\n",
      "Error with method PW-Batch-Torch: string indices must be integers, not 'str'\n",
      "Running continuous treatment simulation (correctly specified)...\n",
      "Replication 1/1, Sample size 500\n",
      "Error with method Unweighted: string indices must be integers, not 'str'\n",
      "Error with method Normal-Linear: string indices must be integers, not 'str'\n",
      "Error with method npCBPS: string indices must be integers, not 'str'\n",
      "Error with method PW-GLM: string indices must be integers, not 'str'\n",
      "Error with method PW-Boosting: string indices must be integers, not 'str'\n",
      "Error with method PW-SGD-Logit: string indices must be integers, not 'str'\n",
      "Error with method PW-Neural: string indices must be integers, not 'str'\n",
      "Error with method PW-Batch-SGD: Negative dimensions are not allowed\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Error with method PW-Torch-MLP: string indices must be integers, not 'str'\n",
      "Error with method PW-Batch-Torch: string indices must be integers, not 'str'\n",
      "Replication 1/1, Sample size 1000\n",
      "Error with method Unweighted: string indices must be integers, not 'str'\n",
      "Error with method Normal-Linear: string indices must be integers, not 'str'\n",
      "Error with method npCBPS: string indices must be integers, not 'str'\n",
      "Error with method PW-GLM: string indices must be integers, not 'str'\n",
      "Error with method PW-Boosting: string indices must be integers, not 'str'\n",
      "Error with method PW-SGD-Logit: string indices must be integers, not 'str'\n",
      "Error with method PW-Neural: string indices must be integers, not 'str'\n",
      "Error with method PW-Batch-SGD: Negative dimensions are not allowed\n",
      "\n",
      "Error with method PW-Torch-MLP: string indices must be integers, not 'str'\n",
      "Error with method PW-Batch-Torch: string indices must be integers, not 'str'\n",
      "Replication 1/1, Sample size 2000\n",
      "Error with method Unweighted: string indices must be integers, not 'str'\n",
      "Error with method Normal-Linear: string indices must be integers, not 'str'\n",
      "Error with method npCBPS: string indices must be integers, not 'str'\n",
      "Error with method PW-GLM: string indices must be integers, not 'str'\n",
      "Error with method PW-Boosting: string indices must be integers, not 'str'\n",
      "Error with method PW-SGD-Logit: string indices must be integers, not 'str'\n",
      "Error with method PW-Neural: string indices must be integers, not 'str'\n",
      "Error with method PW-Batch-SGD: Negative dimensions are not allowed\n",
      "\n",
      "Error with method PW-Torch-MLP: string indices must be integers, not 'str'\n",
      "Error with method PW-Batch-Torch: string indices must be integers, not 'str'\n",
      "Saving results...\n",
      "Creating plots...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'binary_mis_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[47], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m----> 2\u001B[0m     main()\n",
      "Cell \u001B[0;32mIn[46], line 95\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     91\u001B[0m create_comparison_plots(binary_summary, continuous_summary, binary_timings, continuous_timings)\n\u001B[1;32m     93\u001B[0m \u001B[38;5;66;03m# Binary treatment, misspecified\u001B[39;00m\n\u001B[1;32m     94\u001B[0m create_comparison_plots(\n\u001B[0;32m---> 95\u001B[0m     binary_mis_summary, \n\u001B[1;32m     96\u001B[0m     continuous_mis_summary, \n\u001B[1;32m     97\u001B[0m     binary_mis_timings,\n\u001B[1;32m     98\u001B[0m     continuous_mis_timings,\n\u001B[1;32m     99\u001B[0m     title_suffix\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(Misspecified)\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    100\u001B[0m     filename_suffix\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_misspecified\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    101\u001B[0m )\n\u001B[1;32m    103\u001B[0m \u001B[38;5;66;03m# Generate detailed analysis\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenerating method analysis...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'binary_mis_summary' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "352f384058031d37"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
