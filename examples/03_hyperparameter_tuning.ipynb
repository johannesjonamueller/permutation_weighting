{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# HYPERPARAMETER TUNING FOR PROPENSITY SCORE WEIGHTING"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2edd4fe26fde9915"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import expit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import warnings\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm  # For progress tracking TODO: Remove if not needed\n",
    "import os\n",
    "import os\n",
    "import itertools\n",
    "from matplotlib.lines import Line2D\n",
    "import json\n",
    "\n",
    "# For better plot styling\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('colorblind')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if PyTorch is available\n",
    "TORCH_AVAILABLE = False\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "    print(\"PyTorch is available\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch is not available\")\n",
    "\n",
    "# Import the permutation weighting implementation\n",
    "from permutation_weighting.estimator import PW\n",
    "\n",
    "# Function to generate Kang-Schafer data\n",
    "def generate_kang_schafer_binary(n=1000, seed=42, misspecified=False):\n",
    "    \"\"\"\n",
    "    Generate data according to the Kang-Schafer setup with binary treatment\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n: int\n",
    "        Number of observations\n",
    "    seed: int\n",
    "        Random seed\n",
    "    misspecified: bool\n",
    "        Whether to return the misspecified transformations of covariates\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    df: pd.DataFrame\n",
    "        Data frame with columns: X1-X4 (covariates), A (treatment), Y (outcome), \n",
    "        Y1 (potential outcome under treatment), Y0 (potential outcome under control),\n",
    "        and X1_mis to X4_mis (misspecified covariates, if requested)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Generate covariates\n",
    "    X = np.random.normal(0, 1, size=(n, 4))\n",
    "    \n",
    "    # Treatment assignment\n",
    "    ps_linear = X[:, 0] - 0.5 * X[:, 1] + 0.25 * X[:, 2] + 0.1 * X[:, 3]\n",
    "    ps = expit(ps_linear)\n",
    "    A = np.random.binomial(1, ps, size=n)\n",
    "    \n",
    "    # Generate potential outcomes\n",
    "    Y1 = 210 + 1 + 27.4*X[:, 0] + 13.7*X[:, 1] + 13.7*X[:, 2] + 13.7*X[:, 3] + np.random.normal(0, 1, size=n)\n",
    "    Y0 = 210 + 0 + 27.4*X[:, 0] + 13.7*X[:, 1] + 13.7*X[:, 2] + 13.7*X[:, 3] + np.random.normal(0, 1, size=n)\n",
    "    \n",
    "    # Observed outcome\n",
    "    Y = A * Y1 + (1 - A) * Y0\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'X1': X[:, 0],\n",
    "        'X2': X[:, 1],\n",
    "        'X3': X[:, 2],\n",
    "        'X4': X[:, 3],\n",
    "        'A': A,\n",
    "        'Y': Y,\n",
    "        'Y1': Y1,\n",
    "        'Y0': Y0\n",
    "    })\n",
    "    \n",
    "    # Add misspecified covariates if requested\n",
    "    if misspecified:\n",
    "        df['X1_mis'] = np.exp(X[:, 0]/2)\n",
    "        df['X2_mis'] = X[:, 1] / (1 + np.exp(X[:, 0])) + 10\n",
    "        df['X3_mis'] = (X[:, 0] * X[:, 2] / 25 + 0.6)**3\n",
    "        df['X4_mis'] = (X[:, 1] + X[:, 3] + 20)**2\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to calculate true ATE\n",
    "def true_ate(df):\n",
    "    \"\"\"Calculate the true average treatment effect\"\"\"\n",
    "    return np.mean(df['Y1'] - df['Y0'])\n",
    "\n",
    "# Function to estimate ATE using PW weights\n",
    "def estimate_ate(df, weights):\n",
    "    \"\"\"Estimate ATE using PW weights\"\"\"\n",
    "    # Normalize weights to sum to n\n",
    "    n = len(df)\n",
    "    normalized_weights = weights * n / np.sum(weights)\n",
    "    \n",
    "    # Weighted ATE estimate\n",
    "    treated_indices = df['A'] == 1\n",
    "    control_indices = df['A'] == 0\n",
    "    \n",
    "    weighted_treated_mean = np.sum(df.loc[treated_indices, 'Y'] * normalized_weights[treated_indices]) / np.sum(normalized_weights[treated_indices])\n",
    "    weighted_control_mean = np.sum(df.loc[control_indices, 'Y'] * normalized_weights[control_indices]) / np.sum(normalized_weights[control_indices])\n",
    "    \n",
    "    return weighted_treated_mean - weighted_control_mean"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "baa9c80b88a74ac5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Function to perform hyperparameter tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbcc9e758583f88c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Function to perform cross-validation and record performance\n",
    "def cross_validate_hyperparams(df, method, param_grid, method_name, n_folds=5, use_sgd=False, use_torch=False, seed=42):\n",
    "    \"\"\"\n",
    "    Perform cross-validation to tune hyperparameters\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: pd.DataFrame\n",
    "        Data frame with columns: X1-X4, A, Y, Y1, Y0\n",
    "    method: str\n",
    "        Classification method ('logit', 'sgd_logit', 'neural_net', etc.)\n",
    "    param_grid: dict\n",
    "        Dictionary mapping parameter names to lists of values to try\n",
    "    method_name: str\n",
    "        Name to identify this method in results\n",
    "    n_folds: int\n",
    "        Number of cross-validation folds\n",
    "    use_sgd: bool\n",
    "        Whether to use SGD-based training\n",
    "    use_torch: bool\n",
    "        Whether to use PyTorch-based training\n",
    "    seed: int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results: pd.DataFrame\n",
    "        Data frame with results of cross-validation\n",
    "    \"\"\"\n",
    "    # Extract features and target\n",
    "    feature_cols = [col for col in df.columns if col.startswith('X') and not col.endswith('_mis')]\n",
    "    X = df[feature_cols].values\n",
    "    A = df['A'].values\n",
    "    y = df['Y'].values\n",
    "    \n",
    "    # Set up cross-validation\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # True ATE\n",
    "    true_ate_value = true_ate(df)\n",
    "    \n",
    "    # Generate all parameter combinations\n",
    "    param_names = list(param_grid.keys())\n",
    "    param_values = list(itertools.product(*param_grid.values()))\n",
    "    \n",
    "    # Results storage\n",
    "    results = []\n",
    "    \n",
    "    # Progress tracking\n",
    "    total_combos = len(param_values)\n",
    "    print(f\"Running {total_combos} parameter combinations for {method_name}...\")\n",
    "    \n",
    "    # Track time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Loop through parameter combinations\n",
    "    for i, values in enumerate(param_values):\n",
    "        params = dict(zip(param_names, values))\n",
    "        \n",
    "        # Print progress\n",
    "        if i % max(1, total_combos // 10) == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Combo {i+1}/{total_combos} - Elapsed: {elapsed:.1f}s\")\n",
    "        \n",
    "        # Store fold results\n",
    "        fold_results = []\n",
    "        \n",
    "        # Run cross-validation\n",
    "        for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "            # Split data\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            A_train, A_test = A[train_idx], A[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            # Train PW model\n",
    "            try:\n",
    "                pw_result = PW(\n",
    "                    A=A_train, \n",
    "                    X=X_train, \n",
    "                    classifier=method, \n",
    "                    classifier_params=params,\n",
    "                    estimand='ATE',\n",
    "                    num_replicates=1,  # Use 1 for speed during tuning\n",
    "                    use_sgd=use_sgd,\n",
    "                    use_torch=use_torch\n",
    "                )\n",
    "                \n",
    "                # Save in-sample metrics\n",
    "                in_sample_mse = pw_result['train'].get('MSEEvaluator', np.nan)\n",
    "                in_sample_logloss = pw_result['train'].get('LogLossEvaluator', np.nan)\n",
    "                \n",
    "                # Apply to test set\n",
    "                eval_data = {'A': A_test, 'X': X_test}\n",
    "                eval_pw_result = PW(\n",
    "                    A=A_train, \n",
    "                    X=X_train, \n",
    "                    classifier=method, \n",
    "                    classifier_params=params,\n",
    "                    estimand='ATE',\n",
    "                    num_replicates=1,\n",
    "                    eval_data=eval_data,\n",
    "                    use_sgd=use_sgd,\n",
    "                    use_torch=use_torch\n",
    "                )\n",
    "                \n",
    "                # Get out-of-sample metrics\n",
    "                out_sample_mse = eval_pw_result['eval'].get('MSEEvaluator', np.nan)\n",
    "                out_sample_logloss = eval_pw_result['eval'].get('LogLossEvaluator', np.nan)\n",
    "                \n",
    "                # Calculate estimated ATE\n",
    "                df_train = pd.DataFrame({\n",
    "                    'X1': X_train[:, 0], 'X2': X_train[:, 1], \n",
    "                    'X3': X_train[:, 2], 'X4': X_train[:, 3],\n",
    "                    'A': A_train, 'Y': y_train\n",
    "                })\n",
    "                est_ate = estimate_ate(df_train, pw_result['weights'])\n",
    "                \n",
    "                # Calculate absolute error in ATE\n",
    "                ate_error = abs(est_ate - true_ate_value)\n",
    "                \n",
    "                # Save results\n",
    "                fold_result = {\n",
    "                    'fold': fold,\n",
    "                    'in_sample_mse': in_sample_mse,\n",
    "                    'in_sample_logloss': in_sample_logloss,\n",
    "                    'out_sample_mse': out_sample_mse,\n",
    "                    'out_sample_logloss': out_sample_logloss,\n",
    "                    'ate_error': ate_error,\n",
    "                    'converged': pw_result.get('convergence_info', {}).get('converged', True)\n",
    "                }\n",
    "                \n",
    "                fold_results.append(fold_result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error with params {params} for {method_name} in fold {fold}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Skip if no valid results\n",
    "        if not fold_results:\n",
    "            continue\n",
    "        \n",
    "        # Average results across folds\n",
    "        mean_results = {\n",
    "            k: np.mean([r[k] for r in fold_results if k in r and not np.isnan(r[k])]) \n",
    "            for k in ['in_sample_mse', 'in_sample_logloss', 'out_sample_mse', 'out_sample_logloss', 'ate_error']\n",
    "        }\n",
    "        \n",
    "        # Calculate standard errors\n",
    "        std_results = {\n",
    "            f\"{k}_std\": np.std([r[k] for r in fold_results if k in r and not np.isnan(r[k])]) / np.sqrt(len(fold_results))\n",
    "            for k in ['in_sample_mse', 'in_sample_logloss', 'out_sample_mse', 'out_sample_logloss', 'ate_error']\n",
    "        }\n",
    "        \n",
    "        # Save parameter combination results with method name\n",
    "        result = {**params, **mean_results, **std_results, 'method': method_name}\n",
    "        \n",
    "        # Convert any lists in params to strings for safe storage\n",
    "        for k, v in params.items():\n",
    "            if isinstance(v, list):\n",
    "                result[k] = str(v)\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return pd.DataFrame(results)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c38096d73eb5eaf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "89a7bfd4ae618d57"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# VISUALIZATION"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd2935949f34c500"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_model_performance_plots(results_dict, output_dir='tuning_results'):\n",
    "    \"\"\"\n",
    "    Generate plots comparing model performance\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_dict: dict\n",
    "        Dictionary mapping method names to DataFrames of results\n",
    "    output_dir: str\n",
    "        Directory to save plots\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Combine results\n",
    "    combined_results = pd.concat(results_dict.values(), ignore_index=True)\n",
    "    \n",
    "    # Define colors for each method\n",
    "    methods = combined_results['method'].unique()\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(methods)))\n",
    "    method_colors = dict(zip(methods, colors))\n",
    "    \n",
    "    # Create legend elements\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', \n",
    "               markerfacecolor=method_colors[method], markersize=10, label=method)\n",
    "        for method in methods\n",
    "    ]\n",
    "    \n",
    "    # Plot 1: ATE Error vs. In-sample Log Loss\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for method in methods:\n",
    "        method_data = combined_results[combined_results['method'] == method]\n",
    "        plt.scatter(\n",
    "            method_data['in_sample_logloss'], \n",
    "            method_data['ate_error'],\n",
    "            alpha=0.7, s=60, color=method_colors[method],\n",
    "            label=method\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('PW log-loss (in-sample)', fontsize=14)\n",
    "    plt.ylabel('Absolute Causal Error (in-sample)', fontsize=14)\n",
    "    plt.title('In-sample Causal Error vs. In-sample PW Loss', fontsize=16)\n",
    "    plt.legend(handles=legend_elements, loc='upper left', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/in_sample_comparison.png', dpi=300)\n",
    "    \n",
    "    # Plot 2: ATE Error vs. Out-of-sample Log Loss\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for method in methods:\n",
    "        method_data = combined_results[combined_results['method'] == method]\n",
    "        plt.scatter(\n",
    "            method_data['out_sample_logloss'], \n",
    "            method_data['ate_error'],\n",
    "            alpha=0.7, s=60, color=method_colors[method],\n",
    "            label=method\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('PW log-loss (out-of-sample)', fontsize=14)\n",
    "    plt.ylabel('Absolute Causal Error (in-sample)', fontsize=14)\n",
    "    plt.title('In-sample Causal Error vs. Out-of-sample PW Loss', fontsize=16)\n",
    "    plt.legend(handles=legend_elements, loc='upper left', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/out_sample_comparison.png', dpi=300)\n",
    "    \n",
    "    # Find best configuration for each method\n",
    "    best_configs = {}\n",
    "    for method in methods:\n",
    "        method_data = combined_results[combined_results['method'] == method]\n",
    "        if not method_data.empty:\n",
    "            best_idx = method_data['ate_error'].idxmin()\n",
    "            best_configs[method] = {\n",
    "                'ate_error': method_data.loc[best_idx, 'ate_error'],\n",
    "                'in_sample_logloss': method_data.loc[best_idx, 'in_sample_logloss'],\n",
    "                'out_sample_logloss': method_data.loc[best_idx, 'out_sample_logloss']\n",
    "            }\n",
    "            \n",
    "            # Add parameters (excluding method and metrics)\n",
    "            for col in method_data.columns:\n",
    "                if col not in ['method', 'in_sample_mse', 'in_sample_logloss', \n",
    "                               'out_sample_mse', 'out_sample_logloss', 'ate_error',\n",
    "                               'in_sample_mse_std', 'in_sample_logloss_std', \n",
    "                               'out_sample_mse_std', 'out_sample_logloss_std', 'ate_error_std']:\n",
    "                    best_configs[method][col] = method_data.loc[best_idx, col]\n",
    "    \n",
    "    # Save best configurations to JSON\n",
    "    with open(f'{output_dir}/best_configs.json', 'w') as f:\n",
    "        json.dump(best_configs, f, indent=4)\n",
    "    \n",
    "    # Print best configurations\n",
    "    print(\"\\nBest configuration for each method:\")\n",
    "    for method, config in best_configs.items():\n",
    "        print(f\"{method}:\")\n",
    "        print(f\"  ATE Error: {config['ate_error']:.4f}\")\n",
    "        print(f\"  In-sample Log Loss: {config['in_sample_logloss']:.4f}\")\n",
    "        print(f\"  Out-of-sample Log Loss: {config['out_sample_logloss']:.4f}\")\n",
    "        \n",
    "        # Print parameters (excluding metrics)\n",
    "        print(\"  Parameters:\")\n",
    "        for k, v in config.items():\n",
    "            if k not in ['ate_error', 'in_sample_logloss', 'out_sample_logloss']:\n",
    "                print(f\"    {k}: {v}\")\n",
    "        print()\n",
    "    \n",
    "    return best_configs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ab7cf00e86f654e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FINAL MODEL EVALUATION"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "373d12c9c4225895"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def final_model_evaluation(df, methods_to_eval, output_dir='tuning_results', seed=42):\n",
    "    \"\"\"\n",
    "    Train final models and generate ROC curves\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: pd.DataFrame\n",
    "        Data frame with columns: X1-X4, A, Y, Y1, Y0\n",
    "    methods_to_eval: list\n",
    "        List of (method_name, method_config) tuples to evaluate\n",
    "    output_dir: str\n",
    "        Directory to save plots\n",
    "    seed: int\n",
    "        Random seed\n",
    "    \"\"\"\n",
    "    # Extract features and target\n",
    "    feature_cols = [col for col in df.columns if col.startswith('X') and not col.endswith('_mis')]\n",
    "    X = df[feature_cols].values\n",
    "    A = df['A'].values\n",
    "    \n",
    "    # Set up cross-validation\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Results for ROC curves\n",
    "    roc_results = {method_name: {'tpr': [], 'fpr': [], 'auc': []} for method_name, _ in methods_to_eval}\n",
    "    \n",
    "    # Define colors for ROC curves\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(methods_to_eval)))\n",
    "    method_colors = dict(zip([name for name, _ in methods_to_eval], colors))\n",
    "    \n",
    "    # Evaluate each method\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "        # Split data\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        A_train, A_test = A[train_idx], A[test_idx]\n",
    "        \n",
    "        for method_name, config in methods_to_eval:\n",
    "            try:\n",
    "                # Convert string parameters back to appropriate types if needed\n",
    "                fixed_params = config['params'].copy()\n",
    "                for k, v in fixed_params.items():\n",
    "                    if isinstance(v, str) and v.startswith('[') and v.endswith(']'):\n",
    "                        # Convert string representation of list back to list\n",
    "                        try:\n",
    "                            fixed_params[k] = eval(v)\n",
    "                        except:\n",
    "                            pass\n",
    "                \n",
    "                # Train model\n",
    "                pw_result = PW(\n",
    "                    A=A_train, \n",
    "                    X=X_train, \n",
    "                    classifier=config['method'], \n",
    "                    classifier_params=fixed_params,\n",
    "                    estimand='ATE',\n",
    "                    num_replicates=1,\n",
    "                    use_sgd=config.get('use_sgd', False),\n",
    "                    use_torch=config.get('use_torch', False)\n",
    "                )\n",
    "                \n",
    "                # Create balanced datasets for ROC calculation\n",
    "                # For simplicity, we'll create a stacked dataset of permuted and observed data\n",
    "                n = len(X_test)\n",
    "                \n",
    "                # Create permuted data\n",
    "                np.random.seed(seed + fold)\n",
    "                permuted_idx = np.random.permutation(n)\n",
    "                \n",
    "                # Features for classification\n",
    "                X_eval = np.vstack([X_test, X_test])\n",
    "                A_eval = np.concatenate([A_test[permuted_idx], A_test])\n",
    "                \n",
    "                # Labels: 1 for permuted, 0 for observed\n",
    "                y_true = np.concatenate([np.ones(n), np.zeros(n)])\n",
    "                \n",
    "                # Apply model to get probabilities\n",
    "                eval_data = {'A': A_eval, 'X': X_eval}\n",
    "                pw_eval = PW(\n",
    "                    A=A_train, \n",
    "                    X=X_train, \n",
    "                    classifier=config['method'], \n",
    "                    classifier_params=fixed_params,\n",
    "                    estimand='ATE',\n",
    "                    eval_data=eval_data,\n",
    "                    num_replicates=1,\n",
    "                    use_sgd=config.get('use_sgd', False),\n",
    "                    use_torch=config.get('use_torch', False)\n",
    "                )\n",
    "                \n",
    "                # Extract probabilities from weights\n",
    "                weights = pw_eval['weights']\n",
    "                y_score = weights / (1 + weights)\n",
    "                \n",
    "                # Calculate ROC\n",
    "                fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                \n",
    "                # Store results\n",
    "                roc_results[method_name]['fpr'].append(fpr)\n",
    "                roc_results[method_name]['tpr'].append(tpr)\n",
    "                roc_results[method_name]['auc'].append(roc_auc)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error with {method_name} in fold {fold}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Plot ROC curves\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for method_name, result in roc_results.items():\n",
    "        if not result['auc']:\n",
    "            print(f\"No valid results for {method_name}\")\n",
    "            continue\n",
    "            \n",
    "        # Calculate mean AUC\n",
    "        mean_auc = np.mean(result['auc'])\n",
    "        \n",
    "        # Interpolate to get average ROC curve\n",
    "        mean_fpr = np.linspace(0, 1, 100)\n",
    "        tprs = []\n",
    "        \n",
    "        for i in range(len(result['fpr'])):\n",
    "            interp_tpr = np.interp(mean_fpr, result['fpr'][i], result['tpr'][i])\n",
    "            interp_tpr[0] = 0.0\n",
    "            tprs.append(interp_tpr)\n",
    "        \n",
    "        mean_tpr = np.mean(tprs, axis=0)\n",
    "        mean_tpr[-1] = 1.0\n",
    "        \n",
    "        # Plot average ROC curve\n",
    "        color = method_colors.get(method_name, 'b')\n",
    "        plt.plot(\n",
    "            mean_fpr, mean_tpr,\n",
    "            label=f'{method_name} (AUC = {mean_auc:.3f})',\n",
    "            lw=2, alpha=0.8, color=color\n",
    "        )\n",
    "    \n",
    "    # Plot diagonal\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "    \n",
    "    # Configure plot\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)\n",
    "    plt.title('ROC Curves for Different Models', fontsize=16)\n",
    "    plt.legend(loc=\"lower right\", fontsize=12)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/roc_curves.png', dpi=300)\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f7d31242c1e509f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MAIN FUNCTION "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6675180556718744"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Generate Kang-Schafer data\n",
    "    print(\"Generating Kang-Schafer data...\")\n",
    "    df = generate_kang_schafer_binary(n=2000, seed=42, misspecified=False)\n",
    "    print(f\"Generated data with {len(df)} observations\")\n",
    "    print(f\"True ATE: {true_ate(df):.4f}\")\n",
    "    \n",
    "    # Set up hyperparameter grids for different models\n",
    "    # Expanded parameter options for a 15-minute run\n",
    "    \n",
    "    # 1. Standard Logistic Regression (for baseline)\n",
    "    standard_logit_params = {\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'solver': ['saga', 'liblinear']\n",
    "    }\n",
    "    \n",
    "    # 2. SGD Logistic Regression\n",
    "    sgd_logit_params = {\n",
    "        'loss': ['log_loss'],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'learning_rate': ['constant', 'optimal', 'adaptive'],\n",
    "        'eta0': [0.001, 0.01, 0.1]\n",
    "    }\n",
    "    \n",
    "    # 3. Neural Network with SGD\n",
    "    neural_net_params = {\n",
    "        'hidden_layer_sizes': [(10,), (20,), (10, 5)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "        'max_iter': [200]\n",
    "    }\n",
    "    \n",
    "    # 4. Gradient Boosting\n",
    "    boosting_params = {\n",
    "        'n_estimators': [50, 100],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [2, 3, 4]\n",
    "    }\n",
    "    \n",
    "    # 5. PyTorch models (if available)\n",
    "    torch_logistic_params = {\n",
    "        'learning_rate': [0.001, 0.01, 0.1],\n",
    "        'l2_reg': [0.0001, 0.001, 0.01],\n",
    "        'epochs': [50]\n",
    "    }\n",
    "    \n",
    "    torch_mlp_params = {\n",
    "        'hidden_dims': [[16], [32], [16, 8]],\n",
    "        'learning_rate': [0.001, 0.01],\n",
    "        'l2_reg': [0.0001, 0.001],\n",
    "        'epochs': [50]\n",
    "    }\n",
    "    \n",
    "    # Run hyperparameter tuning\n",
    "    all_results = {}\n",
    "    \n",
    "    # 1. Standard Logistic Regression (for baseline)\n",
    "    print(\"\\nTuning Standard Logistic Regression...\")\n",
    "    std_logit_results = cross_validate_hyperparams(\n",
    "        df, 'logit', standard_logit_params, \n",
    "        method_name='Standard Logistic',\n",
    "        n_folds=5, seed=42\n",
    "    )\n",
    "    all_results['Standard Logistic'] = std_logit_results\n",
    "    \n",
    "    # 2. Gradient Boosting\n",
    "    print(\"\\nTuning Gradient Boosting...\")\n",
    "    boosting_results = cross_validate_hyperparams(\n",
    "        df, 'boosting', boosting_params, \n",
    "        method_name='Gradient Boosting',\n",
    "        n_folds=5, seed=42\n",
    "    )\n",
    "    all_results['Gradient Boosting'] = boosting_results\n",
    "    \n",
    "    # 3. SGD Logistic Regression\n",
    "    print(\"\\nTuning SGD Logistic Regression...\")\n",
    "    sgd_results = cross_validate_hyperparams(\n",
    "        df, 'logit', sgd_logit_params, \n",
    "        method_name='SGD Logistic',\n",
    "        n_folds=5, use_sgd=True, seed=42\n",
    "    )\n",
    "    all_results['SGD Logistic'] = sgd_results\n",
    "    \n",
    "    # 4. Neural Network with SGD\n",
    "    print(\"\\nTuning Neural Network...\")\n",
    "    nn_results = cross_validate_hyperparams(\n",
    "        df, 'neural_net', neural_net_params, \n",
    "        method_name='Neural Network',\n",
    "        n_folds=5, use_sgd=True, seed=42\n",
    "    )\n",
    "    all_results['Neural Network'] = nn_results\n",
    "    \n",
    "    # 5. PyTorch models (if available)\n",
    "    if TORCH_AVAILABLE:\n",
    "        print(\"\\nTuning PyTorch Logistic Regression...\")\n",
    "        torch_logistic_results = cross_validate_hyperparams(\n",
    "            df, 'logistic', torch_logistic_params, \n",
    "            method_name='PyTorch Logistic',\n",
    "            n_folds=5, use_torch=True, seed=42\n",
    "        )\n",
    "        all_results['PyTorch Logistic'] = torch_logistic_results\n",
    "        \n",
    "        print(\"\\nTuning PyTorch MLP...\")\n",
    "        torch_mlp_results = cross_validate_hyperparams(\n",
    "            df, 'mlp', torch_mlp_params, \n",
    "            method_name='PyTorch MLP',\n",
    "            n_folds=5, use_torch=True, seed=42\n",
    "        )\n",
    "        all_results['PyTorch MLP'] = torch_mlp_results\n",
    "    \n",
    "    # Generate combined performance plots\n",
    "    best_configs = generate_model_performance_plots(all_results)\n",
    "    \n",
    "    # Set up methods for ROC curve comparison\n",
    "    methods_to_eval = []\n",
    "    \n",
    "    # Add each method with its best configuration\n",
    "    for method_name, best_config in best_configs.items():\n",
    "        # Extract parameters for this method\n",
    "        params = {k: v for k, v in best_config.items() \n",
    "                 if k not in ['ate_error', 'in_sample_logloss', 'out_sample_logloss', 'method']}\n",
    "        \n",
    "        # Determine the base method and whether to use SGD or PyTorch\n",
    "        use_sgd = False\n",
    "        use_torch = False\n",
    "        \n",
    "        if method_name == 'SGD Logistic' or method_name == 'Neural Network':\n",
    "            use_sgd = True\n",
    "            base_method = 'logit' if method_name == 'SGD Logistic' else 'neural_net'\n",
    "        elif method_name == 'PyTorch Logistic' or method_name == 'PyTorch MLP':\n",
    "            use_torch = True\n",
    "            base_method = 'logistic' if method_name == 'PyTorch Logistic' else 'mlp'\n",
    "        elif method_name == 'Gradient Boosting':\n",
    "            base_method = 'boosting'\n",
    "        else:  # Standard Logistic\n",
    "            base_method = 'logit'\n",
    "        \n",
    "        methods_to_eval.append((\n",
    "            method_name,\n",
    "            {'method': base_method, 'params': params, 'use_sgd': use_sgd, 'use_torch': use_torch}\n",
    "        ))\n",
    "    \n",
    "    # Generate ROC curves\n",
    "    print(\"\\nGenerating ROC curves...\")\n",
    "    final_model_evaluation(df, methods_to_eval)\n",
    "    \n",
    "    print(\"\\nHyperparameter tuning complete. Results saved to 'tuning_results' directory.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92628b9fedbda7e3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a9079a15485f07e7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
