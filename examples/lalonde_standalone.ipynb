{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "13f3ed42d3931e33"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a883875a0658d106"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1407348398.py, line 293)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[1], line 293\u001B[0;36m\u001B[0m\n\u001B[0;31m    alpha=0.001,\u001B[0m\n\u001B[0m                ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, SplineTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class PermutationWeighting:\n",
    "    \"\"\"Permutation Weighting implementation as described in Arbour et al. (2020)\"\"\"\n",
    "    \n",
    "    def __init__(self, A, X, classifier='logit', estimand='ATE', num_replicates=100):\n",
    "        \"\"\"\n",
    "        Initialize Permutation Weighting\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        A : array-like\n",
    "            Treatment variable (binary or continuous)\n",
    "        X : array-like\n",
    "            Covariate matrix\n",
    "        classifier : str\n",
    "            Classifier type ('logit', 'boosting', 'sgd', 'mlp')\n",
    "        estimand : str\n",
    "            Estimand type ('ATE' or 'ATT')\n",
    "        num_replicates : int\n",
    "            Number of replicates to run\n",
    "        \"\"\"\n",
    "        self.A = np.array(A).reshape(-1)\n",
    "        self.X = np.array(X)\n",
    "        self.classifier = classifier\n",
    "        self.estimand = estimand.upper()\n",
    "        self.num_replicates = num_replicates\n",
    "        \n",
    "        # Check data\n",
    "        self._check_data()\n",
    "        \n",
    "        # Get data factory\n",
    "        if self._is_binary_treatment() and self.estimand == 'ATE':\n",
    "            self.factory = self._binary_ate_factory()\n",
    "        elif self._is_binary_treatment() and self.estimand == 'ATT':\n",
    "            self.factory = self._att_factory()\n",
    "        else:\n",
    "            self.factory = self._ate_factory()\n",
    "        \n",
    "        # Get trainer factory\n",
    "        self.trainer = self._get_trainer_factory()\n",
    "        \n",
    "        # Compute weights\n",
    "        self.weights = self._compute_weights()\n",
    "    \n",
    "    def _check_data(self):\n",
    "        \"\"\"Validate the input data\"\"\"\n",
    "        if self.A.ndim != 1:\n",
    "            raise ValueError(\"A must be a 1-dimensional array\")\n",
    "        \n",
    "        if self.X.ndim != 2:\n",
    "            raise ValueError(\"X must be a 2-dimensional array (matrix)\")\n",
    "        \n",
    "        if len(self.A) != self.X.shape[0]:\n",
    "            raise ValueError(f\"A and X must have the same number of observations\")\n",
    "    \n",
    "    def _is_binary_treatment(self):\n",
    "        \"\"\"Check if treatment is binary\"\"\"\n",
    "        return len(np.unique(self.A)) == 2\n",
    "    \n",
    "    def _ate_factory(self):\n",
    "        \"\"\"Factory for ATE estimand\"\"\"\n",
    "        N = len(self.A)\n",
    "        \n",
    "        def factory():\n",
    "            # Generate bootstrap indices\n",
    "            idx = np.random.choice(N, N, replace=True)\n",
    "            \n",
    "            # For permuted data, independently sample treatment and covariates\n",
    "            perm_idx = np.random.permutation(N)\n",
    "            pA = self.A[perm_idx]\n",
    "            pX = self.X[idx]\n",
    "            \n",
    "            # For observed data, use same bootstrap indices\n",
    "            oA = self.A[idx]\n",
    "            oX = self.X[idx]\n",
    "            \n",
    "            return {\n",
    "                'permuted': {'C': 1, 'A': pA, 'X': pX},\n",
    "                'observed': {'C': 0, 'A': oA, 'X': oX}\n",
    "            }\n",
    "        \n",
    "        return factory\n",
    "    \n",
    "    def _att_factory(self):\n",
    "        \"\"\"Factory for ATT estimand\"\"\"\n",
    "        N = len(self.A)\n",
    "        A1_idx = np.where(self.A == 1)[0]\n",
    "        \n",
    "        if len(A1_idx) == 0:\n",
    "            raise ValueError('A must take the value of one at least once for the ATT.')\n",
    "        \n",
    "        def factory():\n",
    "            # Sample treatment with replacement\n",
    "            pA = np.random.choice(self.A, N, replace=True)\n",
    "            \n",
    "            # Sample covariates from treated units\n",
    "            p_idx = np.random.choice(A1_idx, N, replace=True)\n",
    "            pX = self.X[p_idx]\n",
    "            \n",
    "            # Sample observed data indices\n",
    "            idx = np.random.choice(N, N, replace=True)\n",
    "            \n",
    "            return {\n",
    "                'permuted': {'C': 1, 'A': pA, 'X': pX},\n",
    "                'observed': {'C': 0, 'A': self.A[idx], 'X': self.X[idx]}\n",
    "            }\n",
    "        \n",
    "        return factory\n",
    "    \n",
    "    def _binary_ate_factory(self):\n",
    "        \"\"\"Factory for binary ATE estimand\"\"\"\n",
    "        N = len(self.A)\n",
    "        unique_A = np.unique(self.A)\n",
    "        \n",
    "        def factory():\n",
    "            # Create cross-product of unique A values with X\n",
    "            return {\n",
    "                'permuted': {\n",
    "                    'C': 1,\n",
    "                    'A': np.repeat(unique_A, N),\n",
    "                    'X': np.vstack([self.X, self.X])\n",
    "                },\n",
    "                'observed': {\n",
    "                    'C': 0,\n",
    "                    'A': self.A,\n",
    "                    'X': self.X\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return factory\n",
    "    \n",
    "    def _construct_df(self, data):\n",
    "        \"\"\"Construct a DataFrame from permuted and observed data\"\"\"\n",
    "        # Extract dimensions\n",
    "        n_permuted = len(data['permuted']['A'])\n",
    "        n_observed = len(data['observed']['A'])\n",
    "        n_features = data['permuted']['X'].shape[1]\n",
    "        \n",
    "        # Create base features\n",
    "        df_dict = {\n",
    "            'C': np.concatenate([\n",
    "                np.repeat(data['permuted']['C'], n_permuted),\n",
    "                np.repeat(data['observed']['C'], n_observed)\n",
    "            ]),\n",
    "            'A': np.concatenate([data['permuted']['A'], data['observed']['A']])\n",
    "        }\n",
    "        \n",
    "        # Add X features\n",
    "        X_combined = np.vstack([data['permuted']['X'], data['observed']['X']])\n",
    "        for i in range(n_features):\n",
    "            df_dict[f'X{i}'] = X_combined[:, i]\n",
    "        \n",
    "        # Add interactions between A and X\n",
    "        for i in range(n_features):\n",
    "            df_dict[f'A_X{i}'] = df_dict['A'] * df_dict[f'X{i}']\n",
    "        \n",
    "        return pd.DataFrame(df_dict)\n",
    "    \n",
    "    def _construct_eval_df(self, A, X):\n",
    "        \"\"\"Construct a DataFrame for evaluation\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        df_dict = {'A': A}\n",
    "        \n",
    "        # Add X features\n",
    "        for i in range(n_features):\n",
    "            df_dict[f'X{i}'] = X[:, i]\n",
    "        \n",
    "        # Add interactions between A and X\n",
    "        for i in range(n_features):\n",
    "            df_dict[f'A_X{i}'] = df_dict['A'] * df_dict[f'X{i}']\n",
    "        \n",
    "        return pd.DataFrame(df_dict)\n",
    "    \n",
    "    def _get_trainer_factory(self):\n",
    "        \"\"\"Get the appropriate trainer based on classifier type\"\"\"\n",
    "        if self.classifier == 'logit':\n",
    "            return self._logit_trainer\n",
    "        elif self.classifier == 'boosting':\n",
    "            return self._boosting_trainer\n",
    "        elif self.classifier == 'sgd':\n",
    "            return self._sgd_trainer\n",
    "        elif self.classifier == 'mlp':\n",
    "            return self._mlp_trainer\n",
    "        else:\n",
    "            raise ValueError(f'Unknown classifier: {self.classifier}')\n",
    "    \n",
    "    def _logit_trainer(self, data):\n",
    "        \"\"\"Train a logistic regression model\"\"\"\n",
    "        df = self._construct_df(data)\n",
    "        \n",
    "        # Separate features and target\n",
    "        X_cols = [col for col in df.columns if col != 'C']\n",
    "        X_train = df[X_cols]\n",
    "        y_train = df['C']\n",
    "        \n",
    "        # Train model\n",
    "        model = LogisticRegression(\n",
    "            penalty='l2',\n",
    "            C=1.0,\n",
    "            solver='lbfgs',\n",
    "            max_iter=1000,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        def weight_function(A, X):\n",
    "            \"\"\"Compute weights from the trained model\"\"\"\n",
    "            eval_df = self._construct_eval_df(A, X)\n",
    "            \n",
    "            # Predict probabilities\n",
    "            probs = model.predict_proba(eval_df)[:, 1]\n",
    "            \n",
    "            # Clip probabilities to avoid extreme weights\n",
    "            probs = np.clip(probs, 0.00001, 0.99999)\n",
    "            \n",
    "            # Compute weights\n",
    "            weights = probs / (1 - probs)\n",
    "            \n",
    "            return weights\n",
    "        \n",
    "        return weight_function\n",
    "    \n",
    "    def _boosting_trainer(self, data):\n",
    "        \"\"\"Train a gradient boosting model\"\"\"\n",
    "        df = self._construct_df(data)\n",
    "        \n",
    "        # Separate features and target\n",
    "        X_cols = [col for col in df.columns if col != 'C']\n",
    "        X_train = df[X_cols]\n",
    "        y_train = df['C']\n",
    "        \n",
    "        # Train model\n",
    "        model = GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=3,\n",
    "            random_state=42\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        def weight_function(A, X):\n",
    "            \"\"\"Compute weights from the trained model\"\"\"\n",
    "            eval_df = self._construct_eval_df(A, X)\n",
    "            \n",
    "            # Predict probabilities\n",
    "            probs = model.predict_proba(eval_df)[:, 1]\n",
    "            \n",
    "            # Clip probabilities to avoid extreme weights\n",
    "            probs = np.clip(probs, 0.00001, 0.99999)\n",
    "            \n",
    "            # Compute weights\n",
    "            weights = probs / (1 - probs)\n",
    "            \n",
    "            return weights\n",
    "        \n",
    "        return weight_function\n",
    "    \n",
    "    def _sgd_trainer(self, data):\n",
    "        \"\"\"Train an SGD-based logistic regression model\"\"\"\n",
    "        df = self._construct_df(data)\n",
    "        \n",
    "        # Separate features and target\n",
    "        X_cols = [col for col in df.columns if col != 'C']\n",
    "        X_train = df[X_cols]\n",
    "        y_train = df['C']\n",
    "        \n",
    "        # Identify columns to scale: everything except 'A'\n",
    "        to_scale_cols = [col for col in X_cols if col != 'A']\n",
    "        \n",
    "        # Apply scaling\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = X_train.copy()\n",
    "        X_train_scaled[to_scale_cols] = scaler.fit_transform(X_train[to_scale_cols])\n",
    "        \n",
    "        # Train model\n",
    "        model = SGDClassifier(\n",
    "            loss='log_loss',\n",
    "            penalty='l2',\n",
    "            alpha=0.001,"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-22T12:01:59.633305Z",
     "start_time": "2025-04-22T12:01:59.618024Z"
    }
   },
   "id": "92b06a578202577",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ed147f39394791f6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2e1efd9283d5db2e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
